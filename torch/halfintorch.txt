Binary file amp/__pycache__/autocast_mode.cpython-310.pyc matches
amp/autocast_mode.py:    You should not call ``half()`` or ``bfloat16()`` on your model(s) or inputs when using autocasting.
Binary file autograd/__pycache__/__init__.cpython-310.pyc matches
autograd/__init__.py:half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).
backends/_nnapi/serializer.py:            jit_half_to_float
Binary file backends/cudnn/__pycache__/__init__.cpython-310.pyc matches
backends/cudnn/__init__.py:    torch.half,
_C/__init__.pyi:half: dtype = ...
_C/__init__.pyi:    def chalf(self, *, memory_format: Optional[memory_format]=None) -> Tensor: ...
_C/__init__.pyi:    def half(self) -> Tensor: ...
_C/_VariableFunctions.pyi:def _log_softmax(input: Tensor, dim: _int, half_to_float: _bool, *, out: Optional[Tensor]=None) -> Tensor: ...
_C/_VariableFunctions.pyi:def _softmax(input: Tensor, dim: _int, half_to_float: _bool, *, out: Optional[Tensor]=None) -> Tensor: ...
csrc/autograd/FunctionsManual.cpp:  auto first_half = input.narrow(dim, 0, input_size);
csrc/autograd/FunctionsManual.cpp:  auto second_half = input.narrow(dim, input_size, input_size);
csrc/autograd/FunctionsManual.cpp:  auto sig_second_half = second_half.sigmoid();
csrc/autograd/FunctionsManual.cpp:  auto one_sub_sig_second_half = 1 - sig_second_half;
csrc/autograd/FunctionsManual.cpp:  auto sig_one_sub_sig = sig_second_half * one_sub_sig_second_half;
csrc/autograd/FunctionsManual.cpp:  auto ggI_first_half = grad.narrow(dim, 0, input_size);
csrc/autograd/FunctionsManual.cpp:  auto ggI_second_half = grad.narrow(dim, input_size, input_size);
csrc/autograd/FunctionsManual.cpp:  auto ggI_second_half_times_first_half = ggI_second_half * first_half;
csrc/autograd/FunctionsManual.cpp:  auto gI_first_half = ggI_second_half * gO * sig_one_sub_sig;
csrc/autograd/FunctionsManual.cpp:  auto second_order_sh = sig_one_sub_sig * one_sub_sig_second_half -
csrc/autograd/FunctionsManual.cpp:      sig_second_half * sig_one_sub_sig;
csrc/autograd/FunctionsManual.cpp:  auto gI_second_half =
csrc/autograd/FunctionsManual.cpp:      ggI_second_half_times_first_half * gO * second_order_sh +
csrc/autograd/FunctionsManual.cpp:      ggI_first_half * gO * sig_one_sub_sig;
csrc/autograd/FunctionsManual.cpp:  return at::cat({gI_first_half, gI_second_half}, dim);
csrc/autograd/FunctionsManual.cpp:  //    1. fill the other half by conjugate symmetry
csrc/autograd/FunctionsManual.cpp:  //     3. discard half of results
csrc/autograd/FunctionsManual.cpp:  //     1. fill the other half with zeros (with `zero_grad_shape` below)
csrc/autograd/FunctionsManual.cpp:  auto half_sizes = grad.sym_sizes();
csrc/autograd/FunctionsManual.cpp:  std::vector<c10::SymInt> new_grad_shape(half_sizes.begin(), half_sizes.end());
csrc/autograd/FunctionsManual.cpp:  const auto last_dim = at::maybe_wrap_dim(dim.back(), half_sizes.size());
csrc/autograd/FunctionsManual.cpp:    complex_full_grad.slice_symint(last_dim, 0, half_sizes[last_dim])
csrc/autograd/FunctionsManual.cpp:  // for half inputs, save_mean, save_invstd are float (ideally, we would cast
csrc/autograd/FunctionsManual.cpp:  // for half inputs, save_mean, save_invstd are float
csrc/autograd/generated/python_torch_functionsEverything.cpp:    "_log_softmax(Tensor input, int64_t dim, bool half_to_float, *, Tensor out=None)",
csrc/autograd/generated/python_torch_functionsEverything.cpp:    // aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
csrc/autograd/generated/python_torch_functionsEverything.cpp:    auto dispatch__log_softmax = [](const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functionsEverything.cpp:      return at::_log_softmax(self, dim, half_to_float);
csrc/autograd/generated/python_torch_functionsEverything.cpp:    // aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
csrc/autograd/generated/python_torch_functionsEverything.cpp:    auto dispatch__log_softmax_out = [](at::Tensor out, const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functionsEverything.cpp:      return at::_log_softmax_out(out, self, dim, half_to_float);
csrc/autograd/generated/python_torch_functionsEverything.cpp:    "_softmax(Tensor input, int64_t dim, bool half_to_float, *, Tensor out=None)",
csrc/autograd/generated/python_torch_functionsEverything.cpp:    // aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
csrc/autograd/generated/python_torch_functionsEverything.cpp:    auto dispatch__softmax = [](const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functionsEverything.cpp:      return at::_softmax(self, dim, half_to_float);
csrc/autograd/generated/python_torch_functionsEverything.cpp:    // aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
csrc/autograd/generated/python_torch_functionsEverything.cpp:    auto dispatch__softmax_out = [](at::Tensor out, const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functionsEverything.cpp:      return at::_softmax_out(out, self, dim, half_to_float);
csrc/autograd/generated/python_torch_functions_1.cpp:    "_softmax(Tensor input, int64_t dim, bool half_to_float, *, Tensor out=None)",
csrc/autograd/generated/python_torch_functions_1.cpp:    // aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
csrc/autograd/generated/python_torch_functions_1.cpp:    auto dispatch__softmax = [](const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functions_1.cpp:      return at::_softmax(self, dim, half_to_float);
csrc/autograd/generated/python_torch_functions_1.cpp:    // aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
csrc/autograd/generated/python_torch_functions_1.cpp:    auto dispatch__softmax_out = [](at::Tensor out, const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functions_1.cpp:      return at::_softmax_out(out, self, dim, half_to_float);
csrc/autograd/generated/TraceType_0.cpp:at::Tensor _log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceType_0.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_0.cpp:  auto result =at::_ops::_log_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceType_0.cpp:at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceType_0.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_0.cpp:  at::_ops::_log_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/TraceType_4.cpp:#include <ATen/ops/chalf_ops.h>
csrc/autograd/generated/TraceType_4.cpp:at::Tensor chalf(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
csrc/autograd/generated/TraceType_4.cpp:    op_name = c10::Symbol::fromQualString("aten::chalf");
csrc/autograd/generated/TraceType_4.cpp:  auto result =at::_ops::chalf::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, memory_format);
csrc/autograd/generated/TraceType_4.cpp:  m.impl("chalf",
csrc/autograd/generated/TraceType_4.cpp:         TORCH_FN(TraceType::chalf)
csrc/autograd/generated/VariableType_3.cpp:at::Tensor _softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableType_3.cpp:    return at::redispatch::_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/VariableType_3.cpp:at::Tensor _sparse_log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableType_3.cpp:      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_log_softmax", *opt_op, ks, self, dim, half_to_float);
csrc/autograd/generated/VariableType_3.cpp:      return at::redispatch::_sparse_log_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/ADInplaceOrViewType_1.cpp:at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewType_1.cpp:    at::_ops::_log_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/ADInplaceOrViewType_1.cpp:at::Tensor & _softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewType_1.cpp:    at::_ops::_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/ADInplaceOrViewType_1.cpp:at::Tensor & _sparse_log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewType_1.cpp:    at::_ops::_sparse_log_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/python_torch_functions_0.cpp:    "_log_softmax(Tensor input, int64_t dim, bool half_to_float, *, Tensor out=None)",
csrc/autograd/generated/python_torch_functions_0.cpp:    // aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
csrc/autograd/generated/python_torch_functions_0.cpp:    auto dispatch__log_softmax = [](const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functions_0.cpp:      return at::_log_softmax(self, dim, half_to_float);
csrc/autograd/generated/python_torch_functions_0.cpp:    // aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
csrc/autograd/generated/python_torch_functions_0.cpp:    auto dispatch__log_softmax_out = [](at::Tensor out, const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_torch_functions_0.cpp:      return at::_log_softmax_out(out, self, dim, half_to_float);
csrc/autograd/generated/VariableType_0.cpp:at::Tensor _log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableType_0.cpp:    return at::redispatch::_log_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/VariableType_0.cpp:at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/VariableType_0.cpp:    at::redispatch::_log_softmax_outf(ks & c10::after_autograd_keyset, self_, dim, half_to_float, out_);
csrc/autograd/generated/TraceType_3.cpp:at::Tensor _softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceType_3.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_3.cpp:  auto result =at::_ops::_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceType_3.cpp:at::Tensor & _softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceType_3.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_3.cpp:  at::_ops::_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/TraceType_3.cpp:at::Tensor _sparse_log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceType_3.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_3.cpp:  auto result =at::_ops::_sparse_log_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceType_3.cpp:at::Tensor & _sparse_log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceType_3.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_3.cpp:  at::_ops::_sparse_log_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/python_variable_methods.cpp:#include <ATen/ops/chalf.h>
csrc/autograd/generated/python_variable_methods.cpp:static PyObject * THPVariable_half(PyObject* self, PyObject* args, PyObject* kwargs) {
csrc/autograd/generated/python_variable_methods.cpp:    "half(*, MemoryFormat? memory_format=None)"
csrc/autograd/generated/python_variable_methods.cpp:// chalf
csrc/autograd/generated/python_variable_methods.cpp:static PyObject * THPVariable_chalf(PyObject* self_, PyObject* args, PyObject* kwargs)
csrc/autograd/generated/python_variable_methods.cpp:    "chalf(*, MemoryFormat? memory_format=None)",
csrc/autograd/generated/python_variable_methods.cpp:  // aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
csrc/autograd/generated/python_variable_methods.cpp:  auto dispatch_chalf = [](const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) -> at::Tensor {
csrc/autograd/generated/python_variable_methods.cpp:    return self.chalf(memory_format);
csrc/autograd/generated/python_variable_methods.cpp:  return wrap(dispatch_chalf(self, _r.memoryformatOptional(0)));
csrc/autograd/generated/python_variable_methods.cpp:  {"half", castPyCFunctionWithKeywords(THPVariable_half), METH_VARARGS | METH_KEYWORDS, NULL},
csrc/autograd/generated/python_variable_methods.cpp:  {"chalf", castPyCFunctionWithKeywords(THPVariable_chalf), METH_VARARGS | METH_KEYWORDS, NULL},
csrc/autograd/generated/TraceTypeEverything.cpp:#include <ATen/ops/chalf_ops.h>
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor chalf(c10::DispatchKeySet ks, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
csrc/autograd/generated/TraceTypeEverything.cpp:    op_name = c10::Symbol::fromQualString("aten::chalf");
csrc/autograd/generated/TraceTypeEverything.cpp:  auto result =at::_ops::chalf::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, memory_format);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor _log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  auto result =at::_ops::_log_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  at::_ops::_log_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor _softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  auto result =at::_ops::_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor & _softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  at::_ops::_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor _sparse_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  auto result =at::_ops::_sparse_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor _sparse_log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  auto result =at::_ops::_sparse_log_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor & _sparse_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  at::_ops::_sparse_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/TraceTypeEverything.cpp:at::Tensor & _sparse_log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceTypeEverything.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceTypeEverything.cpp:  at::_ops::_sparse_log_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/TraceTypeEverything.cpp:  m.impl("chalf",
csrc/autograd/generated/TraceTypeEverything.cpp:         TORCH_FN(TraceType::chalf)
csrc/autograd/generated/VariableTypeEverything.cpp:at::Tensor _log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableTypeEverything.cpp:    return at::redispatch::_log_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/VariableTypeEverything.cpp:at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/VariableTypeEverything.cpp:    at::redispatch::_log_softmax_outf(ks & c10::after_autograd_keyset, self_, dim, half_to_float, out_);
csrc/autograd/generated/VariableTypeEverything.cpp:at::Tensor _softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableTypeEverything.cpp:    return at::redispatch::_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/VariableTypeEverything.cpp:at::Tensor & _softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/VariableTypeEverything.cpp:    at::redispatch::_softmax_outf(ks & c10::after_autograd_keyset, self_, dim, half_to_float, out_);
csrc/autograd/generated/VariableTypeEverything.cpp:at::Tensor _sparse_log_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableTypeEverything.cpp:      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_log_softmax", *opt_op, ks, self, dim, half_to_float);
csrc/autograd/generated/VariableTypeEverything.cpp:      return at::redispatch::_sparse_log_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/VariableTypeEverything.cpp:at::Tensor _sparse_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableTypeEverything.cpp:      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_softmax", *opt_op, ks, self, dim, half_to_float);
csrc/autograd/generated/VariableTypeEverything.cpp:      return at::redispatch::_sparse_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/python_sparse_functions.cpp:    "_sparse_log_softmax(Tensor input, int64_t dim, bool half_to_float)",
csrc/autograd/generated/python_sparse_functions.cpp:      // aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
csrc/autograd/generated/python_sparse_functions.cpp:      auto dispatch__sparse_log_softmax = [](const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_sparse_functions.cpp:        return at::_sparse_log_softmax(self, dim, half_to_float);
csrc/autograd/generated/python_sparse_functions.cpp:    "_sparse_softmax(Tensor input, int64_t dim, bool half_to_float)",
csrc/autograd/generated/python_sparse_functions.cpp:      // aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
csrc/autograd/generated/python_sparse_functions.cpp:      auto dispatch__sparse_softmax = [](const at::Tensor & self, int64_t dim, bool half_to_float) -> at::Tensor {
csrc/autograd/generated/python_sparse_functions.cpp:        return at::_sparse_softmax(self, dim, half_to_float);
csrc/autograd/generated/ADInplaceOrViewType_0.cpp:at::Tensor & _sparse_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewType_0.cpp:    at::_ops::_sparse_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/TraceType_2.cpp:at::Tensor _sparse_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/TraceType_2.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_2.cpp:  auto result =at::_ops::_sparse_softmax::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float);
csrc/autograd/generated/TraceType_2.cpp:at::Tensor & _sparse_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/TraceType_2.cpp:    jit::tracer::addInputs(node, "half_to_float", half_to_float);
csrc/autograd/generated/TraceType_2.cpp:  at::_ops::_sparse_softmax_out::redispatch(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::DispatchKey::Tracer), self, dim, half_to_float, out);
csrc/autograd/generated/VariableType_2.cpp:at::Tensor & _softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/VariableType_2.cpp:    at::redispatch::_softmax_outf(ks & c10::after_autograd_keyset, self_, dim, half_to_float, out_);
csrc/autograd/generated/VariableType_2.cpp:at::Tensor _sparse_softmax(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/autograd/generated/VariableType_2.cpp:      return impl::run_jit_decomposition_with_args_for_jvp<at::Tensor>("_sparse_softmax", *opt_op, ks, self, dim, half_to_float);
csrc/autograd/generated/VariableType_2.cpp:      return at::redispatch::_sparse_softmax(ks & c10::after_autograd_keyset, self_, dim, half_to_float);
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:at::Tensor & _log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:    at::_ops::_log_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:at::Tensor & _softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:    at::_ops::_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:at::Tensor & _sparse_log_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:    at::_ops::_sparse_log_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:at::Tensor & _sparse_softmax_out_out(c10::DispatchKeySet ks, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/autograd/generated/ADInplaceOrViewTypeEverything.cpp:    at::_ops::_sparse_softmax_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, self, dim, half_to_float, out);
csrc/cuda/Module.cpp:  bool has_half = true;
csrc/cuda/Module.cpp:  set_module_attr("has_half", has_half ? Py_True : Py_False);
csrc/distributed/rpc/utils.h:// we'd save at least half the data, and over a minimum hurdle.
csrc/distributed/c10d/ProcessGroupNCCL.cpp:              false, "PreMulSum Data type must be half, float, or double");
csrc/jit/backends/xnnpack/serialization/schema.fbs:  /// IEEE754 half-precision floating-point.
csrc/jit/runtime/register_ops_utils.h:// When a number is exactly halfway between two integers, python builtin round
csrc/jit/runtime/register_ops_utils.h:// special halfway case. For positive 'x', round(x/2)*2 =
csrc/jit/runtime/static/README.md:`ProcessedNodeMetadata` holds various "extra" fields on behalf of `ProcessedNode`. Typically, this field is unused. But a few ops need extra machinery to work:
csrc/jit/runtime/static/ops.cpp:    auto half_to_float = in_t.scalar_type() == at::ScalarType::Half &&
csrc/jit/runtime/static/ops.cpp:    at::cpu::_softmax_out(out_t, in_t, dim, half_to_float);
csrc/jit/runtime/static/generated_ops.cpp:              "aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor"))) {
csrc/jit/runtime/static/generated_ops.cpp:          const auto half_to_float = p_node->Input(2).toBool();
csrc/jit/runtime/static/generated_ops.cpp:            p_node->Output(0) = at::cpu::_log_softmax(self, dim, half_to_float);
csrc/jit/runtime/static/generated_ops.cpp:          at::cpu::_log_softmax_out(out, self, dim, half_to_float);
csrc/jit/runtime/static/generated_ops.cpp:          "aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor"))) {
csrc/jit/runtime/static/generated_ops.cpp:      const auto half_to_float = p_node->Input(2).toBool();
csrc/jit/runtime/static/generated_ops.cpp:        p_node->Output(0) = at::cpu::_softmax(self, dim, half_to_float);
csrc/jit/runtime/static/generated_ops.cpp:      at::cpu::_softmax_out(out, self, dim, half_to_float);
csrc/jit/codegen/onednn/prepare_binary.cpp:      //   >>> (1. + torch.rand([2]).half()).dtype
csrc/jit/codegen/onednn/prepare_binary.cpp:      //   >>> (torch.tensor(1.).unsqueeze(0) + (torch.rand([2]).half())).dtype
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:  __half* _B = reinterpret_cast<__half*>(B);
csrc/jit/codegen/cuda/runtime/tensorcore.cu:      reinterpret_cast<Array<__half, 4, 4>*>(&_B[0]));
csrc/jit/codegen/cuda/runtime/tensorcore.cu:      reinterpret_cast<Array<__half, 4, 4>*>(&_B[4]));
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 4, 4>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* A,
csrc/jit/codegen/cuda/runtime/tensorcore.cu:    Array<__half, 8, 8>* B) {
csrc/jit/codegen/cuda/runtime/tensorcore.cu:  __half* _B = reinterpret_cast<__half*>(B);
csrc/jit/codegen/cuda/runtime/tensorcore.cu:      reinterpret_cast<Array<__half, 4, 4>*>(&_B[0]));
csrc/jit/codegen/cuda/runtime/tensorcore.cu:      reinterpret_cast<Array<__half, 4, 4>*>(&_B[4]));
csrc/jit/codegen/cuda/runtime/memory.cu://  .x4. In .x2 option. the the address register of upper half warp (lane 16-31)
csrc/jit/codegen/cuda/runtime/memory.cu:  // Upper half warp has 8 bytes offset from aligned in .x2 option
csrc/jit/codegen/cuda/runtime/memory.cu:  //  adjust by half warp.
csrc/jit/codegen/cuda/runtime/memory.cu:  constexpr unsigned half_warp = 16;
csrc/jit/codegen/cuda/runtime/memory.cu:  // Adjust only in upper half warp.
csrc/jit/codegen/cuda/runtime/memory.cu:  if (thread_id & half_warp) {
csrc/jit/codegen/cuda/runtime/memory.cu:DEVICE_INLINE void ldMatrix(Array<__half, 4, 4>& out, void const* ptr) {
csrc/jit/codegen/cuda/runtime/memory.cu:DEVICE_INLINE void ldMatrixT(Array<__half, 4, 4>& out, void const* ptr) {
csrc/jit/codegen/cuda/runtime/memory.cu:DEVICE_INLINE void ldMatrix(Array<__half, 8, 8>& out, void const* ptr) {
csrc/jit/codegen/cuda/runtime/memory.cu:DEVICE_INLINE void ldMatrixT(Array<__half, 8, 8>& out, void const* ptr) {
csrc/jit/codegen/cuda/runtime/helpers.cu:__device__ __half print_impl(const char* name, __half value) {
csrc/jit/codegen/cuda/runtime/helpers.cu:      __half2float(value),
csrc/jit/codegen/cuda/runtime/fp16_support.cu:struct __half;
csrc/jit/codegen/cuda/runtime/fp16_support.cu:__device__ __half __float2half(const float);
csrc/jit/codegen/cuda/runtime/fp16_support.cu:struct __align__(2) __half {
csrc/jit/codegen/cuda/runtime/fp16_support.cu:  __half() = default;
csrc/jit/codegen/cuda/runtime/fp16_support.cu:  __device__ __half(const float f) {
csrc/jit/codegen/cuda/runtime/fp16_support.cu:    __x = __float2half(f).__x;
csrc/jit/codegen/cuda/runtime/fp16_support.cu:__device__ __half __float2half(const float f) {
csrc/jit/codegen/cuda/runtime/fp16_support.cu:  __half val;
csrc/jit/codegen/cuda/runtime/fp16_support.cu:__device__ float __half2float(const __half h) {
csrc/jit/codegen/cuda/runtime/fp16_support.cu:__device__ __half __double2half(const double d) {
csrc/jit/codegen/cuda/runtime/fp16_support.cu:  __half val;
csrc/jit/codegen/cuda/runtime/fp16_support.cu:  return __float2half(static_cast<float>(d));
csrc/jit/codegen/cuda/runtime/fp16_support.cu:__device__ double __half2double(const __half h) {
csrc/jit/codegen/cuda/runtime/fp16_support.cu:  return static_cast<double>(__half2float(h));
csrc/jit/codegen/cuda/type.cpp:      return "__half";
csrc/jit/codegen/cuda/type.cpp:      return "__float2half";
csrc/jit/codegen/cuda/type.cpp:      return "__double2half";
csrc/jit/codegen/cuda/type.cpp:      return "__half2float";
csrc/jit/codegen/cuda/type.cpp:      return "__half2double";
csrc/jit/codegen/cuda/fusion_segmenter.cpp:  auto half_precision_tv = make_consumer_tv(original_tv, dtype);
csrc/jit/codegen/cuda/fusion_segmenter.cpp:  IrBuilder::create<UnaryOp>(UnaryOpType::Cast, half_precision_tv, original_tv);
csrc/jit/codegen/cuda/fusion_segmenter.cpp:  IrBuilder::create<UnaryOp>(UnaryOpType::Cast, fp32_tv, half_precision_tv);
csrc/jit/codegen/cuda/fusion_segmenter.cpp:  return half_precision_tv;
csrc/jit/codegen/cuda/fusion_segmenter.cpp:  std::unordered_map<TensorView*, TensorView*> fp32_to_half_cast_map;
csrc/jit/codegen/cuda/fusion_segmenter.cpp:    // tv -> float2half -> output
csrc/jit/codegen/cuda/fusion_segmenter.cpp:    //            \ -> half2float -> other uses in group
csrc/jit/codegen/cuda/fusion_segmenter.cpp:    // The conversion back and forth from half precision can hurt numerics.
csrc/jit/codegen/cuda/fusion_segmenter.cpp:      auto cast_tv_it = fp32_to_half_cast_map.find(edge->val->as<TensorView>());
csrc/jit/codegen/cuda/fusion_segmenter.cpp:      if (cast_tv_it == fp32_to_half_cast_map.end()) {
csrc/jit/codegen/cuda/fusion_segmenter.cpp:            force_half_precision_type_);
csrc/jit/codegen/cuda/fusion_segmenter.cpp:        fp32_to_half_cast_map[edge->val->as<TensorView>()] = cast_tv;
csrc/jit/codegen/cuda/fusion_segmenter.cpp:    auto other_half_type =
csrc/jit/codegen/cuda/fusion_segmenter.cpp:        [&cast_to_type, &other_half_type](auto* val) {
csrc/jit/codegen/cuda/fusion_segmenter.cpp:                other_half_type != dtype,
csrc/jit/codegen/cuda/fusion_segmenter.cpp:        force_half_precision_type_ = dtype;
csrc/jit/codegen/cuda/type_inference.cpp:        const auto half_to_float = constant_as<bool>(node->input(2));
csrc/jit/codegen/cuda/type_inference.cpp:            half_to_float.has_value(),
csrc/jit/codegen/cuda/type_inference.cpp:            "half_to_float bool doesn't have a value.");
csrc/jit/codegen/cuda/type_inference.cpp:        if (half_to_float.value()) {
csrc/jit/codegen/cuda/ops/normalization.cpp:  // There are compilation errors for half precision
csrc/jit/codegen/cuda/ops/composite.cpp:  auto half = IrBuilder::create<Double>(x->container(), 0.5);
csrc/jit/codegen/cuda/ops/composite.cpp:  auto cdf = mul(half, add(one, erf(mul(x, kappa))));
csrc/jit/codegen/cuda/parser.cpp:  // we do NOT support half math type yet
csrc/jit/codegen/cuda/parser.cpp:          "aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor");
csrc/jit/codegen/cuda/parser.cpp:              const auto half_to_float = constant_as<bool>(node->input(2));
csrc/jit/codegen/cuda/parser.cpp:                  half_to_float.has_value(), "Bool half_to_float is not valid");
csrc/jit/codegen/cuda/parser.cpp:              if (half_to_float.value() &&
csrc/jit/codegen/cuda/parser.cpp:              // We can only handle output as half, float, and double;
csrc/jit/codegen/cuda/parser.cpp:              // We can only handle output as half, float, and double;
csrc/jit/codegen/cuda/scheduler/utils.h:// Assume any only half of the register file is available to spend on buffers,
csrc/jit/codegen/cuda/scheduler/utils.h:// T2[I0]     half
csrc/jit/codegen/cuda/scheduler/registry.cpp:        // Don't go persistent if we can't fit half a warp on an SM
csrc/jit/codegen/cuda/scheduler/registry.cpp:                     // half warp
csrc/jit/codegen/cuda/scheduler/normalization.cpp:  // is half and would take half the memory. A more complex scenario of this
csrc/jit/codegen/cuda/scheduler/pointwise.h: * T1[i0, b1, i2] half
csrc/jit/codegen/cuda/scheduler/pointwise.cpp:            // If right transfer size is bigger than half of L2
csrc/jit/codegen/cuda/graph_fuser.cpp:    // only take the second half and we would need the size information.
csrc/jit/codegen/cuda/fusion_segmenter.h:  DataType force_half_precision_type_;
csrc/jit/codegen/cuda/lower_validation.cpp:    // Allow half2, float2, float4 and same sized vtypes.
csrc/jit/codegen/cuda/test/test_gpu_validator.h:  std::array<std::array<double, 2>, 20> sum_tolerances_half = {
csrc/jit/codegen/cuda/test/test_gpu_validator.h:  double base_half_abs_tol = -1;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:  double base_half_rel_tol = -1;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& sum_tolerance_entry = tolerances.sum_tolerances_half;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_abs = tolerances.base_half_abs_tol;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_rel = tolerances.base_half_rel_tol;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& sum_tolerance_entry = tolerances.sum_tolerances_half;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_abs = tolerances.base_half_abs_tol;
csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_rel = tolerances.base_half_rel_tol;
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto options_half = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t0 = at::randn(input_shape, options_half);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t1 = at::randn(input_shape[1], options_half);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t2 = at::randn(input_shape[1], options_half);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto options_half = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t0 = at::randn(shape, options_half);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t1 = at::randn(shape, options_half);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto options_half = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t0 = at::randn(shape, options_half);
csrc/jit/codegen/cuda/test/test_gpu_fused_reduction.cpp:  auto t1 = at::randn(shape, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  auto at_out_half = at_out.to(c10::ScalarType::Half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  std::vector<at::Tensor> aten_outputs = {at_out, at_out_half};
csrc/jit/codegen/cuda/test/test_gpu2.cpp:          //       with half precision. skipping too large volumes for half for
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  auto options_half = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t1 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t3 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t5 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t7 = at::randn({1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t11 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t13 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t15 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:  at::Tensor at_t17 = at::randn({128, 64, 1024}, options_half);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:__global__ void CUDAGeneratedKernel(Tensor<__half, 4> T0, Tensor<__half, 4> T2, Tensor<__half, 4> T7) {
csrc/jit/codegen/cuda/test/test_gpu2.cpp:    __half T9[1];
csrc/jit/codegen/cuda/test/test_gpu2.cpp:    __half T8[1];
csrc/jit/codegen/cuda/test/test_gpu2.cpp:       = __half2float(T9[0]);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:       = __half2float(T8[0]);
csrc/jit/codegen/cuda/test/test_gpu2.cpp:    __half T10[1];
csrc/jit/codegen/cuda/test/test_gpu2.cpp:       = __float2half(T6[0]);
csrc/jit/codegen/cuda/test/test_gpu_view.cpp:        : at::_softmax(at_x, kAxis, false /* half_to_float */);
csrc/jit/codegen/cuda/test/test_gpu_view.cpp:        ? at::_softmax(at_x_view, kAxis, false /* half_to_float */)
csrc/jit/codegen/cuda/test/test_gpu_view.cpp:  auto at_tv1 = at::_softmax(at_x, kAxis, false /* half_to_float */);
csrc/jit/codegen/cuda/test/test_gpu3.cpp:// Test assertion in unsupported pattern: half-inlined loop swizzle.
csrc/jit/codegen/fuser/cuda/resource_strings.h:// This snippet enables half support in the jit. Following the pattern for
csrc/jit/codegen/fuser/cuda/resource_strings.h:// with __half2float(). All mathematical operations are done on float
csrc/jit/codegen/fuser/cuda/resource_strings.h:// converted to half with __float2half() when writing to a half tensor.
csrc/jit/codegen/fuser/cuda/resource_strings.h:constexpr auto half_support_literal =
csrc/jit/codegen/fuser/cuda/resource_strings.h:typedef __half half;
csrc/jit/codegen/fuser/cuda/resource_strings.h:constexpr auto half_support_literal =
csrc/jit/codegen/fuser/cuda/resource_strings.h:  struct __align__(2) __half {
csrc/jit/codegen/fuser/cuda/resource_strings.h:    __host__ __device__ __half() { }
csrc/jit/codegen/fuser/cuda/resource_strings.h:    __device__ __half __float2half(const float f) {
csrc/jit/codegen/fuser/cuda/resource_strings.h:      __half val;
csrc/jit/codegen/fuser/cuda/resource_strings.h:    __device__ float __half2float(const __half h) {
csrc/jit/codegen/fuser/cuda/resource_strings.h:typedef __half half;
csrc/jit/codegen/fuser/codegen.cpp:    return "half";
csrc/jit/codegen/fuser/codegen.cpp:  bool has_half_tensor = false;
csrc/jit/codegen/fuser/codegen.cpp:    // Note: conversion from half is only supported for CUDA kernels.
csrc/jit/codegen/fuser/codegen.cpp:      const auto is_half = input.second.has_value() &&
csrc/jit/codegen/fuser/codegen.cpp:      if (is_half) {
csrc/jit/codegen/fuser/codegen.cpp:            format("__half2float(t${formal}.data[t${formal}_offset])", env));
csrc/jit/codegen/fuser/codegen.cpp:        env.s("access_vec4", format("__half2float(t${formal}_buf[i])", env));
csrc/jit/codegen/fuser/codegen.cpp:        has_half_tensor = true;
csrc/jit/codegen/fuser/codegen.cpp:    // Note: conversion to half is only supported for CUDA kernels.
csrc/jit/codegen/fuser/codegen.cpp:    const auto is_half = (output.second.scalar_type == at::ScalarType::Half);
csrc/jit/codegen/fuser/codegen.cpp:    if (is_half) {
csrc/jit/codegen/fuser/codegen.cpp:      body << format("${access} = __float2half(${node});\n", env);
csrc/jit/codegen/fuser/codegen.cpp:      body_vec4 << format("${access_vec4} = __float2half(${node});\n", env);
csrc/jit/codegen/fuser/codegen.cpp:      has_half_tensor = true;
csrc/jit/codegen/fuser/codegen.cpp:  // Note: CUDA kernels support halfs and random generation, CPU kernels do not
csrc/jit/codegen/fuser/codegen.cpp:  if (has_half_tensor) {
csrc/jit/codegen/fuser/codegen.cpp:    env.s("HalfHeader", cuda::half_support_literal);
csrc/jit/codegen/fuser/codegen.cpp:  if (use_cuda && has_half_tensor) {
csrc/jit/frontend/sugared_value.cpp:      {"half", at::kHalf}};
csrc/jit/JIT-AUTOCAST.md:# this will print half-precision dtype
csrc/jit/tensorexpr/cuda_codegen.cpp:#include <torch/csrc/jit/tensorexpr/half_support.h>
csrc/jit/tensorexpr/cuda_codegen.cpp:      return "half";
csrc/jit/tensorexpr/cuda_codegen.cpp:      ? "__float2half"
csrc/jit/tensorexpr/cuda_codegen.cpp:      ? "__half2float"
csrc/jit/tensorexpr/cuda_codegen.cpp:    // There's no __ldg overload for bool or half.
csrc/jit/tensorexpr/cuda_codegen.cpp:  // half_support_literal.
csrc/jit/tensorexpr/cuda_codegen.cpp:  HalfChecker halfChecker(buffer_args());
csrc/jit/tensorexpr/cuda_codegen.cpp:  stmt_v->accept(&halfChecker);
csrc/jit/tensorexpr/cuda_codegen.cpp:  if (halfChecker.hasHalf()) {
csrc/jit/tensorexpr/cuda_codegen.cpp:  if (halfChecker.hasHalf()) {
csrc/jit/tensorexpr/cuda_codegen.cpp:    os() << fuser::cuda::half_support_literal << std::endl;
csrc/jit/tensorexpr/cuda_codegen.cpp:  if (halfChecker.hasBFloat16()) {
csrc/jit/tensorexpr/cuda_codegen.cpp:  // The registerizer might insert half-type scalars, we don't want this.
csrc/jit/tensorexpr/eval.cpp:      // bool/half not supported
csrc/jit/tensorexpr/eval.cpp:        // bool/half not supported
csrc/jit/tensorexpr/expr.cpp:  ExprHandle half_v = FloatImm::make(0.5f);
csrc/jit/tensorexpr/expr.cpp:  ExprHandle x2 = x * half_v;
csrc/jit/tensorexpr/expr.cpp:  ExprHandle z = (y + one_v) * half_v;
csrc/jit/tensorexpr/intrinsic_symbols.cpp:    // float -> half & half -> float conversio)ns
csrc/jit/tensorexpr/half_support.h:    inserted_half_casts_.insert(ret);
csrc/jit/tensorexpr/half_support.h:      inserted_half_casts_.insert(new_val);
csrc/jit/tensorexpr/half_support.h:      inserted_half_casts_.insert(new_val);
csrc/jit/tensorexpr/half_support.h:    // just don't allow half casts we didn't insert.
csrc/jit/tensorexpr/half_support.h:      if (inserted_half_casts_.count(v) < 1) {
csrc/jit/tensorexpr/half_support.h:      auto from_half = isHalf(cast_child->src_value());
csrc/jit/tensorexpr/half_support.h:      // Cannot simplify the double(float(half)) to double(half) as NNC does
csrc/jit/tensorexpr/half_support.h:      auto not_cast_half_to_doulbe = !(cast_to_double && from_half);
csrc/jit/tensorexpr/half_support.h:          cast_child->dtype().is_floating_point() && not_cast_half_to_doulbe) {
csrc/jit/tensorexpr/half_support.h:  std::unordered_set<ExprPtr> inserted_half_casts_;
csrc/jit/tensorexpr/llvm_codegen.cpp:#include <torch/csrc/jit/tensorexpr/half_support.h>
csrc/jit/tensorexpr/types.cpp:      return "half";
csrc/jit/passes/frozen_conv_folding.cpp:        // If this is on GPU and bias is none and weight was half/bfloat, but
csrc/api/include/torch/python.h:      .def("half", [](ModuleType& module) { module.to(kFloat16); })
csrc/utils/tensor_dtypes.cpp:      return std::make_pair("float16", "half");
csrc/utils/tensor_dtypes.cpp:      return std::make_pair("complex32", "chalf");
csrc/lazy/generated/LazyIr.h:  LogSoftmax(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float, std::vector<torch::lazy::Shape>&& shapes)
csrc/lazy/generated/LazyIr.h:              torch::lazy::MHash(dim, half_to_float)),
csrc/lazy/generated/LazyIr.h:        half_to_float(half_to_float)
csrc/lazy/generated/LazyIr.h:    ss << ", half_to_float=" << half_to_float;
csrc/lazy/generated/LazyIr.h:  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float) const {
csrc/lazy/generated/LazyIr.h:        this->half_to_float == half_to_float);
csrc/lazy/generated/LazyIr.h:    arguments.emplace_back("half_to_float", half_to_float);
csrc/lazy/generated/LazyIr.h:  bool half_to_float;
csrc/lazy/generated/LazyIr.h:  Softmax(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float, std::vector<torch::lazy::Shape>&& shapes)
csrc/lazy/generated/LazyIr.h:              torch::lazy::MHash(dim, half_to_float)),
csrc/lazy/generated/LazyIr.h:        half_to_float(half_to_float)
csrc/lazy/generated/LazyIr.h:    ss << ", half_to_float=" << half_to_float;
csrc/lazy/generated/LazyIr.h:  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float) const {
csrc/lazy/generated/LazyIr.h:        this->half_to_float == half_to_float);
csrc/lazy/generated/LazyIr.h:    arguments.emplace_back("half_to_float", half_to_float);
csrc/lazy/generated/LazyIr.h:  bool half_to_float;
csrc/lazy/generated/LazyNativeFunctions.cpp:    at::Tensor LazyNativeFunctions::_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/lazy/generated/LazyNativeFunctions.cpp:                half_to_float
csrc/lazy/generated/LazyNativeFunctions.cpp:        torch::lazy::NodePtr node = torch::lazy::ReuseNode<LogSoftmax>(lazy_self->GetIrValue(), dim, half_to_float);
csrc/lazy/generated/LazyNativeFunctions.cpp:        auto out_meta = at::meta::_log_softmax(self_meta, dim, half_to_float);
csrc/lazy/generated/LazyNativeFunctions.cpp:                std::vector<torch::jit::IValue> inputs = { self, dim, half_to_float };
csrc/lazy/generated/LazyNativeFunctions.cpp:                const char* schema_str = "aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor";
csrc/lazy/generated/LazyNativeFunctions.cpp:            node = torch::lazy::MakeNode<LogSoftmax>(lazy_self->GetIrValue(), dim, half_to_float, std::move(shapes));
csrc/lazy/generated/LazyNativeFunctions.cpp:    at::Tensor LazyNativeFunctions::_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/lazy/generated/LazyNativeFunctions.cpp:                half_to_float
csrc/lazy/generated/LazyNativeFunctions.cpp:        torch::lazy::NodePtr node = torch::lazy::ReuseNode<Softmax>(lazy_self->GetIrValue(), dim, half_to_float);
csrc/lazy/generated/LazyNativeFunctions.cpp:        auto out_meta = at::meta::_softmax(self_meta, dim, half_to_float);
csrc/lazy/generated/LazyNativeFunctions.cpp:                std::vector<torch::jit::IValue> inputs = { self, dim, half_to_float };
csrc/lazy/generated/LazyNativeFunctions.cpp:                const char* schema_str = "aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor";
csrc/lazy/generated/LazyNativeFunctions.cpp:            node = torch::lazy::MakeNode<Softmax>(lazy_self->GetIrValue(), dim, half_to_float, std::move(shapes));
csrc/lazy/generated/LazyNativeFunctions.h:static at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
csrc/lazy/generated/LazyNativeFunctions.h:static at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
csrc/lazy/generated/RegisterLazy.cpp:at::Tensor wrapper_Lazy___log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/lazy/generated/RegisterLazy.cpp:  return torch::lazy::LazyNativeFunctions::_log_softmax(self, dim, half_to_float);
csrc/lazy/generated/RegisterLazy.cpp:at::Tensor & wrapper_Lazy_out__log_softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/lazy/generated/RegisterLazy.cpp:  auto wrapper_Lazy_out__log_softmax_out_tmp = wrapper_Lazy___log_softmax(self, dim, half_to_float);
csrc/lazy/generated/RegisterLazy.cpp:at::Tensor wrapper_Lazy___softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
csrc/lazy/generated/RegisterLazy.cpp:  return torch::lazy::LazyNativeFunctions::_softmax(self, dim, half_to_float);
csrc/lazy/generated/RegisterLazy.cpp:at::Tensor & wrapper_Lazy_out__softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
csrc/lazy/generated/RegisterLazy.cpp:  auto wrapper_Lazy_out__softmax_out_tmp = wrapper_Lazy___softmax(self, dim, half_to_float);
Binary file cuda/__pycache__/__init__.cpython-310.pyc matches
cuda/__init__.py:has_half: bool = False
cuda/__init__.py:        return torch.half
cuda/__init__.py:    'has_half', 'has_magma', 'init', 'initial_seed', 'ipc_collect', 'is_available', 'is_bf16_supported',
_decomp/decompositions.py:def _softmax(x: Tensor, dim: int, half_to_float: bool):
_decomp/decompositions.py:    if half_to_float:
_decomp/decompositions.py:        assert x.dtype == torch.half
_decomp/decompositions.py:    if not half_to_float:
_decomp/decompositions.py:def _log_softmax(x: Tensor, dim: int, half_to_float: bool):
_decomp/decompositions.py:    if half_to_float:
_decomp/decompositions.py:        assert x.dtype == torch.half
_decomp/decompositions.py:    if not half_to_float:
_decomp/decompositions.py:    # The bounds are passed as twice their value so that half-integer values
Binary file _decomp/__pycache__/decompositions.cpython-310.pyc matches
distributed/algorithms/_comm_hooks/default_hooks.py:    approach that casts ``grad`` to half-precision floating-point format (``torch.float16``).
distributed/algorithms/_comm_hooks/default_hooks.py:    approach that casts ``grad`` to half-precision floating-point format (``torch.float16``).
distributed/algorithms/_quantization/quantization.py:    return torch.clamp(tensor, TORCH_HALF_MIN, TORCH_HALF_MAX).half()
distributed/algorithms/ddp_comm_hooks/default_hooks.py:    approach that casts ``GradBucket`` tensor to half-precision floating-point format (``torch.float16``)
distributed/algorithms/ddp_comm_hooks/default_hooks.py:    approach that casts ``GradBucket`` tensor to half-precision
distributed/algorithms/ddp_comm_hooks/default_hooks.py:    This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision
distributed/algorithms/ddp_comm_hooks/default_hooks.py:    This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision
distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py:    QR factorization doesn't work with half-precision, but it is usually faster with a rank > 2.
distributed/fsdp/flat_param.py:runs the unshard/reshard on behalf of the fully sharded module by overriding
distributed/nn/api/remote_module.py:    def half(self: T) -> T:  # type: ignore[return]
distributed/nn/api/remote_module.py:        _raise_not_supported(self.half.__name__)
distributions/uniform.py:    Generates uniformly distributed random samples from the half-open interval
distributions/kl.py:from .half_normal import HalfNormal
distributions/kl.py:def _kl_halfnormal_halfnormal(p, q):
distributions/kl.py:    half_term1 = (q._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1) -
distributions/kl.py:    return half_term1 + 0.5 * (term2 + term3 - n)
distributions/half_normal.py:    Creates a half-normal distribution parameterized by `scale` where::
distributions/half_normal.py:        >>> m.sample()  # half-normal distributed with scale=1
distributions/constraint_registry.py:@biject_to.register(constraints.half_open_interval)
distributions/constraint_registry.py:@transform_to.register(constraints.half_open_interval)
distributions/multivariate_normal.py:        half_log_det = self._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)
distributions/multivariate_normal.py:        return -0.5 * (self._event_shape[0] * math.log(2 * math.pi) + M) - half_log_det
distributions/multivariate_normal.py:        half_log_det = self._unbroadcasted_scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)
distributions/multivariate_normal.py:        H = 0.5 * self._event_shape[0] * (1.0 + math.log(2 * math.pi)) + half_log_det
distributions/constraints.py:    'half_open_interval',
distributions/constraints.py:    Constrain to a real half line `(lower_bound, inf]`.
distributions/constraints.py:    Constrain to a real half line `[lower_bound, inf)`.
distributions/constraints.py:    Constrain to a real half line `[-inf, upper_bound)`.
distributions/constraints.py:half_open_interval = _HalfOpenInterval
distributions/cauchy.py:        scale (float or Tensor): half width at half maximum.
distributions/half_cauchy.py:    Creates a half-Cauchy distribution parameterized by `scale` where::
distributions/half_cauchy.py:        >>> m.sample()  # half-cauchy distributed with scale=1
Binary file distributions/__pycache__/__init__.cpython-310.pyc matches
Binary file distributions/__pycache__/negative_binomial.cpython-310.pyc matches
Binary file distributions/__pycache__/uniform.cpython-310.pyc matches
Binary file distributions/__pycache__/constraint_registry.cpython-310.pyc matches
Binary file distributions/__pycache__/continuous_bernoulli.cpython-310.pyc matches
Binary file distributions/__pycache__/transformed_distribution.cpython-310.pyc matches
Binary file distributions/__pycache__/multivariate_normal.cpython-310.pyc matches
Binary file distributions/__pycache__/half_cauchy.cpython-310.pyc matches
Binary file distributions/__pycache__/kl.cpython-310.pyc matches
Binary file distributions/__pycache__/half_normal.cpython-310.pyc matches
Binary file distributions/__pycache__/constraints.cpython-310.pyc matches
Binary file distributions/__pycache__/cauchy.cpython-310.pyc matches
distributions/continuous_bernoulli.py:        cut_probs_below_half = torch.where(torch.le(cut_probs, 0.5),
distributions/continuous_bernoulli.py:        cut_probs_above_half = torch.where(torch.ge(cut_probs, 0.5),
distributions/continuous_bernoulli.py:            torch.log1p(-2.0 * cut_probs_below_half),
distributions/continuous_bernoulli.py:            torch.log(2.0 * cut_probs_above_half - 1.0))
distributions/__init__.py:from .half_cauchy import HalfCauchy
distributions/__init__.py:from .half_normal import HalfNormal
distributions/transformed_distribution.py:    :class:`~torch.distributions.half_cauchy.HalfCauchy`,
distributions/transformed_distribution.py:    :class:`~torch.distributions.half_normal.HalfNormal`,
distributions/negative_binomial.py:        probs (Tensor): Event probabilities of success in the half open interval [0, 1)
distributions/negative_binomial.py:                       'probs': constraints.half_open_interval(0., 1.),
_dynamo/utils.py:    torch.HalfTensor: (torch.float16, torch.half),
Binary file fft/__pycache__/__init__.cpython-310.pyc matches
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    though, for real inputs, half of these values are redundant.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    though, for real inputs, half of these values are redundant.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    input (Tensor): the input tensor representing a half-Hermitian signal
fft/__init__.py:    Supports torch.half on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:        The last dimension must be the half-Hermitian compressed dimension.
fft/__init__.py:    Supports torch.half on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:        The last dimension must be the half-Hermitian compressed dimension.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    input (Tensor): the input tensor representing a half-Hermitian signal
fft/__init__.py:    Supports torch.half on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:        The last dimension must be the half-Hermitian compressed dimension.
fft/__init__.py:    Supports torch.half on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:    Hermitian time-space signal takes up only half the space.
fft/__init__.py:    Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater.
fft/__init__.py:        The last dimension must be the half-Hermitian compressed dimension.
fft/__init__.py:    Supports torch.half on CUDA with GPU Architecture SM53 or greater.
functional.py:        onesided (bool, optional): controls whether to return half of results to
_functorch/top_operators_github_usage.py:    'half',
_functorch/fx_minifier.py:    2. Delta Debugging: Tries replacing half of the graph with inputs. If fails,
__future__.py:2. `module.float()` / `.double()` / `.half()` (for converting `module` to a different dtype)
include/oneapi/dnnl/dnnl.hpp:        /// [16-bit/half-precision floating point](https://en.wikipedia.org/wiki/Half-precision_floating-point_format).
include/oneapi/dnnl/dnnl_graph.hpp:        /// 16-bit/half-precision floating point.
include/oneapi/dnnl/dnnl_graph.hpp:        /// value can be "half_pixel" or "align_corners". The attribute is
include/oneapi/dnnl/dnnl_graph_types.h:    /// 16-bit/half-precision floating point.
include/oneapi/dnnl/dnnl_graph_types.h:    /// value can be "half_pixel" or "align_corners". The attribute is defined
include/oneapi/dnnl/dnnl_types.h:    /// 16-bit/half-precision floating point.
include/xnnpack.h:  /// IEEE754 half-precision floating-point.
include/ATen/core/aten_interned_strings.h:_(aten, chalf) \
include/ATen/core/aten_interned_strings.h:_(attr, half_to_float) \
include/ATen/core/TensorBody.h:  at::Tensor chalf(c10::optional<at::MemoryFormat> memory_format=c10::nullopt) const;
include/ATen/core/TensorBody.h:// aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
include/ATen/core/TensorBody.h:inline at::Tensor Tensor::chalf(c10::optional<at::MemoryFormat> memory_format) const {
include/ATen/core/TensorBody.h:    return at::_ops::chalf::call(const_cast<Tensor&>(*this), memory_format);
include/ATen/core/TransformationHelper.h:template <> struct DistAccumType<half> { using type = float; };
include/ATen/core/TransformationHelper.h:      // we need log to be not 0, and not underflow when converted to half
include/ATen/Functions.h:#include <ATen/ops/chalf.h>
include/ATen/cuda/Atomic.cuh:  return atomicAdd(reinterpret_cast<__half*>(address), val);
include/ATen/cuda/llvm_jit_strings.h:TORCH_CUDA_CPP_API const std::string &get_complex_half_body_string();
include/ATen/cuda/CUDATensorMethods.cuh:inline __half* Tensor::data() const {
include/ATen/cuda/CUDATensorMethods.cuh:  return reinterpret_cast<__half*>(data<Half>());
include/ATen/cuda/cub.cuh:  using type = __half;
include/ATen/VmapGeneratedPlumbing.h:at::Tensor chalf_generated_plumbing(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
include/ATen/VmapGeneratedPlumbing.h:    return at::_ops::chalf::call(self, memory_format);
include/ATen/VmapGeneratedPlumbing.h:at::Tensor _log_softmax_generated_plumbing(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/VmapGeneratedPlumbing.h:    return at::_ops::_log_softmax::call(self, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:  auto results = batch_rule(self_value, self_bdim, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:at::Tensor _softmax_generated_plumbing(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/VmapGeneratedPlumbing.h:    return at::_ops::_softmax::call(self, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:  auto results = batch_rule(self_value, self_bdim, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:at::Tensor _sparse_softmax_generated_plumbing(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/VmapGeneratedPlumbing.h:    return at::_ops::_sparse_softmax::call(self, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:  auto results = batch_rule(self_value, self_bdim, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:at::Tensor _sparse_log_softmax_generated_plumbing(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/VmapGeneratedPlumbing.h:    return at::_ops::_sparse_log_softmax::call(self, dim, half_to_float);
include/ATen/VmapGeneratedPlumbing.h:  auto results = batch_rule(self_value, self_bdim, dim, half_to_float);
include/ATen/cpu/vec/vec256/vec256_complex_float.h:  const Vectorized i_half = _mm256_setr_ps(0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5);
include/ATen/cpu/vec/vec256/vec256_complex_float.h:  return i_half*ln;                                                 // i/2*ln()
include/ATen/cpu/vec/vec256/vec256_qint.h:    // The vectorized code above always rounds to even in halfway cases
include/ATen/cpu/vec/vec256/vec256_qint.h:    // using std::round because it does rounding away from zero in halfway
include/ATen/cpu/vec/vec256/vec256_int.h:  // int32_t has half the size of double
include/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h:    return ln * vd_imag_half; // i/2*ln()
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:constexpr int blendChoice(uint32_t mask, uint32_t half1 = 0xF, uint32_t half2 = 0xF0) {
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  uint32_t both = half1 | half2;
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  else if (mask == half1)
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  else if (mask == half2)
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  else if (mask > 0 && mask < half1)
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  else if ((mask & half2) == half2)
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  else if ((mask & half1) == 0 && mask > half1)
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:  else if ((mask & half1) == half1 && mask > half1)
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:const vfloat32 half = vec_splats(0.5f);
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:const vfloat32 tanh_half_max = vec_splats(44.014845935754205f);
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:const vfloat32 imag_half = vfloat32{0.f, 0.5f, 0.f, 0.5f};
include/ATen/cpu/vec/vec256/vsx/vsx_helpers.h:const vfloat64 vd_imag_half = vfloat64{0.0, 0.5};
include/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h:    return ln * imag_half; // i/2*ln()
include/ATen/cpu/vec/vec256/vec256_complex_double.h:  const Vectorized i_half = _mm256_setr_pd(0.0, 0.5, 0.0, 0.5);
include/ATen/cpu/vec/vec256/vec256_complex_double.h:  return i_half*ln;                                                 // i/2*ln()
include/ATen/cpu/vec/vec512/vec512_complex_float.h:  const Vectorized i_half = _mm512_setr_ps(0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5,
include/ATen/cpu/vec/vec512/vec512_complex_float.h:  return i_half*ln;                                                 // i/2*ln()
include/ATen/cpu/vec/vec512/vec512_int.h:  // int32_t has half the size of double
include/ATen/cpu/vec/vec512/vec512_complex_double.h:  const Vectorized i_half = _mm512_setr_pd(0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5);
include/ATen/cpu/vec/vec512/vec512_complex_double.h:  return i_half*ln;                                                 // i/2*ln()
include/ATen/cpu/vec/vec512/vec512_qint.h:    // The vectorized code above always rounds to even in halfway cases
include/ATen/cpu/vec/vec512/vec512_qint.h:    // using std::round because it does rounding away from zero in halfway
include/ATen/cpu/vec/vec_base.h:  static constexpr int half_size = size / 2;
include/ATen/cpu/vec/vec_base.h:  for (const auto i : c10::irange(half_size)) {
include/ATen/cpu/vec/vec_base.h:    buffer1[half_size + i] = b_arr[i * 2];
include/ATen/cpu/vec/vec_base.h:    buffer2[half_size + i] = b_arr[i * 2 + 1];
include/ATen/cpu/vec/vec_base.h:  static constexpr int half_size = size / 2;
include/ATen/cpu/vec/vec_base.h:  for (const auto i : c10::irange(half_size)) {
include/ATen/cpu/vec/vec_base.h:    buffer2[i * 2] = a_arr[half_size + i];
include/ATen/cpu/vec/vec_base.h:    buffer2[i * 2 + 1] = b_arr[half_size + i];
include/ATen/Dispatch.h:// but NOT booleans (bool), half-precision floats (Half) or
include/ATen/Dispatch.h:// are NOT integers mod 2, half precision operations ~essentially
include/ATen/Dispatch.h:// 2. Should half be supported?  (If you're on CPU, the answer is almost
include/ATen/NativeMetaFunctions.h:#include <ATen/ops/chalf_meta.h>
include/ATen/CompositeImplicitAutogradFunctions_inl.h:#include <ATen/ops/chalf_compositeimplicitautograd_dispatch.h>
include/ATen/NativeFunctions.h:#include <ATen/ops/chalf_native.h>
include/ATen/Operators.h:#include <ATen/ops/chalf_ops.h>
include/ATen/native/LinearAlgebraUtils.h:// Returns the epsilon value for floating types except half
include/ATen/native/cuda/CuFFTPlanCache.h:    // For half, base strides on the real part of real-to-complex and
include/ATen/native/cuda/CuFFTPlanCache.h:      // cuFFT on half requires compute capability of at least SM_53
include/ATen/native/cuda/CuFFTPlanCache.h:               "cuFFT doesn't support signals of half type with compute "
include/ATen/native/cuda/CuFFTPlanCache.h:               "capability less than SM_53, but the device containing input half "
include/ATen/native/cuda/CuFFTPlanCache.h:            " computing in half precision, but got a signal size of",
include/ATen/native/cuda/fused_adam_utils.cuh:// For most optimizers, GradScaler unscales gradients on behalf of those optimizers.
include/ATen/native/cuda/Math.cuh:    constexpr T half{0.5};
include/ATen/native/cuda/Math.cuh:    s -= half * b;
include/ATen/native/cuda/PersistentSoftmax.cuh:// input_t=half,  acc_t=float, output_t=half  => read half tensor, float accumulators, write half tensor.
include/ATen/native/cuda/PersistentSoftmax.cuh:// input_t=half,  acc_t=float, output_t=float => read half tensor, float accumulators, write float tensor.
include/ATen/native/cuda/PersistentSoftmax.cuh:// input_t_float, acc_t=float, output_t=half  => read float tensor, float accumulators, write half tensor.
include/ATen/native/cuda/Reduce.cuh:  static constexpr bool is_inp_out_type_half_or_chalf =
include/ATen/native/cuda/Reduce.cuh:      !(is_inp_out_type_half_or_chalf || is_inp_out_type_bfloat16);
include/ATen/native/cuda/Reduce.cuh:  static constexpr bool is_inp_out_type_half_or_chalf =
include/ATen/native/cuda/Reduce.cuh:      !(is_inp_out_type_half_or_chalf || is_inp_out_type_bfloat16);
include/ATen/native/cuda/SortingRadixSelect.cuh:    RadixType x = __half_as_ushort(v);
include/ATen/native/cuda/SortingRadixSelect.cuh:    return __ushort_as_half(v ^ mask);
include/ATen/native/cuda/KernelUtils.cuh:  __half* target_addr = reinterpret_cast<__half*>(tensor + index);
include/ATen/native/cuda/KernelUtils.cuh:  bool low_byte = (reinterpret_cast<std::uintptr_t>(target_addr) % sizeof(__half2) == 0);
include/ATen/native/cuda/KernelUtils.cuh:    __half2 value2;
include/ATen/native/cuda/KernelUtils.cuh:    value2.y = __int2half_rz(0);
include/ATen/native/cuda/KernelUtils.cuh:    atomicAdd(reinterpret_cast<__half2*>(target_addr), value2);
include/ATen/native/cuda/KernelUtils.cuh:    __half2 value2;
include/ATen/native/cuda/KernelUtils.cuh:    value2.x = __int2half_rz(0);
include/ATen/native/cuda/KernelUtils.cuh:    atomicAdd(reinterpret_cast<__half2*>(target_addr - 1), value2);
include/ATen/native/cuda/KernelUtils.cuh:        reinterpret_cast<__half*>(tensor) + index, static_cast<__half>(value));
include/ATen/native/cuda/GridSampler.cuh:// The bounds are passed as twice their value so that half-integer values
include/ATen/native/SpectralOpsUtils.h:// Therefore, in such cases, FFT libraries return only roughly half of the
include/ATen/native/SpectralOpsUtils.h:    void (*)(ScalarType dtype, IntArrayRef mirror_dims, IntArrayRef half_sizes,
include/ATen/native/SpectralOpsUtils.h:// In real-to-complex transform, cuFFT and MKL only fill half of the values
include/ATen/native/SpectralOpsUtils.h:// due to conjugate symmetry. This function fills in the other half of the full
include/ATen/native/ComplexHelper.h:    "view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: ", self.scalar_type());
include/ATen/native/Pool.h:                "pad should be at most half of kernel size, but got pad=",
include/ATen/native/Pool.h:              "pad should be smaller than or equal to half of kernel size, but got ",
include/ATen/native/Pool.h:              "pad should be smaller than or equal to half of kernel size, but got "
include/ATen/native/Math.h:  constexpr acc_t half = acc_t{0.5};
include/ATen/native/Math.h:  s -= half * b;
include/ATen/native/GridSampler.h:// The bounds are passed as twice their value so that half-integer values
include/ATen/native/DistributionTemplates.h://    auto actual = torch::empty({3, 3}, torch::half);
include/ATen/native/DistributionTemplates.h:// If random's uint64_t arithmetics produces 65503 as a random value after casting to torch::half it becomes 65504
include/ATen/native/DistributionTemplates.h:// available number for torch::half dtype.
include/ATen/native/DistributionTemplates.h:    // is half of the input variance
include/ATen/native/quantized/cpu/QuantUtils.h:    // Convert raw 16 bits half precision floating point number
include/ATen/MethodOperators.h:#include <ATen/ops/chalf_ops.h>
include/ATen/ops/_sparse_log_softmax_compositeexplicitautograd_dispatch.h:TORCH_API at::Tensor & _sparse_log_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_log_softmax_compositeexplicitautograd_dispatch.h:TORCH_API at::Tensor & _sparse_log_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_softmax_cuda_dispatch.h:TORCH_API at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_cuda_dispatch.h:TORCH_API at::Tensor & _softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_cuda_dispatch.h:TORCH_API at::Tensor & _softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/chalf_compositeimplicitautograd_dispatch.h:TORCH_API at::Tensor chalf(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format=c10::nullopt);
include/ATen/ops/_log_softmax.h:// aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/ops/_log_softmax.h:inline at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_log_softmax.h:    return at::_ops::_log_softmax::call(self, dim, half_to_float);
include/ATen/ops/_log_softmax.h:// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_log_softmax.h:inline at::Tensor & _log_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_log_softmax.h:    return at::_ops::_log_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_log_softmax.h:// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_log_softmax.h:inline at::Tensor & _log_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/ops/_log_softmax.h:    return at::_ops::_log_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")
include/ATen/ops/_softmax_ops.h:  static at::Tensor call(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_ops.h:  static at::Tensor redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")
include/ATen/ops/_softmax_ops.h:  static at::Tensor & call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_softmax_ops.h:  static at::Tensor & redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_sparse_softmax.h:// aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/ops/_sparse_softmax.h:inline at::Tensor _sparse_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_sparse_softmax.h:    return at::_ops::_sparse_softmax::call(self, dim, half_to_float);
include/ATen/ops/_sparse_softmax.h:// aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_sparse_softmax.h:inline at::Tensor & _sparse_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_sparse_softmax.h:    return at::_ops::_sparse_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_sparse_softmax.h:// aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_sparse_softmax.h:inline at::Tensor & _sparse_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/ops/_sparse_softmax.h:    return at::_ops::_sparse_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_log_softmax_native.h:void impl(const at::Tensor & self, int64_t dim, bool half_to_float, const at::Tensor & out);
include/ATen/ops/_log_softmax_native.h:void impl(const at::Tensor & self, int64_t dim, bool half_to_float, const at::Tensor & out);
include/ATen/ops/_sparse_log_softmax_native.h:TORCH_API at::Tensor & _sparse_log_softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_sparse_log_softmax_native.h:TORCH_API at::Tensor log_softmax_sparse_cpu(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_log_softmax_native.h:TORCH_API at::Tensor log_softmax_sparse_cuda(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_meta.h:    void meta(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")
include/ATen/ops/_sparse_softmax_ops.h:  static at::Tensor call(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_softmax_ops.h:  static at::Tensor redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")
include/ATen/ops/_sparse_softmax_ops.h:  static at::Tensor & call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_sparse_softmax_ops.h:  static at::Tensor & redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_softmax_cpu_dispatch.h:TORCH_API at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_cpu_dispatch.h:TORCH_API at::Tensor & _softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_cpu_dispatch.h:TORCH_API at::Tensor & _softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_log_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")
include/ATen/ops/_log_softmax_ops.h:  static at::Tensor call(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_ops.h:  static at::Tensor redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")
include/ATen/ops/_log_softmax_ops.h:  static at::Tensor & call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_log_softmax_ops.h:  static at::Tensor & redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_log_softmax_compositeexplicitautogradnonfunctional_dispatch.h:TORCH_API at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_log_softmax.h:// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/ops/_sparse_log_softmax.h:inline at::Tensor _sparse_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_sparse_log_softmax.h:    return at::_ops::_sparse_log_softmax::call(self, dim, half_to_float);
include/ATen/ops/_sparse_log_softmax.h:// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_sparse_log_softmax.h:inline at::Tensor & _sparse_log_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_sparse_log_softmax.h:    return at::_ops::_sparse_log_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_sparse_log_softmax.h:// aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_sparse_log_softmax.h:inline at::Tensor & _sparse_log_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/ops/_sparse_log_softmax.h:    return at::_ops::_sparse_log_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_sparse_log_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")
include/ATen/ops/_sparse_log_softmax_ops.h:  static at::Tensor call(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_log_softmax_ops.h:  static at::Tensor redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_log_softmax_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")
include/ATen/ops/_sparse_log_softmax_ops.h:  static at::Tensor & call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_sparse_log_softmax_ops.h:  static at::Tensor & redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/chalf_ops.h:struct TORCH_API chalf {
include/ATen/ops/chalf_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(name, "aten::chalf")
include/ATen/ops/chalf_ops.h:  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor")
include/ATen/ops/_sparse_softmax_native.h:TORCH_API at::Tensor & _sparse_softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_sparse_softmax_native.h:TORCH_API at::Tensor softmax_sparse_cpu(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_softmax_native.h:TORCH_API at::Tensor softmax_sparse_cuda(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_compositeexplicitautogradnonfunctional_dispatch.h:TORCH_API at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_softmax_compositeexplicitautograd_dispatch.h:TORCH_API at::Tensor & _sparse_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_sparse_softmax_compositeexplicitautograd_dispatch.h:TORCH_API at::Tensor & _sparse_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/chalf.h:#include <ATen/ops/chalf_ops.h>
include/ATen/ops/_log_softmax_meta_dispatch.h:TORCH_API at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_meta_dispatch.h:TORCH_API at::Tensor & _log_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_meta_dispatch.h:TORCH_API at::Tensor & _log_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_softmax_native.h:void impl(const at::Tensor & self, int64_t dim, bool half_to_float, const at::Tensor & out);
include/ATen/ops/_softmax_native.h:void impl(const at::Tensor & self, int64_t dim, bool half_to_float, const at::Tensor & out);
include/ATen/ops/_softmax_native.h:TORCH_API at::Tensor softmax_nested(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_native.h:TORCH_API at::Tensor mkldnn_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax.h:// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/ops/_softmax.h:inline at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_softmax.h:    return at::_ops::_softmax::call(self, dim, half_to_float);
include/ATen/ops/_softmax.h:// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_softmax.h:inline at::Tensor & _softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/ops/_softmax.h:    return at::_ops::_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_softmax.h:// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/ops/_softmax.h:inline at::Tensor & _softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/ops/_softmax.h:    return at::_ops::_softmax_out::call(self, dim, half_to_float, out);
include/ATen/ops/_softmax_meta.h:    void meta(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_cpu_dispatch.h:TORCH_API at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_cpu_dispatch.h:TORCH_API at::Tensor & _log_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_cpu_dispatch.h:TORCH_API at::Tensor & _log_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/_softmax_meta_dispatch.h:TORCH_API at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_meta_dispatch.h:TORCH_API at::Tensor & _softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_softmax_meta_dispatch.h:TORCH_API at::Tensor & _softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/ops/chalf_native.h:TORCH_API at::Tensor chalf(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format=c10::nullopt);
include/ATen/ops/_log_softmax_cuda_dispatch.h:TORCH_API at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_cuda_dispatch.h:TORCH_API at::Tensor & _log_softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float);
include/ATen/ops/_log_softmax_cuda_dispatch.h:TORCH_API at::Tensor & _log_softmax_outf(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out);
include/ATen/RegistrationDeclarations.h:Tensor chalf(const Tensor & self, c10::optional<MemoryFormat> memory_format); // {"schema": "aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor", "dispatch": "False", "default": "True"}
include/ATen/RegistrationDeclarations.h:Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float); // {"schema": "aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "dispatch": "True", "default": "True"}
include/ATen/RegistrationDeclarations.h:Tensor & _log_softmax_out(const Tensor & self, int64_t dim, bool half_to_float, Tensor & out); // {"schema": "aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)", "dispatch": "True", "default": "False"}
include/ATen/RegistrationDeclarations.h:Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float); // {"schema": "aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "dispatch": "True", "default": "True"}
include/ATen/RegistrationDeclarations.h:Tensor & _softmax_out(const Tensor & self, int64_t dim, bool half_to_float, Tensor & out); // {"schema": "aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)", "dispatch": "True", "default": "False"}
include/ATen/RegistrationDeclarations.h:Tensor _sparse_softmax(const Tensor & self, int64_t dim, bool half_to_float); // {"schema": "aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "dispatch": "True", "default": "False"}
include/ATen/RegistrationDeclarations.h:Tensor _sparse_log_softmax(const Tensor & self, int64_t dim, bool half_to_float); // {"schema": "aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "dispatch": "True", "default": "False"}
include/ATen/RegistrationDeclarations.h:Tensor & _sparse_softmax_out(const Tensor & self, int64_t dim, bool half_to_float, Tensor & out); // {"schema": "aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)", "dispatch": "True", "default": "True"}
include/ATen/RegistrationDeclarations.h:Tensor & _sparse_log_softmax_out(const Tensor & self, int64_t dim, bool half_to_float, Tensor & out); // {"schema": "aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)", "dispatch": "True", "default": "True"}
include/ATen/AccumulateType.h:struct AccumulateType<half, true> {
include/ATen/RedispatchFunctions.h:    // aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
include/ATen/RedispatchFunctions.h:    inline at::Tensor chalf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format=c10::nullopt) {
include/ATen/RedispatchFunctions.h:        return at::_ops::chalf::redispatch(dispatchKeySet, self, memory_format);
include/ATen/RedispatchFunctions.h:    // aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/RedispatchFunctions.h:    inline at::Tensor _log_softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_log_softmax::redispatch(dispatchKeySet, self, dim, half_to_float);
include/ATen/RedispatchFunctions.h:    // aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _log_softmax_out(c10::DispatchKeySet dispatchKeySet, at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_log_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _log_softmax_outf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_log_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/RedispatchFunctions.h:    inline at::Tensor _softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_softmax::redispatch(dispatchKeySet, self, dim, half_to_float);
include/ATen/RedispatchFunctions.h:    // aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _softmax_out(c10::DispatchKeySet dispatchKeySet, at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _softmax_outf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/RedispatchFunctions.h:    inline at::Tensor _sparse_softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_sparse_softmax::redispatch(dispatchKeySet, self, dim, half_to_float);
include/ATen/RedispatchFunctions.h:    // aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
include/ATen/RedispatchFunctions.h:    inline at::Tensor _sparse_log_softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_sparse_log_softmax::redispatch(dispatchKeySet, self, dim, half_to_float);
include/ATen/RedispatchFunctions.h:    // aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _sparse_softmax_out(c10::DispatchKeySet dispatchKeySet, at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_sparse_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _sparse_softmax_outf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_sparse_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _sparse_log_softmax_out(c10::DispatchKeySet dispatchKeySet, at::Tensor & out, const at::Tensor & self, int64_t dim, bool half_to_float) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_sparse_log_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/ATen/RedispatchFunctions.h:    // aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
include/ATen/RedispatchFunctions.h:    inline at::Tensor & _sparse_log_softmax_outf(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
include/ATen/RedispatchFunctions.h:        return at::_ops::_sparse_log_softmax_out::redispatch(dispatchKeySet, self, dim, half_to_float, out);
include/torch/csrc/distributed/rpc/utils.h:// we'd save at least half the data, and over a minimum hurdle.
include/torch/csrc/jit/runtime/register_ops_utils.h:// When a number is exactly halfway between two integers, python builtin round
include/torch/csrc/jit/runtime/register_ops_utils.h:// special halfway case. For positive 'x', round(x/2)*2 =
include/torch/csrc/jit/codegen/cuda/scheduler/utils.h:// Assume any only half of the register file is available to spend on buffers,
include/torch/csrc/jit/codegen/cuda/scheduler/utils.h:// T2[I0]     half
include/torch/csrc/jit/codegen/cuda/scheduler/pointwise.h: * T1[i0, b1, i2] half
include/torch/csrc/jit/codegen/cuda/fusion_segmenter.h:  DataType force_half_precision_type_;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:  std::array<std::array<double, 2>, 20> sum_tolerances_half = {
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:  double base_half_abs_tol = -1;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:  double base_half_rel_tol = -1;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& sum_tolerance_entry = tolerances.sum_tolerances_half;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_abs = tolerances.base_half_abs_tol;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_rel = tolerances.base_half_rel_tol;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& sum_tolerance_entry = tolerances.sum_tolerances_half;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_abs = tolerances.base_half_abs_tol;
include/torch/csrc/jit/codegen/cuda/test/test_gpu_validator.h:      const auto& base_rel = tolerances.base_half_rel_tol;
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:// This snippet enables half support in the jit. Following the pattern for
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:// with __half2float(). All mathematical operations are done on float
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:// converted to half with __float2half() when writing to a half tensor.
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:constexpr auto half_support_literal =
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:typedef __half half;
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:constexpr auto half_support_literal =
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:  struct __align__(2) __half {
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:    __host__ __device__ __half() { }
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:    __device__ __half __float2half(const float f) {
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:      __half val;
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:    __device__ float __half2float(const __half h) {
include/torch/csrc/jit/codegen/fuser/cuda/resource_strings.h:typedef __half half;
include/torch/csrc/jit/tensorexpr/half_support.h:    inserted_half_casts_.insert(ret);
include/torch/csrc/jit/tensorexpr/half_support.h:      inserted_half_casts_.insert(new_val);
include/torch/csrc/jit/tensorexpr/half_support.h:      inserted_half_casts_.insert(new_val);
include/torch/csrc/jit/tensorexpr/half_support.h:    // just don't allow half casts we didn't insert.
include/torch/csrc/jit/tensorexpr/half_support.h:      if (inserted_half_casts_.count(v) < 1) {
include/torch/csrc/jit/tensorexpr/half_support.h:      auto from_half = isHalf(cast_child->src_value());
include/torch/csrc/jit/tensorexpr/half_support.h:      // Cannot simplify the double(float(half)) to double(half) as NNC does
include/torch/csrc/jit/tensorexpr/half_support.h:      auto not_cast_half_to_doulbe = !(cast_to_double && from_half);
include/torch/csrc/jit/tensorexpr/half_support.h:          cast_child->dtype().is_floating_point() && not_cast_half_to_doulbe) {
include/torch/csrc/jit/tensorexpr/half_support.h:  std::unordered_set<ExprPtr> inserted_half_casts_;
include/torch/csrc/api/include/torch/python.h:      .def("half", [](ModuleType& module) { module.to(kFloat16); })
include/torch/csrc/lazy/generated/LazyIr.h:  LogSoftmax(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float, std::vector<torch::lazy::Shape>&& shapes)
include/torch/csrc/lazy/generated/LazyIr.h:              torch::lazy::MHash(dim, half_to_float)),
include/torch/csrc/lazy/generated/LazyIr.h:        half_to_float(half_to_float)
include/torch/csrc/lazy/generated/LazyIr.h:    ss << ", half_to_float=" << half_to_float;
include/torch/csrc/lazy/generated/LazyIr.h:  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float) const {
include/torch/csrc/lazy/generated/LazyIr.h:        this->half_to_float == half_to_float);
include/torch/csrc/lazy/generated/LazyIr.h:    arguments.emplace_back("half_to_float", half_to_float);
include/torch/csrc/lazy/generated/LazyIr.h:  bool half_to_float;
include/torch/csrc/lazy/generated/LazyIr.h:  Softmax(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float, std::vector<torch::lazy::Shape>&& shapes)
include/torch/csrc/lazy/generated/LazyIr.h:              torch::lazy::MHash(dim, half_to_float)),
include/torch/csrc/lazy/generated/LazyIr.h:        half_to_float(half_to_float)
include/torch/csrc/lazy/generated/LazyIr.h:    ss << ", half_to_float=" << half_to_float;
include/torch/csrc/lazy/generated/LazyIr.h:  bool CanBeReused(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float) const {
include/torch/csrc/lazy/generated/LazyIr.h:        this->half_to_float == half_to_float);
include/torch/csrc/lazy/generated/LazyIr.h:    arguments.emplace_back("half_to_float", half_to_float);
include/torch/csrc/lazy/generated/LazyIr.h:  bool half_to_float;
include/torch/csrc/lazy/generated/LazyNativeFunctions.h:static at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/torch/csrc/lazy/generated/LazyNativeFunctions.h:static at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float);
include/fp16/fp16.h: * Convert a 16-bit floating-point number in IEEE half-precision format, in bit representation, to
include/fp16/fp16.h:	 * Extend the half-precision floating-point number to 32 bits and shift to the upper part of the 32-bit word:
include/fp16/fp16.h:	 * Renorm shift is the number of bits to shift mantissa left to make the half-precision number normalized.
include/fp16/fp16.h:	 * Iff half-precision number has exponent of 15, the addition overflows it into bit 31,
include/fp16/fp16.h:	 *                   0x7F800000 if the half-precision number had exponent of 15 (i.e. was NaN or infinity)
include/fp16/fp16.h:	 *                0xFFFFFFFF if the half-precision number was zero (+0.0h or -0.0h)
include/fp16/fp16.h:	 *    (0x7F for single-precision number less 0xF for half-precision number).
include/fp16/fp16.h: * Convert a 16-bit floating-point number in IEEE half-precision format, in bit representation, to
include/fp16/fp16.h:	 * Extend the half-precision floating-point number to 32 bits and shift to the upper part of the 32-bit word:
include/fp16/fp16.h:	 * - The exponent needs to be corrected by the difference in exponent bias between single-precision and half-precision
include/fp16/fp16.h:	 *   Therefore, if the biased exponent of the half-precision input was 0x1F (max possible value), the biased exponent
include/fp16/fp16.h:	 * Convert denormalized half-precision inputs into single-precision results (always normalized).
include/fp16/fp16.h:	 * Now, remember that denormalized half-precision numbers are represented as:
include/fp16/fp16.h:	 * The trick is to construct a normalized single-precision number with the same mantissa and thehalf-precision input
include/fp16/fp16.h:	 * Therefore, when the biased exponent is 126, a unit change in the mantissa of the input denormalized half-precision
include/fp16/fp16.h:	 * The last step is to adjust the bias of the constructed single-precision number. When the input half-precision number
include/fp16/fp16.h:	 * the input half-precision number.
include/fp16/fp16.h: * IEEE half-precision format, in bit representation.
include/fp16/fp16.h: * Convert a 16-bit floating-point number in ARM alternative half-precision format, in bit representation, to
include/fp16/fp16.h:	 * Extend the half-precision floating-point number to 32 bits and shift to the upper part of the 32-bit word:
include/fp16/fp16.h:	 * Renorm shift is the number of bits to shift mantissa left to make the half-precision number normalized.
include/fp16/fp16.h:	 *                0xFFFFFFFF if the half-precision number was zero (+0.0h or -0.0h)
include/fp16/fp16.h:	 *    (0x7F for single-precision number less 0xF for half-precision number).
include/fp16/fp16.h: * Convert a 16-bit floating-point number in ARM alternative half-precision format, in bit representation, to
include/fp16/fp16.h:	 * Extend the half-precision floating-point number to 32 bits and shift to the upper part of the 32-bit word:
include/fp16/fp16.h:	 * Next, the exponent is adjusted for the difference in exponent bias between single-precision and half-precision
include/fp16/fp16.h:	 * half-precision exponent is 0x1F and after the adjustment is can not exceed 0x8F < 0xFE (largest single-precision
include/fp16/fp16.h:	 * Convert denormalized half-precision inputs into single-precision results (always normalized).
include/fp16/fp16.h:	 * Now, remember that denormalized half-precision numbers are represented as:
include/fp16/fp16.h:	 * The trick is to construct a normalized single-precision number with the same mantissa and thehalf-precision input
include/fp16/fp16.h:	 * Therefore, when the biased exponent is 126, a unit change in the mantissa of the input denormalized half-precision
include/fp16/fp16.h:	 * The last step is to adjust the bias of the constructed single-precision number. When the input half-precision number
include/fp16/fp16.h:	 * the input half-precision number.
include/fp16/fp16.h: * ARM alternative half-precision format, in bit representation.
include/fp16/avx.py:def fp16_alt_xmm_to_fp32_xmm(xmm_half):
include/fp16/avx.py:	VPUNPCKLWD(xmm_word, xmm_zero, xmm_half)
include/fp16/avx.py:	xmm_shl1_half = XMMRegister()
include/fp16/avx.py:	VPADDW(xmm_shl1_half, xmm_half, xmm_half)
include/fp16/avx.py:	VPUNPCKLWD(xmm_denorm_nonsign, xmm_shl1_half, magic_mask)
include/fp16/avx2.py:def fp16_alt_xmm_to_fp32_ymm(xmm_half):
include/fp16/avx2.py:	ymm_half = YMMRegister()
include/fp16/avx2.py:	VPERMQ(ymm_half, xmm_half.as_ymm, 0b01010000)
include/fp16/avx2.py:	VPUNPCKLWD(ymm_word, ymm_zero, ymm_half)
include/fp16/avx2.py:	ymm_shl1_half = YMMRegister()
include/fp16/avx2.py:	VPADDW(ymm_shl1_half, ymm_half, ymm_half)
include/fp16/avx2.py:	VPUNPCKLWD(ymm_denorm_nonsign, ymm_shl1_half, magic_mask)
include/fp16/psimd.h:PSIMD_INTRINSIC psimd_f32 fp16_ieee_to_fp32_psimd(psimd_u16 half) {
include/fp16/psimd.h:	const psimd_u32 word = (psimd_u32) psimd_interleave_lo_u16(psimd_zero_u16(), half);
include/fp16/psimd.h:	const psimd_f32 denorm_nonsign = psimd_sub_f32((psimd_f32) psimd_interleave_lo_u16(half + half, magic_mask), magic_bias);
include/fp16/psimd.h:PSIMD_INTRINSIC psimd_f32x2 fp16_ieee_to_fp32x2_psimd(psimd_u16 half) {
include/fp16/psimd.h:	const psimd_u32 word_lo = (psimd_u32) psimd_interleave_lo_u16(psimd_zero_u16(), half);
include/fp16/psimd.h:	const psimd_u32 word_hi = (psimd_u32) psimd_interleave_hi_u16(psimd_zero_u16(), half);
include/fp16/psimd.h:	const psimd_u16 shl1_half = half + half;
include/fp16/psimd.h:	const psimd_f32 denorm_nonsign_lo = psimd_sub_f32((psimd_f32) psimd_interleave_lo_u16(shl1_half, magic_mask), magic_bias);
include/fp16/psimd.h:	const psimd_f32 denorm_nonsign_hi = psimd_sub_f32((psimd_f32) psimd_interleave_hi_u16(shl1_half, magic_mask), magic_bias);
include/fp16/psimd.h:PSIMD_INTRINSIC psimd_f32 fp16_alt_to_fp32_psimd(psimd_u16 half) {
include/fp16/psimd.h:	const psimd_u32 word = (psimd_u32) psimd_interleave_lo_u16(psimd_zero_u16(), half);
include/fp16/psimd.h:PSIMD_INTRINSIC psimd_f32x2 fp16_alt_to_fp32x2_psimd(psimd_u16 half) {
include/fp16/psimd.h:	const psimd_u32 word_lo = (psimd_u32) psimd_interleave_lo_u16(psimd_zero_u16(), half);
include/fp16/psimd.h:	const psimd_u32 word_hi = (psimd_u32) psimd_interleave_hi_u16(psimd_zero_u16(), half);
include/asmjit/core/archcommons.h:  //! 16-bit floating point (half precision), specified as `.f16` in assembly.
include/asmjit/core/archtraits.h:  //! Describes 'half' (most likely 16-bit word).
include/asmjit/core/cpuinfo.h:      kFP16CONV,                 //!< CPU has FP16 (half-float) conversion.
include/asmjit/x86/x86operand.h:  //! Casts this register to a register that has half the size (or XMM if it's already XMM).
include/asmjit/x86/x86operand.h:  inline Vec half() const noexcept {
include/asmjit/x86/x86operand.h:  //! Casts this register to a register that has half the size (XMM).
include/asmjit/x86/x86operand.h:  inline Xmm half() const noexcept { return Xmm(id()); }
include/asmjit/x86/x86operand.h:  //! Casts this register to a register that has half the size (XMM).
include/asmjit/x86/x86operand.h:  inline Xmm half() const noexcept { return Xmm(id()); }
include/asmjit/x86/x86operand.h:  //! Casts this register to a register that has half the size (YMM).
include/asmjit/x86/x86operand.h:  inline Ymm half() const noexcept { return Ymm(id()); }
include/asmjit/arm/armoperand.h:    //! Halfword elements grouped by 2 halfwords (H2).
include/asmjit/arm/a64instdb.h:  //! SIMD element access of half-words can only be used with v0..15.
include/asmjit/arm/a64utils.h://! Returns true if the given half precision floating point `val` can be encoded as ARM IMM8 value, which represents
include/fbgemm/Types.h:static inline float16 cpu_float2half_rn(float f) {
include/fbgemm/Types.h:static inline float16 cpu_float2half_rz(float f) {
include/fbgemm/Types.h:static inline float cpu_half2float(float16 h) {
include/fbgemm/FbgemmEmbedding.h:      std::int64_t counter_halflife)>; // frequency adjust happens only after
include/fbgemm/QuantUtils.h:  // in exactly halfway cases.
include/fbgemm/QuantUtils.h: * Convert float or half inputs to rowwise quantized (8-bit) outputs.
include/fbgemm/QuantUtils.h: * Convert fused rowwise quantized (8-bit) inputs to float or half outputs.
include/fbgemm/FbgemmFP16.h:    return cpu_float2half_rn(fp16);
include/caffe2/core/types.h:// at::Half is defined in c10/util/Half.h. Currently half float operators are
include/caffe2/core/types.h:// The reason we do not directly use the cuda __half data type is because that
include/caffe2/core/types.h:// with the cuda __half data type, but will allow us to refer to the data type
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:    bool out_sb_half = std::is_same<Tsb, at::Half>::value;
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      if (out_sb_half) {
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      bool is_half = std::is_same<T, at::Half>::value;
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      CAFFE_ENFORCE(is_half);
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:        if (out_sb_half) {
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:    bool in_sb_half = std::is_same<Tsb, at::Half>::value;
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      if (in_sb_half) {
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      bool is_half = std::is_same<T, at::Half>::value;
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      CAFFE_ENFORCE(is_half);
include/caffe2/operators/fused_rowwise_8bit_conversion_ops.h:        if (in_sb_half) {
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void affine(constant half4* scale[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   constant half4* shift[[buffer(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 scale_c = scale[gid.z % divRoundUp(C, 4)];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 shift_c = shift[gid.z % divRoundUp(C, 4)];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 x = in.read(gid_, gid.z);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 y = scale_c * x + shift_c;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void affine_nonarray(constant half4* scale[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            constant half4* shift[[buffer(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 scale_c = scale[0];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 shift_c = shift[0];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 x = in.read(gid);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 y = scale_c * x + shift_c;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void prelu_nonshared(constant half4* weights[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 w = channel_shared ? half4(weights[0][0], weights[0][0], weights[0][0], weights[0][0])
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 x = in.read(gid_, gid.z);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 y = select(x * w, x, x > 0.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void prelu_nonshared_nonarray(constant half4* weights[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                     texture2d<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                     texture2d<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 w = channel_shared ? half4(weights[0][0], weights[0][0], weights[0][0], weights[0][0])
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 x = in.read(gid);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 y = select(x * w, x, x > 0.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                          constant half4* weights[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                          constant half4* bias[[buffer(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                          constant half4* preluWeights[[ buffer(2), function_constant(instance_norm_has_prelu) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                          texture2d_array<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                          texture2d_array<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 w;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:        w = channel_shared ? half4(preluWeights[0][0]) : preluWeights[c];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:            half4 scaled =
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:            static_cast<half4>(static_cast<AccT>(in.read(ushort2(x, y), gid.z)) * scale + shift);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                   constant half4* weights[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                   constant half4* bias[[buffer(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                   constant half4* preluWeights[[ buffer(2), function_constant(instance_norm_has_prelu) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                   texture2d<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                   texture2d<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 w;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:        w = channel_shared ? half4(preluWeights[0][0]) : preluWeights[0];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:            half4 scaled = static_cast<half4>(static_cast<AccT>(in.read(ushort2(x, y))) * scale + shift);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                               texture2d_array<half, access::write> out[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 trns;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                        texture2d<half, access::write> out[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 trns;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void copy_metal_to_nchw(texture2d_array<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 cs = in.read(gid.xy, gid.z);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void copy_metal_to_nchw_nonarray(texture2d<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 cs = in.read(gid.xy);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void convtranspose_upscale(texture2d_array<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                  texture2d_array<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 zero(0.0h, 0.0h, 0.0h, 0.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:            half4 input = in.read(in_pos, gid.z);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::read> ina[[ texture(0), function_constant(has_in_arr) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::read> in[[ texture(0), function_constant(has_in_tex) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::write> outa[[ texture(1), function_constant(has_out_arr) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::write> out[[ texture(1), function_constant(has_out_tex) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   constant half4* bias[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                half4 components(0, 0, 0, 0);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:        outa.write(static_cast<half4>(val), gid.xy, gid.z);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:        out.write(static_cast<half4>(val), gid.xy);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                constant half* mean[[buffer(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                constant half4* noise[[buffer(2)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                texture2d<half, access::write> out[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 mean_half(mean[0], mean[1], mean[2], 0.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    const half4 input_noise = noise[input_noise_idx];
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 input_half = static_cast<half4>(input);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    out.write(input_half - mean_half + input_noise, gid);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void deprocess_stylizer(texture2d<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                               constant half* mean[[buffer(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 value = in.read(gid);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 mean_h(mean[0], mean[1], mean[2], 0.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 min_h(0.0h, 0.0h, 0.0h, 255.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 max_h(255.0h, 255.0h, 255.0h, 255.0h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 clamped = clamp(value + mean_h, min_h, max_h);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void reflection_padding_nonarray(texture2d<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                        texture2d<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void reflection_padding(texture2d_array<half, access::read> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                               texture2d_array<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void bilinear_upsample(texture2d<half, access::sample> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                              texture2d<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    half4 value = in.sample(sampler, static_cast<float2>(src));
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void elementwise_mul(texture2d<half, access::read> in0[[texture(0), function_constant(in0_is_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::read> ina0[[texture(0), function_constant(in0_is_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d<half, access::write> out[[texture(2), function_constant(in0_is_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::write> outa[[texture(2), function_constant(in0_is_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void elementwise_sub(texture2d<half, access::read> in0[[texture(0), function_constant(in0_is_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::read> ina0[[texture(0), function_constant(in0_is_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d<half, access::write> out[[texture(2), function_constant(in0_is_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::write> outa[[texture(2), function_constant(in0_is_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void elementwise_add_nonarray(texture2d<half, access::read> in0[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                     texture2d<half, access::read> in1[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                     texture2d<half, access::write> out[[texture(2)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void elementwise_add(texture2d_array<half, access::read> in0[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::read> in1[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                            texture2d_array<half, access::write> out[[texture(2)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::read> in0[[ texture(0), function_constant(has_in0_tex) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::read> in1[[ texture(1), function_constant(has_in1_tex) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::read> in2[[ texture(2), function_constant(has_in2_tex) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::read> in3[[ texture(3), function_constant(has_in3_tex) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::read> ina0[[ texture(0), function_constant(has_in0_array) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::read> ina1[[ texture(1), function_constant(has_in1_array) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::read> ina2[[ texture(2), function_constant(has_in2_array) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::read> ina3[[ texture(3), function_constant(has_in3_array) ]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d<half, access::write> out[[texture(5),
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                   texture2d_array<half, access::write> outa[[texture(5),
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:  half4 value;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:using RoIT = half;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:using RoIT4 = half4;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void roi_warp(texture2d_array<half, access::sample> ina[[texture(0), function_constant(rw_has_in_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                     texture2d<half, access::sample> in[[texture(0), function_constant(rw_has_in_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                     texture2d_array<half, access::write> outa[[texture(1), function_constant(rw_has_out_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                     texture2d<half, access::write> out[[texture(1), function_constant(rw_has_out_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                     constant half4* rois[[buffer(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:  const half spatial_scale = half(ushort_arg_0) / 10000;
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    outa.write(static_cast<half4>(output_val), gid.xy, gid.z);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    out.write(static_cast<half4>(output_val), gid.xy);
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void resize_nearest(texture2d_array<half, access::sample> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                           texture2d_array<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:kernel void resize_nearest_nonarray(texture2d<half, access::sample> in[[texture(0)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:                                    texture2d<half, access::write> out[[texture(1)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    texture2d<half, access::read> in0[[texture(0), function_constant(in0_is_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    texture2d_array<half, access::read> ina0[[texture(0), function_constant(in0_is_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    texture2d<half, access::write> out[[texture(1), function_constant(in0_is_tex)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:    texture2d_array<half, access::write> outa[[texture(1), function_constant(in0_is_arr)]],
include/caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h:  half4 value;
include/caffe2/mobile/contrib/libvulkan-stub/include/vulkan/vulkan.h:#define VK_AMD_gpu_shader_half_float 1
include/caffe2/mobile/contrib/libvulkan-stub/include/vulkan/vulkan.h:#define VK_AMD_GPU_SHADER_HALF_FLOAT_EXTENSION_NAME "VK_AMD_gpu_shader_half_float"
include/caffe2/mobile/contrib/libopencl-stub/include/CL/cl_platform.h:typedef unsigned __int16        cl_half;
include/caffe2/mobile/contrib/libopencl-stub/include/CL/cl_platform.h:typedef uint16_t        cl_half     __attribute__((aligned(2)));
include/caffe2/experiments/operators/fully_connected_op_prune.h:    // at most mask half of the matrix at time
include/caffe2/sgd/adagrad_op.h:          /*counter_halflife=*/0);
include/caffe2/sgd/adagrad_op.h:          /*counter_halflife=*/0);
include/caffe2/sgd/adagrad_op.h:        counter_halflife_(
include/caffe2/sgd/adagrad_op.h:            this->template GetSingleArgument<int64_t>("counter_halflife", -1)) {
include/caffe2/sgd/adagrad_op.h:            << " counter_halflife=" << counter_halflife_;
include/caffe2/sgd/adagrad_op.h:    const auto* count = counter_halflife_ == -1
include/caffe2/sgd/adagrad_op.h:          (counter_halflife_ > 0) ? count : nullptr,
include/caffe2/sgd/adagrad_op.h:          counter_halflife_);
include/caffe2/sgd/adagrad_op.h:          (counter_halflife_ > 0) ? count : nullptr,
include/caffe2/sgd/adagrad_op.h:          counter_halflife_);
include/caffe2/sgd/adagrad_op.h:      float freq = (counter_halflife_ > 0 && count[idx] > 0)
include/caffe2/sgd/adagrad_op.h:          ? counter_halflife_ / count[idx]
include/caffe2/sgd/adagrad_op.h:  const int64_t counter_halflife_;
include/caffe2/sgd/rowwise_adagrad_fused.h:        counter_halflife_(
include/caffe2/sgd/rowwise_adagrad_fused.h:            this->template GetSingleArgument<int64_t>("counter_halflife", -1)) {
include/caffe2/sgd/rowwise_adagrad_fused.h:            << " counter_halflife=" << counter_halflife_
include/caffe2/sgd/rowwise_adagrad_fused.h:    const auto* count = counter_halflife_ == -1
include/caffe2/sgd/rowwise_adagrad_fused.h:        counter_halflife_,
include/caffe2/sgd/rowwise_adagrad_fused.h:      T counter_halflife,
include/caffe2/sgd/rowwise_adagrad_fused.h:        float freq = (counter_halflife > 0 && count[idx] > 0)
include/caffe2/sgd/rowwise_adagrad_fused.h:        ? counter_halflife / count[idx]
include/caffe2/sgd/rowwise_adagrad_fused.h:      T counter_halflife,
include/caffe2/sgd/rowwise_adagrad_fused.h:          /*counter_halflife=*/-1,
include/caffe2/sgd/rowwise_adagrad_fused.h:          counter_halflife,
include/caffe2/sgd/rowwise_adagrad_fused.h:  T counter_halflife_;
include/caffe2/sgd/rowwise_adagrad_fused.h:        counter_halflife_(
include/caffe2/sgd/rowwise_adagrad_fused.h:            this->template GetSingleArgument<int64_t>("counter_halflife", -1)) {
include/caffe2/sgd/rowwise_adagrad_fused.h:            << " counter_halflife=" << counter_halflife_
include/caffe2/sgd/rowwise_adagrad_fused.h:    const auto* count = counter_halflife_ == -1
include/caffe2/sgd/rowwise_adagrad_fused.h:        counter_halflife_,
include/caffe2/sgd/rowwise_adagrad_fused.h:      T counter_halflife,
include/caffe2/sgd/rowwise_adagrad_fused.h:        float freq = (counter_halflife > 0 && count[idx] > 0)
include/caffe2/sgd/rowwise_adagrad_fused.h:        ? counter_halflife / count[idx]
include/caffe2/sgd/rowwise_adagrad_fused.h:      T counter_halflife,
include/caffe2/sgd/rowwise_adagrad_fused.h:          /*counter_halflife=*/-1,
include/caffe2/sgd/rowwise_adagrad_fused.h:          counter_halflife,
include/caffe2/sgd/rowwise_adagrad_fused.h:  T counter_halflife_;
include/caffe2/sgd/rowwise_adagrad_fused.h:        counter_halflife_(
include/caffe2/sgd/rowwise_adagrad_fused.h:            this->template GetSingleArgument<int64_t>("counter_halflife", -1)) {
include/caffe2/sgd/rowwise_adagrad_fused.h:            << " counter_halflife=" << counter_halflife_
include/caffe2/sgd/rowwise_adagrad_fused.h:    const auto* count = counter_halflife_ == -1
include/caffe2/sgd/rowwise_adagrad_fused.h:        float freq = (counter_halflife_ > 0 && count[idx] > 0)
include/caffe2/sgd/rowwise_adagrad_fused.h:        ? counter_halflife_ / count[idx]
include/caffe2/sgd/rowwise_adagrad_fused.h:  T counter_halflife_;
include/caffe2/sgd/rowwise_counter.h:        counter_halflife_(
include/caffe2/sgd/rowwise_counter.h:            this->template GetSingleArgument<int64_t>("counter_halflife", -1)),
include/caffe2/sgd/rowwise_counter.h:    if (counter_halflife_ > 0) {
include/caffe2/sgd/rowwise_counter.h:      counter_neg_log_rho_ = std::log(2.0) / counter_halflife_;
include/caffe2/sgd/rowwise_counter.h:    if (counter_halflife_ <= 0) {
include/caffe2/sgd/rowwise_counter.h:  int64_t counter_halflife_;
include/caffe2/quantization/server/fb_fc_packed_op.h:            fbgemm::cpu_half2float(W->pmat()[i]);
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:    float halfRes = offsetv[i];
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:    if (std::isinf(halfRes)) {
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:      if (halfRes > 0) {
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:        halfRes = qmax;
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:        halfRes = qmin;
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:    if (halfRes > qmax) {
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:      halfRes = qmax;
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:    if (halfRes < qmin) {
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:      halfRes = qmin;
include/caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h:    out[i] = static_cast<uint8_t>(halfRes);
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:        float halfRes = offsetv[i];
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:        halfRes = round(halfRes);
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:        if (std::isinf(halfRes)) {
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:          if (halfRes > 0) {
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:            halfRes = qmax;
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:            halfRes = qmin;
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:        if (halfRes > qmax) {
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:          halfRes = qmax;
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:        if (halfRes < qmin) {
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:          halfRes = qmin;
include/caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h:        Y_uint8_data[i] = static_cast<uint8_t>(halfRes);
include/caffe2/serialize/crc_alt.h:// - crc32_halfbyte has its own small lookup table
include/caffe2/serialize/crc_alt.h:/// compute CRC32 (half-byte algoritm)
include/caffe2/serialize/crc_alt.h:uint32_t crc32_halfbyte(const void* data, size_t length, uint32_t previousCrc32 = 0);
include/caffe2/serialize/crc_alt.h:// - crc32_halfbyte has its own small lookup table
include/caffe2/serialize/crc_alt.h:/// compute CRC32 (half-byte algoritm)
include/caffe2/serialize/crc_alt.h:uint32_t crc32_halfbyte(const void* data, size_t length, uint32_t previousCrc32)
include/caffe2/serialize/crc_alt.h:  /// look-up table for half-byte, same as crc32Lookup[0][16*i]
include/caffe2/serialize/crc_alt.h:  return crc32_halfbyte(data, length, previousCrc32);
include/caffe2/serialize/crc_alt.h:    // note: the first number of every second row corresponds to the half-byte look-up table !
include/c10/core/Device.h:    // half of the resulting integer.
include/c10/core/ScalarType.h:  _(half, Half) /* 0 */                \
include/c10/util/Float8.h: * 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/Float8.h://   inline C10_HOST_DEVICE Float8(const sycl::half& value);
include/c10/util/Float8.h://   inline C10_HOST_DEVICE operator sycl::half() const;
include/c10/util/Half-inl.h:      x(__half_as_short(__float2half(value)))
include/c10/util/Half-inl.h:      x(sycl::bit_cast<uint16_t>(sycl::half(value)))
include/c10/util/Half-inl.h:  return __half2float(*reinterpret_cast<const __half*>(&x));
include/c10/util/Half-inl.h:  return float(sycl::bit_cast<sycl::half>(x));
include/c10/util/Half-inl.h:inline C10_HOST_DEVICE Half::Half(const __half& value) {
include/c10/util/Half-inl.h:inline C10_HOST_DEVICE Half::operator __half() const {
include/c10/util/Half-inl.h:  return *reinterpret_cast<const __half*>(&x);
include/c10/util/Half-inl.h:inline C10_HOST_DEVICE Half::Half(const sycl::half& value) {
include/c10/util/Half-inl.h:inline C10_HOST_DEVICE Half::operator sycl::half() const {
include/c10/util/Half-inl.h:  return *reinterpret_cast<const sycl::half*>(&x);
include/c10/util/Half-inl.h:  return __ldg(reinterpret_cast<const __half*>(ptr));
include/c10/util/Half-inl.h:  return -sycl::bit_cast<sycl::half>(a);
include/c10/util/irange.h:/// Creates an integer range for the half-open interval [begin, end)
include/c10/util/irange.h:/// Creates an integer range for the half-open interval [0, end)
include/c10/util/Foat8-inl.h://       x(sycl::bit_cast<uint16_t>(sycl::half(value)))
include/c10/util/Foat8-inl.h://   return float(sycl::bit_cast<sycl::half>(x));
include/c10/util/Foat8-inl.h:// inline C10_HOST_DEVICE Half::Half(const sycl::half& value) {
include/c10/util/Foat8-inl.h:// inline C10_HOST_DEVICE Half::operator sycl::half() const {
include/c10/util/Foat8-inl.h://   return *reinterpret_cast<const sycl::half*>(&x);
include/c10/util/Foat8-inl.h://   return -sycl::bit_cast<sycl::half>(a);
include/c10/util/Half.h:/// Defines the Half type (half-precision floating-point) including conversions
include/c10/util/Half.h:/// performing the operation in float32, instead of using CUDA half intrinsics.
include/c10/util/Half.h:/// element-wise kernels, and the half intrinsics aren't efficient on all GPUs.
include/c10/util/Half.h:/// If you are writing a compute bound kernel, you can use the CUDA half
include/c10/util/Half.h: * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/Half.h:   * Extend the half-precision floating-point number to 32 bits and shift to the
include/c10/util/Half.h:   * half-precision number normalized. If the initial number is normalized, some
include/c10/util/Half.h:   * Iff half-precision number has exponent of 15, the addition overflows
include/c10/util/Half.h:   * into 1. Thus inf_nan_mask == 0x7F800000 if the half-precision number
include/c10/util/Half.h:   * 0xFFFFFFFF if the half-precision number was zero (+0.0h or -0.0h)
include/c10/util/Half.h:   * for half-precision number).
include/c10/util/Half.h: * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/Half.h:   * Extend the half-precision floating-point number to 32 bits and shift to the
include/c10/util/Half.h:   * between single-precision and half-precision formats (0x7F - 0xF = 0x70)
include/c10/util/Half.h:   * exponent of the half-precision input was 0x1F (max possible value), the
include/c10/util/Half.h:   * Convert denormalized half-precision inputs into single-precision results
include/c10/util/Half.h:   * Now, remember that denormalized half-precision numbers are represented as:
include/c10/util/Half.h:   * same mantissa and thehalf-precision input and with an exponent which would
include/c10/util/Half.h:   * half-precision number causes a change of the constructud single-precision
include/c10/util/Half.h:   * number. When the input half-precision number is zero, the constructed
include/c10/util/Half.h:   * half-precision number.
include/c10/util/Half.h: * 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/Half.h:  inline C10_HOST_DEVICE Half(const __half& value);
include/c10/util/Half.h:  inline C10_HOST_DEVICE operator __half() const;
include/c10/util/Half.h:  inline C10_HOST_DEVICE Half(const sycl::half& value);
include/c10/util/Half.h:  inline C10_HOST_DEVICE operator sycl::half() const;
include/c10/util/copysign.h:// copysign faster for the half-precision types
include/c10/util/copysign.h:// Implement copysign for half precision floats using bit ops
include/c10/util/copysign.h:// Sign is the most significant bit for both half and bfloat16 types
include/c10/util/float8.h:/// Defines the Half type (half-precision floating-point) including conversions
include/c10/util/float8.h:/// performing the operation in float32, instead of using CUDA half intrinsics.
include/c10/util/float8.h:/// element-wise kernels, and the half intrinsics aren't efficient on all GPUs.
include/c10/util/float8.h:/// If you are writing a compute bound kernel, you can use the CUDA half
include/c10/util/float8.h: * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/float8.h:   * Extend the half-precision floating-point number to 32 bits and shift to the
include/c10/util/float8.h:   * half-precision number normalized. If the initial number is normalized, some
include/c10/util/float8.h:   * Iff half-precision number has exponent of 15, the addition overflows
include/c10/util/float8.h:   * into 1. Thus inf_nan_mask == 0x7F800000 if the half-precision number
include/c10/util/float8.h:   * 0xFFFFFFFF if the half-precision number was zero (+0.0h or -0.0h)
include/c10/util/float8.h:   * for half-precision number).
include/c10/util/float8.h: * Convert a 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/float8.h:   * Extend the half-precision floating-point number to 32 bits and shift to the
include/c10/util/float8.h:   * between single-precision and half-precision formats (0x7F - 0xF = 0x70)
include/c10/util/float8.h:   * exponent of the half-precision input was 0x1F (max possible value), the
include/c10/util/float8.h:   * Convert denormalized half-precision inputs into single-precision results
include/c10/util/float8.h:   * Now, remember that denormalized half-precision numbers are represented as:
include/c10/util/float8.h:   * same mantissa and thehalf-precision input and with an exponent which would
include/c10/util/float8.h:   * half-precision number causes a change of the constructud single-precision
include/c10/util/float8.h:   * number. When the input half-precision number is zero, the constructed
include/c10/util/float8.h:   * half-precision number.
include/c10/util/float8.h: * 16-bit floating-point number in IEEE half-precision format, in bit
include/c10/util/float8.h:  inline C10_HOST_DEVICE Half(const __half& value);
include/c10/util/float8.h:  inline C10_HOST_DEVICE operator __half() const;
include/c10/util/float8.h:  inline C10_HOST_DEVICE Half(const sycl::half& value);
include/c10/util/float8.h:  inline C10_HOST_DEVICE operator sycl::half() const;
include/c10/util/Float8-inl.h://       x(sycl::bit_cast<uint16_t>(sycl::half(value)))
include/c10/util/Float8-inl.h://   return float(sycl::bit_cast<sycl::half>(x));
include/c10/util/Float8-inl.h:// inline C10_HOST_DEVICE Half::Half(const sycl::half& value) {
include/c10/util/Float8-inl.h:// inline C10_HOST_DEVICE Half::operator sycl::half() const {
include/c10/util/Float8-inl.h://   return *reinterpret_cast<const sycl::half*>(&x);
include/c10/util/Float8-inl.h://   return -sycl::bit_cast<sycl::half>(a);
_inductor/codegen/cpp_prefix.h:typedef at::Half half;
_inductor/codegen/cpp.py:    torch.float16: "half",
_inductor/codegen/cpp.py:            # Since load promotes all half-precision inputs to float, constants
_inductor/decomposition.py:    if x.dtype == torch.float16 or x.dtype == torch.half or x.dtype == torch.bfloat16:
__init__.py:        return torch.half
jit/_script.py:        "half",
Binary file jit/__pycache__/_script.cpython-310.pyc matches
Binary file lib/libtorch_cpu.so matches
Binary file lib/libasmjit.a matches
Binary file lib/libdnnl_graph.a matches
Binary file lib/libdnnl.a matches
Binary file lib/libtorch_python.so matches
Binary file linalg/__pycache__/__init__.cpython-310.pyc matches
linalg/__init__.py:If :attr:`upper`\ `= True` (resp. `False`) just the upper (resp. lower) triangular half of :attr:`A`
linalg/__init__.py:Supports input of half, bfloat16, float, double, cfloat, cdouble and integral dtypes.
_meta_registrations.py:        torch.complex32: torch.half,
_meta_registrations.py:        last_dim_halfsize = (output_sizes[last_dim] // 2) + 1
_meta_registrations.py:        output_sizes[last_dim] = last_dim_halfsize
_meta_registrations.py:            (src.dtype == torch.float or src.dtype == torch.half)
_meta_registrations.py:        lambda: f"pad should be at most half of kernel size, but got pad={pad} and kernel_size={kernelSize}",
_meta_registrations.py:        lambda: "pad should be smaller than or equal to half of kernel size, but got "
nn/parameter.py:        torch.Tensor.half,
Binary file nn/__pycache__/parameter.cpython-310.pyc matches
Binary file nn/__pycache__/functional.cpython-310.pyc matches
nn/functional.py:        >>> # pool of square window and target output size being half of input image size
nn/functional.py:        >>> # pool of cubic window and target output size being half of input size
nn/functional.py:    where `input` is split in half along `dim` to form `a` and `b`, :math:`\sigma`
nn/utils/rnn.py:    def half(self):
nn/utils/rnn.py:        return self.to(dtype=torch.half)
nn/utils/memory_format.py:        >>>     nn.Conv2d(8, 4, 3)).cuda().half()
Binary file nn/utils/__pycache__/rnn.cpython-310.pyc matches
Binary file nn/utils/__pycache__/memory_format.cpython-310.pyc matches
nn/utils/rnn.pyi:    def half(self: T) -> T: ...
nn/modules/pooling.py:        >>> # pool of square window and target output size being half of input image size
nn/modules/pooling.py:        >>> # pool of cubic window and target output size being half of input size
nn/modules/module.py:    def half(self: T) -> T:
nn/modules/module.py:        r"""Casts all floating point parameters and buffers to ``half`` datatype.
nn/modules/module.py:        return self._apply(lambda t: t.half() if t.is_floating_point() else t)
nn/modules/module.py:            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
Binary file nn/modules/__pycache__/conv.cpython-310.pyc matches
Binary file nn/modules/__pycache__/activation.cpython-310.pyc matches
Binary file nn/modules/__pycache__/pooling.cpython-310.pyc matches
Binary file nn/modules/__pycache__/module.cpython-310.pyc matches
nn/modules/activation.py:    :math:`{GLU}(a, b)= a \otimes \sigma(b)` where :math:`a` is the first half
nn/modules/activation.py:    of the input matrices and :math:`b` is the second half.
nn/modules/conv.py:          layers side by side, each seeing half the input channels
nn/modules/conv.py:          and producing half the output channels, and both subsequently
onnx/symbolic_opset9.py:def _log_softmax(g: jit_utils.GraphContext, input, dim, half_to_float):
onnx/symbolic_opset9.py:        half_to_float
onnx/symbolic_opset9.py:    is_type_half = (
onnx/symbolic_opset9.py:    if is_type_half:
onnx/symbolic_opset9.py:    if is_type_half:
onnx/symbolic_opset9.py:    if is_type_half:
onnx/symbolic_opset9.py:        half = torch.tensor(0.5, dtype=torch.double)
onnx/symbolic_opset9.py:        return mul(g, half, mul(g, self, add(g, one, g.op("Tanh", inner))))
onnx/symbolic_helper.py:            else "half_pixel"
onnx/symbolic_helper.py:        else "half_pixel"
onnx/symbolic_helper.py:    "half": "Half",
onnx/symbolic_helper.py:    torch.half,  # 5
onnx/symbolic_helper.py:    "Half": torch.half,
onnx/_type_utils.py:    "half",
onnx/_type_utils.py:    JitScalarType.HALF: "half",
onnx/_type_utils.py:    JitScalarType.HALF: torch.half,
Binary file onnx/__pycache__/symbolic_helper.cpython-310.pyc matches
Binary file onnx/__pycache__/symbolic_opset9.cpython-310.pyc matches
Binary file onnx/__pycache__/_type_utils.cpython-310.pyc matches
Binary file onnx/__pycache__/_patch_torch.cpython-310.pyc matches
onnx/verification.py:        half_idx = len(included_node_indices) // 2 - 1
onnx/verification.py:        if half_idx >= 0 and len(included_node_indices) > half_idx:
onnx/verification.py:            return included_node_indices[half_idx] + 1
onnx/_patch_torch.py:    elif type_ == "half":
onnx/_patch_torch.py:            "char, short, int, long, half, float, double"
optim/lr_scheduler.py:    * "triangular2": A basic triangular cycle that scales initial amplitude by half each cycle.
optim/lr_scheduler.py:            increasing half of a cycle. Default: 2000
optim/lr_scheduler.py:            decreasing half of a cycle. If step_size_down is None,
Binary file optim/__pycache__/lr_scheduler.cpython-310.pyc matches
overrides.py:        Tensor.half: lambda self, memory_format=torch.preserve_format: -1,
overrides.py:        Tensor.chalf: lambda self, memory_format=torch.preserve_format: -1,
Binary file _prims_common/__pycache__/__init__.cpython-310.pyc matches
_prims_common/__init__.py:        torch.half: DataType.Half,
_prims_common/__init__.py:          (for example, long + half -> half, even if the half tensor has zero dimensions)
profiler/_pattern_matcher.py:        if arg_dtype in (torch.bfloat16, torch.half) and not mutiple_of(input_shapes(event), 8):
Binary file __pycache__/__init__.cpython-310.pyc matches
Binary file __pycache__/__future__.cpython-310.pyc matches
Binary file __pycache__/overrides.cpython-310.pyc matches
Binary file __pycache__/storage.cpython-310.pyc matches
Binary file __pycache__/functional.cpython-310.pyc matches
Binary file __pycache__/_tensor_docs.cpython-310.pyc matches
Binary file __pycache__/_meta_registrations.cpython-310.pyc matches
Binary file __pycache__/__init__.cpython-38.pyc matches
Binary file __pycache__/_torch_docs.cpython-310.pyc matches
_refs/_conversions.py:    "chalf",
_refs/_conversions.py:    "half",
_refs/_conversions.py:chalf = _make_conversion_method("chalf", torch.complex32)
_refs/_conversions.py:half = _make_conversion_method("half", torch.half)
Binary file _refs/__pycache__/_conversions.cpython-310.pyc matches
share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/run_nvcc.cmake:  # Since nvcc can sometimes leave half done files make sure that we delete the output file.
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      or by an individual or Legal Entity authorized to submit on behalf of
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      and issue tracking systems that are managed by, or on behalf of, the
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      on behalf of whom a Contribution has been received by Licensor and
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      on Your own behalf and on Your sole responsibility, not on behalf
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      or by an individual or Legal Entity authorized to submit on behalf of
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      and issue tracking systems that are managed by, or on behalf of, the
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      on behalf of whom a Contribution has been received by Licensor and
share/doc/dnnl_graph/THIRD-PARTY-PROGRAMS:      on Your own behalf and on Your sole responsibility, not on behalf
share/doc/dnnl_graph/LICENSE:      or by an individual or Legal Entity authorized to submit on behalf of
share/doc/dnnl_graph/LICENSE:      and issue tracking systems that are managed by, or on behalf of, the
share/doc/dnnl_graph/LICENSE:      on behalf of whom a Contribution has been received by Licensor and
share/doc/dnnl_graph/LICENSE:      on Your own behalf and on Your sole responsibility, not on behalf
share/doc/dnnl/THIRD-PARTY-PROGRAMS:      or by an individual or Legal Entity authorized to submit on behalf of
share/doc/dnnl/THIRD-PARTY-PROGRAMS:      and issue tracking systems that are managed by, or on behalf of, the
share/doc/dnnl/THIRD-PARTY-PROGRAMS:      on behalf of whom a Contribution has been received by Licensor and
share/doc/dnnl/THIRD-PARTY-PROGRAMS:      on Your own behalf and on Your sole responsibility, not on behalf
share/doc/dnnl/LICENSE:      or by an individual or Legal Entity authorized to submit on behalf of
share/doc/dnnl/LICENSE:      and issue tracking systems that are managed by, or on behalf of, the
share/doc/dnnl/LICENSE:      on behalf of whom a Contribution has been received by Licensor and
share/doc/dnnl/LICENSE:      on Your own behalf and on Your sole responsibility, not on behalf
share/ATen/Declarations.yaml:- name: chalf
share/ATen/Declarations.yaml:  operator_name: chalf
share/ATen/Declarations.yaml:  schema_string: aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
share/ATen/Declarations.yaml:  schema_string: aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_sparse_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:  schema_string: aten::_sparse_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
share/ATen/Declarations.yaml:    name: half_to_float
share/ATen/Declarations.yaml:    name: half_to_float
storage.py:    def half(self):
storage.py:        """Casts this storage to half type"""
storage.py:        return self._to(torch.half)
storage.py:        torch.half: 'HalfStorage',
storage.py:    def half(self):
storage.py:        """Casts this storage to half type"""
storage.py:        return self._to(torch.half)
_tensor_docs.py:    "chalf",
_tensor_docs.py:chalf(memory_format=torch.preserve_format) -> Tensor
_tensor_docs.py:``self.chalf()`` is equivalent to ``self.to(torch.complex32)``. See :func:`to`.
_tensor_docs.py:    "half",
_tensor_docs.py:half(memory_format=torch.preserve_format) -> Tensor
_tensor_docs.py:``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.
_tensor_docs.py:will be half that of :attr:`self`. If :attr:`dtype` element size is half that
Binary file test/c10_Half_test matches
Binary file test/half_test matches
testing/_internal/common_dtype.py:_floating_types_and_half = _floating_types + (torch.half,)
testing/_internal/common_dtype.py:def floating_types_and_half():
testing/_internal/common_dtype.py:    return _floating_types_and_half
testing/_internal/common_dtype.py:_all_types_and_half = _all_types + (torch.half,)
testing/_internal/common_dtype.py:def all_types_and_half():
testing/_internal/common_dtype.py:    return _all_types_and_half
testing/_internal/common_dtype.py:def get_all_dtypes(include_half=True,
testing/_internal/common_dtype.py:    dtypes = get_all_int_dtypes() + get_all_fp_dtypes(include_half=include_half, include_bfloat16=include_bfloat16)
testing/_internal/common_dtype.py:    return get_all_int_dtypes() + get_all_fp_dtypes(include_half=device.startswith('cuda'),
testing/_internal/common_dtype.py:def get_all_fp_dtypes(include_half=True, include_bfloat16=True) -> List[torch.dtype]:
testing/_internal/common_dtype.py:    if include_half:
testing/_internal/common_utils.py:                      torch.half: 1e-2,
testing/_internal/common_jit.py:                   if v is not None and v.dtype in floating_and_complex_types_and(torch.half, torch.bfloat16))
testing/_internal/common_modules.py:               dtypes=floating_and_complex_types_and(torch.chalf),
testing/_internal/common_modules.py:                   # Not implmented for chalf on CPU
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                'test_if_train_and_eval_modes_differ', dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cuda'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cuda'),
testing/_internal/common_modules.py:                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_pickle', dtypes=(torch.chalf,)),
testing/_internal/common_modules.py:               dtypes=floating_and_complex_types_and(torch.chalf),
testing/_internal/common_modules.py:                   # Not implmented for chalf on CPU
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                'test_if_train_and_eval_modes_differ', dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cuda'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cuda'),
testing/_internal/common_modules.py:                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_pickle', dtypes=(torch.chalf,)),
testing/_internal/common_modules.py:               dtypes=floating_and_complex_types_and(torch.chalf),
testing/_internal/common_modules.py:                   # Not implmented for chalf on CPU
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                'test_if_train_and_eval_modes_differ', dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cuda'),
testing/_internal/common_modules.py:                                dtypes=(torch.chalf,), device_type='cuda'),
testing/_internal/common_modules.py:                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_pickle', dtypes=(torch.chalf,)),
testing/_internal/opinfo/core.py:                if t.dtype is torch.chalf:
testing/_internal/opinfo/core.py:    is_fp16_or_chalf = dtype == torch.complex32 or dtype == torch.half
testing/_internal/opinfo/core.py:    if not is_fp16_or_chalf:
testing/_internal/opinfo/core.py:        # cuFFT supports powers of 2 for half and complex half precision
testing/_internal/opinfo/core.py:            s=(3, 10) if not is_fp16_or_chalf else (4, 8),
testing/_internal/opinfo/core.py:            s=(3, 10) if not is_fp16_or_chalf else (4, 8),
testing/_internal/opinfo/core.py:        yield SampleInput(nd_tensor(), s=(6, 8) if not is_fp16_or_chalf else (4, 8))
testing/_internal/opinfo/core.py:            n=10 if not is_fp16_or_chalf else 8,
testing/_internal/opinfo/core.py:        yield SampleInput(nd_tensor(), n=7 if not is_fp16_or_chalf else 8)
testing/_internal/opinfo/core.py:                toleranceOverride({torch.chalf: tol(4e-2, 4e-2)}),
testing/_internal/opinfo/core.py:                "test_complex_half_reference_testing",
testing/_internal/opinfo/core.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half),
testing/_internal/opinfo/utils.py:    all_types_and_half,
testing/_internal/opinfo/utils.py:    floating_types_and_half,
testing/_internal/opinfo/utils.py:    all_types_and_half,
testing/_internal/opinfo/utils.py:    floating_types_and_half,
testing/_internal/opinfo/utils.py:    for dtype in all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half):
testing/_internal/opinfo/definitions/linalg.py:        dtypesIfCUDA=all_types_and_complex_and(torch.half),
testing/_internal/opinfo/definitions/linalg.py:            torch.half, *[torch.bfloat16] if (CUDA11OrLater or TEST_WITH_ROCM) else []
testing/_internal/opinfo/definitions/linalg.py:            torch.half, *[torch.bfloat16] if (CUDA11OrLater or TEST_WITH_ROCM) else []
testing/_internal/opinfo/definitions/_masked.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/opinfo/definitions/_masked.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/opinfo/definitions/_masked.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/opinfo/definitions/_masked.py:        dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/opinfo/definitions/_masked.py:                dtypes=[torch.half],
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:            torch.bool, *(() if (TEST_WITH_ROCM or not SM53OrLater) else (torch.half,))
testing/_internal/opinfo/definitions/fft.py:            torch.bool, *(() if (TEST_WITH_ROCM or not SM53OrLater) else (torch.half,))
testing/_internal/opinfo/definitions/fft.py:            torch.bool, *(() if (TEST_WITH_ROCM or not SM53OrLater) else (torch.half,))
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:            torch.bool, *(() if (TEST_WITH_ROCM or not SM53OrLater) else (torch.half,))
testing/_internal/opinfo/definitions/fft.py:            torch.bool, *(() if (TEST_WITH_ROCM or not SM53OrLater) else (torch.half,))
testing/_internal/opinfo/definitions/fft.py:            torch.bool, *(() if (TEST_WITH_ROCM or not SM53OrLater) else (torch.half,))
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:                else (torch.half, torch.complex32)
testing/_internal/opinfo/definitions/fft.py:            torch.bool, torch.bfloat16, torch.half, torch.chalf
testing/_internal/opinfo/definitions/fft.py:            torch.bool, torch.bfloat16, torch.half, torch.chalf
testing/_internal/opinfo/definitions/special.py:        dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/opinfo/definitions/special.py:        dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/opinfo/definitions/special.py:        dtypes=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/opinfo/definitions/special.py:        dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/jit_utils.py:# HIP supports half, no version check necessary
testing/_internal/autocast_test_lists.py:            ("nll_loss2d", (torch.rand((n, n, n, n), device=dev, dtype=torch.half),
testing/_internal/common_device_type.py:# @precisionOverride({torch.half : 1e-2, torch.float : 1e-4})
testing/_internal/common_device_type.py:# @dtypes(torch.half, torch.float, torch.double)
testing/_internal/common_device_type.py:# @dtypes(torch.half, torch.float, torch.double)
testing/_internal/generated/annotated_fn_args.py:    torch._C._VariableFunctions._log_softmax: [{'name': 'self', 'simple_type': 'Tensor'}, {'name': 'dim', 'simple_type': 'int64_t'}, {'name': 'half_to_float', 'simple_type': 'bool'}],
testing/_internal/generated/annotated_fn_args.py:    torch._C._VariableFunctions._log_softmax: [{'name': 'self', 'simple_type': 'Tensor'}, {'name': 'dim', 'simple_type': 'int64_t'}, {'name': 'half_to_float', 'simple_type': 'bool'}],
testing/_internal/generated/annotated_fn_args.py:    torch._C._VariableFunctions._softmax: [{'name': 'self', 'simple_type': 'Tensor'}, {'name': 'dim', 'simple_type': 'int64_t'}, {'name': 'half_to_float', 'simple_type': 'bool'}],
testing/_internal/generated/annotated_fn_args.py:    torch._C._VariableFunctions._softmax: [{'name': 'self', 'simple_type': 'Tensor'}, {'name': 'dim', 'simple_type': 'int64_t'}, {'name': 'half_to_float', 'simple_type': 'bool'}],
testing/_internal/generated/annotated_fn_args.py:    torch.Tensor.chalf: [{'name': 'self', 'simple_type': 'Tensor'}],
testing/_internal/distributed/_tensor/dtensor_lagging_op_db.py:    ("chalf", ""),
testing/_internal/distributed/_tensor/dtensor_lagging_op_db.py:    ("half", ""),
testing/_internal/distributed/distributed_test.py:                (torch.half, -0.1, True),
testing/_internal/common_methods_invocations.py:                         error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:                     error_regex='pad should be at most half of kernel size')
testing/_internal/common_methods_invocations.py:            if ord in (1, 2) and dtype in floating_types_and(torch.half, torch.bfloat16):
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfROCM=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                                    "test_inplace", dtypes=(torch.cdouble, torch.cfloat, torch.chalf)),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                                                     torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                            toleranceOverride({torch.chalf: tol(atol=1e-2, rtol=0)}),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.chalf, torch.float16, torch.bfloat16, torch.bool),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                            toleranceOverride({torch.chalf: tol(atol=1e-2, rtol=0)}),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing', device_type='cpu'),
testing/_internal/common_methods_invocations.py:                            toleranceOverride({torch.chalf: tol(atol=5e-3, rtol=0)}),
testing/_internal/common_methods_invocations.py:                            toleranceOverride({torch.chalf: tol(atol=5e-3, rtol=0)}),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                                                    torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:                                                    torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:               DecorateInfo(unittest.skip("Skipped!"), 'TestNNCOpInfo', 'test_nnc_correctness', dtypes=(torch.half,)),
testing/_internal/common_methods_invocations.py:               DecorateInfo(unittest.skip("Skipped!"), 'TestCudaFuserOpInfo', 'test_nvfuser_correctness', dtypes=(torch.half,)),
testing/_internal/common_methods_invocations.py:                    dtypes=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.half,
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                                    dtypes=(torch.chalf,), active_if=IS_WINDOWS),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                                    dtypes=(torch.chalf,), active_if=IS_WINDOWS),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.half,
testing/_internal/common_methods_invocations.py:           backward_dtypesIfCUDA=all_types_and_complex_and(torch.half, *[torch.bfloat16]
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=complex_types_and(torch.chalf),
testing/_internal/common_methods_invocations.py:                                                 torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   backward_dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                                    dtypes=all_types_and_complex_and(torch.half, torch.bfloat16)),
testing/_internal/common_methods_invocations.py:                                    dtypes=all_types_and_complex_and(torch.half, torch.bfloat16)),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:               DecorateInfo(unittest.expectedFailure, 'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:               DecorateInfo(unittest.skip('Fails on cuda + rocm'), 'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.float16, torch.chalf,
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=5e-2, rtol=5e-2), }),
testing/_internal/common_methods_invocations.py:                   'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.float16, torch.chalf,
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=8e-2, rtol=8e-2), }),
testing/_internal/common_methods_invocations.py:                   'TestCommon', 'test_complex_half_reference_testing')],
testing/_internal/common_methods_invocations.py:               torch.float16, torch.chalf, *[torch.bfloat16] if (CUDA11OrLater or TEST_WITH_ROCM) else []),
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=9e-2, rtol=9e-2), }),
testing/_internal/common_methods_invocations.py:                   'TestCommon', 'test_complex_half_reference_testing')],
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.float16, torch.chalf,
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=1e-2, rtol=5e-2)}),
testing/_internal/common_methods_invocations.py:                   'TestCommon', 'test_complex_half_reference_testing'
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=1e-3, rtol=1e-3)}),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.float16, torch.chalf,
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=6e-2, rtol=5e-2)}),
testing/_internal/common_methods_invocations.py:                   'TestCommon', 'test_complex_half_reference_testing',
testing/_internal/common_methods_invocations.py:                   toleranceOverride({torch.chalf: tol(atol=1e-2, rtol=1e-2)}),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.uint8),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.uint8),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypesIfCUDA=all_types_and_complex_and(torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                    backward_dtypesIfCUDA=floating_and_complex_types_and(torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:                        # For `chalf`, reference computation in `numpy` is computed in `cfloat`.
testing/_internal/common_methods_invocations.py:                        # Output of `chalf` saturates to `inf` quicker than reference due to its small range
testing/_internal/common_methods_invocations.py:                        DecorateInfo(unittest.skip("Skipped!"), 'TestCommon', 'test_complex_half_reference_testing',
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.half, torch.bfloat16, torch.bool),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:                   backward_dtypes=floating_and_complex_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                   backward_dtypesIfCUDA=floating_and_complex_types_and(torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool, torch.chalf),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:                    dtypes=floating_types_and(torch.bfloat16, torch.half,),
testing/_internal/common_methods_invocations.py:                    dtypesIfCUDA=all_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                    backward_dtypesIfCUDA=all_types_and_complex_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and_complex_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bfloat16, torch.half, torch.bool, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and(torch.bfloat16, torch.half, torch.bool),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and(torch.half, torch.bool, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.half, torch.bool, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   decorators=(precisionOverride({torch.half: 5e-2}),),
testing/_internal/common_methods_invocations.py:                                    dtypes=(torch.chalf,)),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                           toleranceOverride({torch.chalf: tol(atol=1e-2, rtol=0)}),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.chalf, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfROCM=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool),
testing/_internal/common_methods_invocations.py:                   backward_dtypesIfCUDA=floating_and_complex_types_and(torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_and_complex_types_and(torch.half,
testing/_internal/common_methods_invocations.py:           backward_dtypesIfCUDA=floating_and_complex_types_and(torch.half, *[torch.bfloat16]
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           backward_dtypesIfCUDA=floating_and_complex_types_and(torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        'half',
testing/_internal/common_methods_invocations.py:        op=lambda x, *args, **kwargs: x.half(*args, **kwargs),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        'chalf',
testing/_internal/common_methods_invocations.py:        op=lambda x, *args, **kwargs: x.chalf(*args, **kwargs),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:               DecorateInfo(unittest.skip("Skipped!"), 'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16, torch.complex32),
testing/_internal/common_methods_invocations.py:           dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16, torch.complex32),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=floating_types_and(torch.half, torch.bfloat16, torch.complex32, torch.complex64, torch.complex128),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                            'TestCommon', 'test_complex_half_reference_testing'),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=floating_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=floating_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                    dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                    dtypes=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and(torch.bool, torch.bfloat16, torch.half),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.half, torch.chalf),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.chalf, torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfROCM=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfROCM=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypesIfCUDA=floating_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                                    dtypes=[torch.chalf, torch.complex64, torch.cdouble])),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and_complex_and(torch.complex32, torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                   dtypesIfCUDA=all_types_and(torch.bool, torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.half, torch.bfloat16, torch.chalf)),
testing/_internal/common_methods_invocations.py:           dtypes=all_types_and_complex_and(torch.bool, torch.bfloat16, torch.float16, torch.chalf),
testing/_internal/common_methods_invocations.py:                    torch.half: tol(atol=1e-2, rtol=1e-2),
testing/_internal/common_methods_invocations.py:        dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypes=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=floating_and_complex_types_and(torch.half, torch.bfloat16),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypesIfCUDA=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        dtypes=all_types_and_complex_and(torch.bool, torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:        backward_dtypesIfCUDA=floating_and_complex_types_and(torch.float16, torch.bfloat16, torch.chalf),
testing/_internal/common_methods_invocations.py:                         dtypes=(torch.chalf,), device_type='cpu', active_if=not (IS_MACOS or IS_WINDOWS)),
testing/_internal/common_methods_invocations.py:                         dtypes=(torch.chalf,), device_type='cpu', active_if=not (IS_MACOS or IS_WINDOWS)),
testing/_internal/common_methods_invocations.py:                         dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_methods_invocations.py:                         dtypes=(torch.chalf,), device_type='cpu'),
testing/_internal/common_methods_invocations.py:        "_refs._conversions.half",
testing/_internal/common_methods_invocations.py:        torch_opinfo_name="half",
testing/_internal/common_methods_invocations.py:        "_refs._conversions.chalf",
testing/_internal/common_methods_invocations.py:        torch_opinfo_name="chalf",
testing/_internal/common_methods_invocations.py:        # FIXME: doesn't support chalf
testing/_internal/common_methods_invocations.py:        # FIXME: doesn't support chalf
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/autograd_function_db.py:        dtypes=all_types_and(torch.bool, torch.half),
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        skip_half=True,
testing/_internal/common_nn.py:        check_half=True,
testing/_internal/common_nn.py:        check_half=False,
testing/_internal/common_nn.py:        check_half=False,
testing/_internal/common_nn.py:    #     check_half=False,
testing/_internal/common_nn.py:        check_half=False,
testing/_internal/common_nn.py:        check_half=False,
testing/_internal/common_nn.py:        check_half=False,
testing/_internal/common_nn.py:        self.skip_half = kwargs.get('skip_half', False)
testing/_internal/common_nn.py:            def to_half(x):
testing/_internal/common_nn.py:                # test half()
testing/_internal/common_nn.py:                if not self.skip_half:
testing/_internal/common_nn.py:                    input_tuple = tuple(to_half(t).cuda() for t in input_tuple)
testing/_internal/common_nn.py:                    module.half().cuda()
testing/_internal/common_nn.py:        self.check_half = kwargs.get('check_half', True)
testing/_internal/common_nn.py:        if dtype in {torch.half, torch.bfloat16}:
testing/_internal/common_nn.py:                              atol=1e-1 if dtype in {torch.half, torch.bfloat16} else 4e-4, rtol=0, exact_dtype=False)
testing/_internal/common_nn.py:                              atol=1e-1 if dtype in {torch.half, torch.bfloat16} else 4e-4, rtol=0, exact_dtype=False)
testing/_internal/common_fsdp.py:def _get_state_dict(model, cpu_offload=False, half=False):
testing/_internal/common_fsdp.py:    if half:
testing/_internal/common_fsdp.py:        model.half()
testing/_internal/common_fsdp.py:                        input = input.half()
testing/_internal/common_fsdp.py:                        input = tuple(x.half() for x in input)
testing/_internal/common_fsdp.py:            ref_model = ref_model.half()
testing/_internal/common_fsdp.py:            fsdp_model = fsdp_model.half()
testing/_internal/common_fsdp.py:        # the DDP parameters are in FP16 (from `half()`) while the FSDP
_torch_docs.py:    This function implements the "round half to even" to
_torch_docs.py:    zero_point (Tensor): quantization zero_point, per channel in ``torch.int32`` or ``torch.half`` or ``torch.float32``
utils/data/dataloader.py:        #     DataLoader process can use half of them which is 32, then the rational max number of
utils/data/dataloader.py:        # See (1) and the second half of the note.
utils/data/dataloader.py:                # See (1) and the second half of the note.
utils/data/datapipes/iter/combining.py:    Container to hold instance-specific information on behalf of ForkerIterDataPipe. It tracks
utils/data/datapipes/iter/combining.py:    Container to hold instance-specific information on behalf of DemultiplexerIterDataPipe. It tracks
Binary file utils/data/datapipes/iter/__pycache__/combining.cpython-310.pyc matches
utils/hipify/cuda_to_hip_mappings.py:            ("rocblas_precision_half", CONV_NUMERIC_LITERAL, API_BLAS, HIP_UNSUPPORTED),
_utils.py:# variables.  This kind of half-worked--Python can pickle global functions
_VF.pyi:def _log_softmax(input: Tensor, dim: _int, half_to_float: _bool, *, out: Optional[Tensor]=None) -> Tensor: ...
_VF.pyi:def _softmax(input: Tensor, dim: _int, half_to_float: _bool, *, out: Optional[Tensor]=None) -> Tensor: ...
