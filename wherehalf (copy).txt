aten/src/ATen/core/type_factory.cpp:  _(HalfTensor, TensorType)         \
aten/src/ATen/core/PhiloxRNGEngine.h:#include <c10/util/Half.h>
aten/src/ATen/core/ATen_pch.h:#include <c10/util/Half.h>
aten/src/ATen/core/DistributionsHelper.h:#include <c10/util/Half.h>
aten/src/ATen/core/TransformationHelper.h:#include <c10/util/Half.h>
aten/src/ATen/core/TransformationHelper.h:template <> struct DistAccumType<Half> { using type = float; };
aten/src/ATen/core/TransformationHelper.h:  } else if (std::is_same<T, at::Half>::value || std::is_same<T, at::BFloat16>::value) {
aten/src/ATen/DLConvertor.cpp:    case ScalarType::Half:
aten/src/ATen/DLConvertor.cpp:    case ScalarType::ComplexHalf:
aten/src/ATen/DLConvertor.cpp:          stype = ScalarType::Half;
aten/src/ATen/DLConvertor.cpp:          stype = ScalarType::ComplexHalf;
aten/src/ATen/templates/RegisterDispatchKey.cpp:#include <c10/util/Half.h>
aten/src/ATen/OpMathType.h:#include <c10/util/Half.h>
aten/src/ATen/OpMathType.h:struct OpMathType<at::Half> {
aten/src/ATen/OpMathType.h:struct OpMathType<c10::complex<Half>> {
aten/src/ATen/cuda/NumericLimits.cuh:struct numeric_limits<at::Half> {
aten/src/ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half lowest() { return at::Half(0xFBFF, at::Half::from_bits()); }
aten/src/ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half max() { return at::Half(0x7BFF, at::Half::from_bits()); }
aten/src/ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half lower_bound() { return at::Half(0xFC00, at::Half::from_bits()); }
aten/src/ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half upper_bound() { return at::Half(0x7C00, at::Half::from_bits()); }
aten/src/ATen/cuda/Atomic.cuh:#include <c10/util/Half.h>
aten/src/ATen/cuda/Atomic.cuh:struct AtomicFPOp<at::Half> {
aten/src/ATen/cuda/Atomic.cuh:  inline __device__ at::Half operator() (at::Half *address, at::Half val, const func_t& func) {
aten/src/ATen/cuda/Atomic.cuh:    at::Half hsum;
aten/src/ATen/cuda/Atomic.cuh:static inline  __device__ at::Half gpuAtomicAdd(at::Half *address, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
aten/src/ATen/cuda/Atomic.cuh:                                [](at::Half hsum, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:static inline __device__ at::Half atomicAdd(at::Half *address, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:static inline __device__ void gpuAtomicAddNoReturn(at::Half *address, at::Half val) { gpuAtomicAdd(address, val); }
aten/src/ATen/cuda/Atomic.cuh:inline __device__ at::Half gpuAtomicMul(at::Half * address, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
aten/src/ATen/cuda/Atomic.cuh:                                [](at::Half bsum, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:inline __device__ at::Half gpuAtomicMax(at::Half * address, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
aten/src/ATen/cuda/Atomic.cuh:                                [](at::Half bsum, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:inline __device__ at::Half gpuAtomicMin(at::Half * address, at::Half val) {
aten/src/ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
aten/src/ATen/cuda/Atomic.cuh:                                [](at::Half bsum, at::Half val) {
aten/src/ATen/cuda/CUDADataType.h:template<> inline cudaDataType getCudaDataType<at::Half>() {
aten/src/ATen/cuda/CUDADataType.h:template<> inline cudaDataType getCudaDataType<c10::complex<c10::Half>>() {
aten/src/ATen/cuda/CUDADataType.h:    case c10::ScalarType::Half:
aten/src/ATen/cuda/CUDADataType.h:    case c10::ScalarType::ComplexHalf:
aten/src/ATen/cuda/CUDABlas.h:  where Dtype is double, float, at::Half or at::BFloat16 (ROCm, NOT for dot).
aten/src/ATen/cuda/CUDABlas.h:void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half));
aten/src/ATen/cuda/CUDABlas.h:void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half));
aten/src/ATen/cuda/CUDABlas.h:void gemv<at::Half>(CUDABLAS_GEMV_ARGTYPES(at::Half));
aten/src/ATen/cuda/CUDABlas.h:void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half));
aten/src/ATen/cuda/CUDABlas.cpp:void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half)) {
aten/src/ATen/cuda/CUDABlas.cpp:  BGEMM_CHECK_ARGVALUES(at::Half);
aten/src/ATen/cuda/CUDABlas.cpp:      at::cuda::blas::gemm<at::Half>(
aten/src/ATen/cuda/CUDABlas.cpp:void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half)) {
aten/src/ATen/cuda/CUDABlas.cpp:  GEMM_CHECK_ARGVALUES(at::Half);
aten/src/ATen/cuda/CUDABlas.cpp:  } else if (std::is_same<Dtype, at::Half>::value) {
aten/src/ATen/cuda/CUDABlas.cpp:    at::opmath_type<at::Half> alpha_val,
aten/src/ATen/cuda/CUDABlas.cpp:    const at::Half* mat1_ptr,
aten/src/ATen/cuda/CUDABlas.cpp:    const at::Half* mat2_ptr,
aten/src/ATen/cuda/CUDABlas.cpp:    const at::Half* bias,
aten/src/ATen/cuda/CUDABlas.cpp:    at::Half* result_ptr,
aten/src/ATen/cuda/CUDABlas.cpp:void gemv<at::Half>(CUDABLAS_GEMV_ARGTYPES(at::Half)) {
aten/src/ATen/cuda/CUDABlas.cpp:  // There's no such thing as "cublasHalfgemv", so here we hack gemv with a gemm.
aten/src/ATen/cuda/CUDABlas.cpp:  // have to swap args based on whether it's calling blas::gemv<at::Half> or <float>.
aten/src/ATen/cuda/CUDABlas.cpp:  gemm<at::Half>(
aten/src/ATen/cuda/CUDABlas.cpp:void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half)) {
aten/src/ATen/cuda/DeviceUtils.cuh:#include <c10/util/Half.h>
aten/src/ATen/cuda/DeviceUtils.cuh:__device__ __forceinline__ c10::Half WARP_SHFL_DOWN<c10::Half>(c10::Half value, unsigned int delta, int width, unsigned int mask)
aten/src/ATen/cuda/DeviceUtils.cuh:  return c10::Half(WARP_SHFL_DOWN<unsigned short>(value.x, delta, width, mask), c10::Half::from_bits_t{});
aten/src/ATen/cuda/cub-RadixSortKeys.cu:AT_FORALL_SCALAR_TYPES_AND2(Bool, Half, AT_INSTATIATE_CUB_TEMPLATES)
aten/src/ATen/cuda/CUDATensorMethods.cuh:#include <c10/util/Half.h>
aten/src/ATen/cuda/CUDATensorMethods.cuh:  return reinterpret_cast<__half*>(data<Half>());
aten/src/ATen/cuda/llvm_complex.cpp:struct alignas(2) complex<at::Half> {
aten/src/ATen/cuda/llvm_complex.cpp:  at::Half real_;
aten/src/ATen/cuda/llvm_complex.cpp:  at::Half imag_;
aten/src/ATen/cuda/llvm_complex.cpp:  // NOTE: computation of `complex<Half>` will occur in `complex<float>`
aten/src/ATen/cuda/llvm_complex.cpp:  at::Half real() const {return real_;}
aten/src/ATen/cuda/llvm_complex.cpp:  at::Half imag() const {return imag_;}
aten/src/ATen/cuda/cub.cuh:struct cuda_type<c10::Half> {
aten/src/ATen/cuda/cub-RadixSortPairs.cu:AT_FORALL_SCALAR_TYPES_AND2(Bool, Half, AT_INSTANTIATE_SORT_PAIRS_8)
aten/src/ATen/cpu/vec/vec_base.h:// at::Half and at::BFloat16 should be treated as floating point
aten/src/ATen/cpu/vec/vec_base.h:      std::is_same<T, at::Half>::value ||
aten/src/ATen/Dispatch.h:#include <c10/util/Half.h>
aten/src/ATen/Dispatch.h:    "use AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, ...) instead")
aten/src/ATen/Dispatch.h:    "use AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(at::ScalarType::Half, ...) "
aten/src/ATen/Dispatch.h:// but NOT booleans (bool), half-precision floats (Half) or
aten/src/ATen/Dispatch.h:  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
aten/src/ATen/Dispatch.h:      AT_DISPATCH_CASE_ALL_TYPES_AND(at::ScalarType::Half, __VA_ARGS__))
aten/src/ATen/ScalarOps.cpp:      kComplexHalf, kHalf, kBool, kBFloat16, self.scalar_type(), "fill_out", [&]() {
aten/src/ATen/miopen/Descriptors.h:    case miopenHalf: return 2;
aten/src/ATen/miopen/Descriptors.h:    if (dataType == miopenHalf || dataType == miopenFloat || dataType == miopenBFloat16) {
aten/src/ATen/miopen/Types.cpp:  } else if (tensor.scalar_type() == at::kHalf) {
aten/src/ATen/miopen/Types.cpp:    return miopenHalf;
aten/src/ATen/miopen/Descriptors.cpp:  } else if (scalar_type == at::kHalf) {
aten/src/ATen/miopen/Descriptors.cpp:    return miopenHalf;
aten/src/ATen/miopen/Descriptors.cpp:    case miopenHalf:
aten/src/ATen/miopen/Descriptors.cpp:      return "miopenHalf";
aten/src/ATen/native/GatedLinearUnit.cpp:  Tensor firstHalf = self.narrow(wrap_dim, 0, selfSize);
aten/src/ATen/native/GatedLinearUnit.cpp:  Tensor secondHalf = self.narrow(wrap_dim, selfSize, selfSize);
aten/src/ATen/native/GatedLinearUnit.cpp:  build_borrowing_binary_op(maybe_get_output(), firstHalf, secondHalf);
aten/src/ATen/native/GatedLinearUnit.cpp:  Tensor firstHalf = input.narrow(wrap_dim, 0, inputSize);
aten/src/ATen/native/GatedLinearUnit.cpp:  Tensor secondHalf = input.narrow(wrap_dim, inputSize, inputSize);
aten/src/ATen/native/GatedLinearUnit.cpp:  Tensor gradInputfirstHalf = grad_input.narrow(wrap_dim, 0, inputSize);
aten/src/ATen/native/GatedLinearUnit.cpp:  Tensor gradInputsecondHalf = grad_input.narrow(wrap_dim, inputSize, inputSize);
aten/src/ATen/native/GatedLinearUnit.cpp:  at::sigmoid_out(gradInputfirstHalf, secondHalf);
aten/src/ATen/native/GatedLinearUnit.cpp:    .add_output(gradInputsecondHalf)
aten/src/ATen/native/GatedLinearUnit.cpp:    .add_input(gradInputfirstHalf)
aten/src/ATen/native/GatedLinearUnit.cpp:    .add_input(firstHalf)
aten/src/ATen/native/GatedLinearUnit.cpp:  gradInputfirstHalf.mul_(grad_output);
aten/src/ATen/native/TriangularOps.cpp:      ScalarType::Half,
aten/src/ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_cpu", [&] {
aten/src/ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_cpu", [&] {
aten/src/ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_backward_cpu", [&] {
aten/src/ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_backward_cpu", [&] {
aten/src/ATen/native/RowwisePrune.cpp:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half,
aten/src/ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "angle_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "angle_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryComplexKernels.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_CASE_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, [&] {
aten/src/ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_CASE(kComplexHalf, conj_chalf)
aten/src/ATen/native/cuda/EmbeddingBag.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, weight.scalar_type(), "embedding_bag_cuda", [&] {
aten/src/ATen/native/cuda/TensorTransformations.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/TensorTransformations.cu:      at::ScalarType::ComplexHalf,
aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu:        if(grad.dtype() == at::kHalf || grad.dtype() == at::kBFloat16) {
aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu:        kComplexHalf, common_dtype, "sinh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu:        kComplexHalf, common_dtype, "sinh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/ReduceLogicKernel.cu:      kHalf, kBFloat16, kBool, iter.common_dtype(), "and_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceLogicKernel.cu:      kHalf, kBFloat16, kBool, iter.common_dtype(), "or_cuda", [&]() {
aten/src/ATen/native/cuda/Shape.cu:          kComplexHalf, kHalf, kBool, kBFloat16,
aten/src/ATen/native/cuda/Shape.cu:          kComplexHalf, kHalf, kBool, kBFloat16,
aten/src/ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op__cuda", [&]() {
aten/src/ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op__cuda", [&]() {
aten/src/ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu:        kComplexHalf, common_dtype, "cosh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu:        kComplexHalf, common_dtype, "cosh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/PowKernel.cu:/* complex<Half> support impl */
aten/src/ATen/native/cuda/PowKernel.cu:void pow_scalar_tensor_impl(TensorIteratorBase& iter, c10::complex<at::Half> base) {
aten/src/ATen/native/cuda/PowKernel.cu:  using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/PowKernel.cu:/* complex<Half> support impl */
aten/src/ATen/native/cuda/PowKernel.cu:/* complex<Half> support impl */
aten/src/ATen/native/cuda/PowKernel.cu:  using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/PowKernel.cu:  if (common_dtype == kComplexHalf) {
aten/src/ATen/native/cuda/PowKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/PowKernel.cu:        kHalf, kBFloat16, iter.common_dtype(), "pow_cuda", [&] {
aten/src/ATen/native/cuda/PowKernel.cu:    if (iter.common_dtype() == kComplexHalf) {
aten/src/ATen/native/cuda/PowKernel.cu:      using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/PowKernel.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "pow_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(0), "logical_not_cuda", [&]() {});
aten/src/ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(1), "logical_not_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "neg_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "neg_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_ALL_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, dtype, "neg_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_ALL_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.dtype(), "sign_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, ScalarType::Half, iter.input_dtype(), "signbit_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sgn_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sgn_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu:      kBFloat16, kHalf, kBool, iter.dtype(), "max_values_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu:      kBFloat16, kHalf, kBool, iter.input_dtype(), "max_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.input_dtype(), "max_all_cuda", [&] {
aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/DilatedMaxPool2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/DilatedMaxPool2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/BinaryRemainderKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "remainder_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryRemainderKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "fmod_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu:  if (iter.dtype(1) == kHalf) {
aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu:    argmax_kernel_cuda_impl<at::Half, float>(iter);
aten/src/ATen/native/cuda/NLLLoss2d.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/NLLLoss2d.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/NLLLoss2d.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/NLLLoss2d.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Col2Im.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/SoftMax.cu:    TORCH_CHECK(input_.scalar_type() == ScalarType::Half, "conversion is supported for Half type only");
aten/src/ATen/native/cuda/SoftMax.cu:  static_assert(std::is_same<acc_type<at::Half, true>, float>::value, "accscalar_t for half should be float");
aten/src/ATen/native/cuda/SoftMax.cu:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "host_softmax", [&] {
aten/src/ATen/native/cuda/SoftMax.cu:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "host_softmax", [&] {
aten/src/ATen/native/cuda/SoftMax.cu:  static_assert(std::is_same<acc_type<at::Half, true>, float>::value, "accscalar_t for half should be float");
aten/src/ATen/native/cuda/SoftMax.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, gI.scalar_type(), "host_softmax_backward", [&] {
aten/src/ATen/native/cuda/SoftMax.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, gI.scalar_type(), "host_softmax_backward", [&] {
aten/src/ATen/native/cuda/SoftMax.cu:         input_dtype == ScalarType::Half),
aten/src/ATen/native/cuda/SoftMax.cu:        "expected input and grad types to match, or input to be at::Half and grad to be at::Float");
aten/src/ATen/native/cuda/SoftMax.cu:         input_dtype == ScalarType::Half),
aten/src/ATen/native/cuda/SoftMax.cu:        "expected input and grad types to match, or input to be at::Half and grad to be at::Float");
aten/src/ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
aten/src/ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
aten/src/ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
aten/src/ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
aten/src/ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
aten/src/ATen/native/cuda/Nonzero.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.dtype(), "min_values_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.input_dtype(), "min_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.input_dtype(), "min_all_cuda", [&] {
aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu:        kComplexHalf, common_dtype, "tanh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu:        kComplexHalf, common_dtype, "tanh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/ForeachBinaryOpList.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpList.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpList.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpList.cu:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda_", [&]() {
aten/src/ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/ReflectionPad.cu:      kHalf, kBFloat16, input.scalar_type(), "reflection_pad1d_out_template", [&] {
aten/src/ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/DilatedMaxPool3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/DilatedMaxPool3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/SummaryOps.cu:  if (self.scalar_type() == ScalarType::Half) {
aten/src/ATen/native/cuda/SummaryOps.cu:    AT_ERROR("HalfTensor is not supported");
aten/src/ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "max_elementwise_cuda", [&]() {
aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "min_elementwise_cuda", [&]() {
aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmax_cuda", [&]() {
aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmin_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda", [&]() {
aten/src/ATen/native/cuda/SortStable.cu:      kBool, kHalf, kBFloat16, self.scalar_type(), "sort", [&] {
aten/src/ATen/native/cuda/CopysignKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "copysign_cuda", [&]() {
aten/src/ATen/native/cuda/CuFFTPlanCache.h:    if (dtype == ScalarType::Half) {
aten/src/ATen/native/cuda/CuFFTPlanCache.h:    } else if (dtype == ScalarType::Half) {
aten/src/ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16, iter.dtype(), "index_cuda", [&] {
aten/src/ATen/native/cuda/IndexKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, kComplexHalf,
aten/src/ATen/native/cuda/IndexKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, kComplexHalf,
aten/src/ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16, iter.dtype(), "index_put", [&] {
aten/src/ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, iter.dtype(), "put_cuda", [&] {
aten/src/ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, iter.dtype(), "take_cuda", [&] {
aten/src/ATen/native/cuda/IndexKernel.cu:      ScalarType::Half,
aten/src/ATen/native/cuda/IndexKernel.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/Im2Col.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/LogcumsumexpKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/Randperm.cu:    AT_DISPATCH_ALL_TYPES_AND(kHalf, result.scalar_type(), "randperm_out_cuda", [&] {
aten/src/ATen/native/cuda/Randperm.cu:    AT_DISPATCH_ALL_TYPES_AND(kHalf, result.scalar_type(), "randperm_out_cuda", [&] {
aten/src/ATen/native/cuda/ActivationMishKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationMishKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/WeightNorm.cu:  // g.scalar_type() may be at::ScalarType::Double, Float, or Half.
aten/src/ATen/native/cuda/WeightNorm.cu:  // If Half, stash norms as float.
aten/src/ATen/native/cuda/WeightNorm.cu:  at::ScalarType AccType = g.scalar_type() == at::ScalarType::Half ?
aten/src/ATen/native/cuda/ComplexKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.input_dtype(0), "complex_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu:        kComplexHalf, common_dtype, "sin_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu:        kComplexHalf, common_dtype, "sin_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/Math.cuh:  static_assert(!std::is_same<scalar_t, Half>() && !std::is_same<scalar_t, BFloat16>(), "don't instantiate with low precision type");
aten/src/ATen/native/cuda/Math.cuh:  static_assert(!std::is_same<scalar_t, Half>() && !std::is_same<scalar_t, BFloat16>(), "don't instantiate with low precision type");
aten/src/ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, it.dtype(), "GeluCUDAKernelImpl", [&]() {
aten/src/ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, it.dtype(), "GeluCUDAKernelImpl", [&]() {
aten/src/ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/PointwiseOpsKernel.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, "addcmul_cuda", [&]() {
aten/src/ATen/native/cuda/PointwiseOpsKernel.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, "addcdiv_cuda", [&]() {
aten/src/ATen/native/cuda/PointwiseOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND(kHalf, iter.dtype(), "smooth_l1_backward_cuda", [&iter, &norm, beta] {
aten/src/ATen/native/cuda/PointwiseOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "huber_backward_cuda", [&iter, &norm, delta] {
aten/src/ATen/native/cuda/PointwiseOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "mse_backward_cuda", [&]() {
aten/src/ATen/native/cuda/SpectralOps.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "_fft_fill_with_conjugate_symmetry", [&] {
aten/src/ATen/native/cuda/ForeachReduceOp.cu:      kHalf, kBFloat16, tensor_lists[0][0].scalar_type(), "foreach_tensor_norm_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachReduceOp.cu:      kHalf, kBFloat16, tensor_lists[0][0].scalar_type(), "foreach_tensor_norm_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:// jiterated specialization for `complex<Half>`
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:struct sum_functor<c10::complex<at::Half>> {
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:// jiterated specialization for `complex<Half>`
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:struct prod_functor<c10::complex<at::Half>> {
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:// for handling Half-Precision floating types.
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu://       except for `at::Half` and `at::BFloat16`.
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:  if (iter.dtype() == kHalf) {
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    return OpFunctor<at::Half, float>{}(iter);
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:  } else if (iter.dtype(1) == kHalf && iter.dtype() == kFloat) {
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    return OpFunctor<at::Half, float, float>{}(iter);
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:        kBool, kComplexHalf, iter.dtype(), "sum_cuda", [&]() {
aten/src/ATen/native/cuda/ReduceSumProdKernel.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kComplexHalf, kBool, iter.dtype(), "prod_cuda", [&]() {
aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/MultinomialKernel.cu:inline __device__ bool _isinf(c10::Half x) {
aten/src/ATen/native/cuda/Blas.cpp:           scalar_type == at::ScalarType::Half ||
aten/src/ATen/native/cuda/Blas.cpp:           (scalar_type != at::ScalarType::Half &&
aten/src/ATen/native/cuda/Blas.cpp:           (scalar_type != at::ScalarType::Half &&
aten/src/ATen/native/cuda/Blas.cpp:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Blas.cpp:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Blas.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "baddbmm_cuda", [&] {
aten/src/ATen/native/cuda/Blas.cpp:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/Blas.cpp:      AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, mat.scalar_type(), "addmv_impl_cuda", [&] {
aten/src/ATen/native/cuda/ActivationThresholdKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/LogAddExpKernel.cu:      ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu:        kComplexHalf, common_dtype, "acos_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu:        kComplexHalf, common_dtype, "acos_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, ret.scalar_type(), "poisson_cuda", [&] {
aten/src/ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, ret.scalar_type(), "gamma_cuda", [&] {
aten/src/ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "_standard_gamma_grad_cuda", [&] {
aten/src/ATen/native/cuda/AveragePool3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/AveragePool3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/AveragePool3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "huber_cuda", [&iter, delta] {
aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "mse_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "xlogy_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "xlog1py_cuda", [&]() {
aten/src/ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, self.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, self.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:    return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_out.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_out.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_out.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:        kHalf, kBFloat16, self.scalar_type(), "batch_norm_stats_cuda", [&] {
aten/src/ATen/native/cuda/Normalization.cu:          kHalf, kBFloat16, self.scalar_type(), "batch_norm_stats_cuda", [&] {
aten/src/ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, running_mean.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, running_mean.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, running_var.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:    return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/Normalization.cu:  return AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, scalar_type, "batch_norm_update_stats_cuda", [&] {
aten/src/ATen/native/cuda/Normalization.cu:  return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_output.scalar_type(), "batch_norm_backward_reduce", [&] {
aten/src/ATen/native/cuda/Normalization.cu:  return AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "batch_norm_backward_elemt", [&] {
aten/src/ATen/native/cuda/Normalization.cu:    bool is_half_float = std::is_same<scalar_t, at::Half>::value && mean_st == at::kFloat;
aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu:        kComplexHalf, common_dtype, "cos_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu:        kComplexHalf, common_dtype, "cos_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/ForeachTernaryOp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_ternary_cuda",
aten/src/ATen/native/cuda/ForeachTernaryOp.cu:        at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_ternary_cuda_",
aten/src/ATen/native/cuda/ForeachTernaryOp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_scalar_cuda",
aten/src/ATen/native/cuda/ForeachTernaryOp.cu:        at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_scalar_cuda_",
aten/src/ATen/native/cuda/Reduce.cuh:  // at::Half/at::ComplexHalf overflows easily as it's range is very small.
aten/src/ATen/native/cuda/Reduce.cuh:  // So when scalar_t and out_scalar_t are at::Half/at::ComplexHalf, we
aten/src/ATen/native/cuda/Reduce.cuh:      (std::is_same<at::Half, scalar_t>::value &&
aten/src/ATen/native/cuda/Reduce.cuh:       std::is_same<at::Half, out_scalar_t>::value) ||
aten/src/ATen/native/cuda/Reduce.cuh:      (std::is_same<c10::complex<Half>, scalar_t>::value &&
aten/src/ATen/native/cuda/Reduce.cuh:       std::is_same<c10::complex<Half>, out_scalar_t>::value);
aten/src/ATen/native/cuda/Reduce.cuh:  // at::Half/at::ComplexHalf overflows easily as it's range is very small.
aten/src/ATen/native/cuda/Reduce.cuh:  // So when scalar_t and out_scalar_t are at::Half/at::ComplexHalf, we
aten/src/ATen/native/cuda/Reduce.cuh:      (std::is_same<at::Half, scalar_t>::value &&
aten/src/ATen/native/cuda/Reduce.cuh:       std::is_same<at::Half, out_scalar_t>::value) ||
aten/src/ATen/native/cuda/Reduce.cuh:      (std::is_same<c10::complex<Half>, scalar_t>::value &&
aten/src/ATen/native/cuda/Reduce.cuh:       std::is_same<c10::complex<Half>, out_scalar_t>::value);
aten/src/ATen/native/cuda/SortingRadixSelect.cuh:struct TopKTypeConfig<at::Half> {
aten/src/ATen/native/cuda/SortingRadixSelect.cuh:  static inline __device__ RadixType convert(at::Half v) {
aten/src/ATen/native/cuda/SortingRadixSelect.cuh:  static inline __device__ at::Half deconvert(RadixType v) {
aten/src/ATen/native/cuda/SortingRadixSelect.cuh:    return static_cast<at::Half>(0);
aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "atan_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "atan_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/KernelUtils.cuh:    typename std::enable_if<std::is_same<c10::Half, scalar_t>::value>::type* =
aten/src/ATen/native/cuda/KernelUtils.cuh:      reinterpret_cast<at::Half*>(tensor) + index,
aten/src/ATen/native/cuda/KernelUtils.cuh:      static_cast<at::Half>(value));
aten/src/ATen/native/cuda/KernelUtils.cuh:    typename std::enable_if<!std::is_same<c10::Half, scalar_t>::value && !std::is_same<c10::BFloat16, scalar_t>::value >::type* =
aten/src/ATen/native/cuda/ReduceNormKernel.cu:  if (iter.dtype(0) == kHalf) {
aten/src/ATen/native/cuda/ReduceNormKernel.cu:    return norm_kernel_cuda_impl<at::Half, float>(iter, ord);
aten/src/ATen/native/cuda/ReduceNormKernel.cu:  } else if (iter.input_dtype() == kHalf && iter.dtype(0) == kFloat) {
aten/src/ATen/native/cuda/ReduceNormKernel.cu:    return norm_kernel_cuda_impl<at::Half, float, float>(iter, ord);
aten/src/ATen/native/cuda/UpSampleNearest3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte,input.scalar_type(), "upsample_nearest3d_out_frame", [&] {
aten/src/ATen/native/cuda/UpSampleNearest3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest3d_backward_out_frame", [&] {
aten/src/ATen/native/cuda/TriangularOps.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, at::ScalarType::Half, at::ScalarType::Bool,
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "exp2_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:        ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0e_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0e_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sigmoid_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sigmoid_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, common_dtype, "sigmoid_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:        ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "erf_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "erfc_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.dtype(), "kaiser_window_cuda", [&](){
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.dtype(), "kaiser_window_cuda", [&](){
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "entr_cuda", [&]() {
aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/MaxUnpooling.cu:    AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
aten/src/ATen/native/cuda/MaxUnpooling.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
aten/src/ATen/native/cuda/MaxUnpooling.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
aten/src/ATen/native/cuda/MaxUnpooling.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, params[0].scalar_type(),
aten/src/ATen/native/cuda/group_norm_kernel.cu:        (X.scalar_type() == kHalf || X.scalar_type() == kBFloat16)
aten/src/ATen/native/cuda/group_norm_kernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/group_norm_kernel.cu:        (X.scalar_type() == kHalf || X.scalar_type() == kBFloat16)
aten/src/ATen/native/cuda/group_norm_kernel.cu:      (X.scalar_type() == kHalf || X.scalar_type() == kBFloat16)
aten/src/ATen/native/cuda/group_norm_kernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationPreluKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "prelu_cuda", [&] {
aten/src/ATen/native/cuda/ActivationPreluKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "prelu_backward_cuda", [&] {
aten/src/ATen/native/cuda/Copy.cu:        kHalf, kBool, kBFloat16, kComplexHalf, dtype, "copy_", [&] {
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:  if (input_dtype == kHalf && iter.dtype() == kFloat) {
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:    std_var_kernel_impl<at::Half, float>(iter, correction, take_sqrt);
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:  if (iter.dtype() == kHalf) {
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:    mean_kernel_impl<at::Half, float>(iter);
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:  } else if (iter.dtype(1) == kHalf && iter.dtype() == kFloat) {
aten/src/ATen/native/cuda/ReduceMomentKernel.cu:    mean_kernel_impl<at::Half, float, float>(iter);
aten/src/ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBFloat16, kBool, iter.dtype(), "where_cuda", [&] {
aten/src/ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isposinf_cuda", [&]() {
aten/src/ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isneginf_cuda", [&]() {
aten/src/ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "clamp_cuda", [&] {
aten/src/ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "clamp_scalar_cuda", [&] {
aten/src/ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, self.scalar_type(), "_assert_async_cuda", [&] {
aten/src/ATen/native/cuda/Pow.cuh:// pow for at::Half
aten/src/ATen/native/cuda/Pow.cuh:static inline __host__ __device__ at::Half pow_(at::Half base, at::Half exp) {
aten/src/ATen/native/cuda/Pow.cuh:  return static_cast<at::Half>(std::pow(static_cast<float>(base), static_cast<float>(exp)));
aten/src/ATen/native/cuda/ActivationEluKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationEluKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/jit_utils.cpp:  _(at::Half, Half) /* 5 */                                  \
aten/src/ATen/native/cuda/jit_utils.cpp:  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \
aten/src/ATen/native/cuda/jit_utils.cpp:  _(at::Half, Half)                                                \
aten/src/ATen/native/cuda/jit_utils.cpp:  _(std::complex<at::Half>, ComplexHalf)                           \
aten/src/ATen/native/cuda/jit_utils.cpp:struct alignas(2) Half {
aten/src/ATen/native/cuda/jit_utils.cpp:  Half() = default;
aten/src/ATen/native/cuda/jit_utils.cpp:  inline __host__ __device__ Half(float value){
aten/src/ATen/native/cuda/jit_utils.cpp:  struct static_cast_with_inter_type<std::complex<at::Half>, at::BFloat16> {
aten/src/ATen/native/cuda/jit_utils.cpp:    static inline std::complex<at::Half> apply(at::BFloat16 src) {
aten/src/ATen/native/cuda/jit_utils.cpp:      return static_cast<std::complex<at::Half>>(float{src});
aten/src/ATen/native/cuda/jit_utils.cpp:  struct static_cast_with_inter_type<std::complex<at::Half>, at::Half> {
aten/src/ATen/native/cuda/jit_utils.cpp:    static inline std::complex<at::Half> apply(at::Half src) {
aten/src/ATen/native/cuda/jit_utils.cpp:      return static_cast<std::complex<at::Half>>(float{src});
aten/src/ATen/native/cuda/jit_utils.cpp:      std::complex<at::Half>,
aten/src/ATen/native/cuda/jit_utils.cpp:    static inline std::complex<at::Half> apply(std::complex<double> src) {
aten/src/ATen/native/cuda/jit_utils.cpp:      return static_cast<std::complex<at::Half>>(static_cast<std::complex<float>>(src));
aten/src/ATen/native/cuda/jit_utils.cpp:  if (f_inputs_type == "at::Half" || result_type == "at::Half" ||
aten/src/ATen/native/cuda/jit_utils.cpp:      f_inputs_type == "std::complex<at::Half>" ||
aten/src/ATen/native/cuda/jit_utils.cpp:      result_type == "std::complex<at::Half>" || dynamic_casting) {
aten/src/ATen/native/cuda/jit_utils.cpp:    // complex<Half> depends on complex<T> and Half dtypes.
aten/src/ATen/native/cuda/jit_utils.cpp:      f_inputs_type == "std::complex<at::Half>" || result_type == "std::complex<at::Half>") {
aten/src/ATen/native/cuda/jit_utils.cpp:    // complex<Half> depends on complex<T> and Half dtypes.
aten/src/ATen/native/cuda/jit_utils.cpp:  if (f_inputs_type == "std::complex<at::Half>" ||
aten/src/ATen/native/cuda/jit_utils.cpp:      result_type == "std::complex<at::Half>" || dynamic_casting) {
aten/src/ATen/native/cuda/jit_utils.cpp:    // include complex<at::Half>
aten/src/ATen/native/cuda/jit_utils.cpp:      if (f_inputs_type == "at::Half" || f_inputs_type == "std::complex<at::Half>") {
aten/src/ATen/native/cuda/jit_utils.cpp:        // complex<Half> depends on complex<T> and Half dtypes.
aten/src/ATen/native/cuda/jit_utils.cpp:          f_inputs_type == "std::complex<at::Half>" ) {
aten/src/ATen/native/cuda/jit_utils.cpp:        // complex<Half> depends on complex<T> and Half dtypes.
aten/src/ATen/native/cuda/jit_utils.cpp:      if (f_inputs_type == "std::complex<at::Half>") {
aten/src/ATen/native/cuda/CumprodKernel.cu:      ScalarType::Half, ScalarType::BFloat16, self.scalar_type(), "cumprod_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "log_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, iter.common_dtype(), "log_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log10_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryLogKernels.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log1p_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log2_cuda", [&]() {
aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/CUDAScalar.cu:    kComplexHalf, kHalf, kBool, kBFloat16, self.scalar_type(), "_local_scalar_dense_cuda", [&] {
aten/src/ATen/native/cuda/BinaryGeometricKernels.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/BinaryGeometricKernels.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/SegmentReduce.cu:            kHalf,
aten/src/ATen/native/cuda/SegmentReduce.cu:            at::ScalarType::Half,
aten/src/ATen/native/cuda/CumsumKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/Loss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "binary_cross_entropy_backward_out_cuda", [&]() {
aten/src/ATen/native/cuda/Loss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "binary_cross_entropy_out_cuda", [&]() {
aten/src/ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/CompareEQKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBFloat16, kBool,
aten/src/ATen/native/cuda/DepthwiseConv2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/DepthwiseConv2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_output.scalar_type(),
aten/src/ATen/native/cuda/DepthwiseConv2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_output.scalar_type(),
aten/src/ATen/native/cuda/AbsKernel.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "abs_cuda", [&]() {
aten/src/ATen/native/cuda/AbsKernel.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "abs_cuda", [&]() {
aten/src/ATen/native/cuda/AbsKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/FillKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kHalf, kBFloat16, iter.dtype(), "fill_cuda", [&]() {
aten/src/ATen/native/cuda/UniqueCub.cu:INSTANTIATE_UNIQUE_CUDA_TEMPLATE(at::Half);
aten/src/ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique", [&] {
aten/src/ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique", [&] {
aten/src/ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique_dim", [&] {
aten/src/ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique_dim", [&] {
aten/src/ATen/native/cuda/Unique.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique", [&] {
aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/jit_utils.h:template <> inline std::string typeName<c10::complex<at::Half>>(){
aten/src/ATen/native/cuda/jit_utils.h:    return "std::complex<at::Half>";
aten/src/ATen/native/cuda/jit_utils.h:template <> inline std::string typeName<at::Half>(){
aten/src/ATen/native/cuda/jit_utils.h:    return "at::Half";
aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu:        kComplexHalf, common_dtype, "asinh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu:        kComplexHalf, common_dtype, "asinh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "exp_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "exp_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, common_dtype, "exp_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "rsqrt_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "rsqrt_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:          // In CUDA, ::rsqrt is overloaded for float and at::Half here is implicitly cast to float.
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sqrt_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sqrt_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, common_dtype, "sqrt_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "clamp_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "clamp_min_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "clamp_max_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "nan_to_num_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND(ScalarType::Half,
aten/src/ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, input.scalar_type(), "upsample_nearest2d_nhwc_out_frame", [&] {
aten/src/ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, input.scalar_type(), "upsample_nearest2d_out_frame", [&] {
aten/src/ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest2d_backward_nhwc_out_frame", [&] {
aten/src/ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest2d_backward_out_frame", [&] {
aten/src/ATen/native/cuda/ActivationGluKernel.cu:      kHalf, kBFloat16, iter.dtype(), "glu_cuda", [&]() {
aten/src/ATen/native/cuda/ActivationGluKernel.cu:      kHalf, kBFloat16, iter.dtype(), "glu_cuda", [&]() {
aten/src/ATen/native/cuda/ActivationGluKernel.cu:      kHalf, kBFloat16, iter.common_dtype(), "glu_backward_cuda", [&] {
aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu:        kHalf, kBFloat16, dtype, "div_floor_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu:        kHalf, kBFloat16, dtype, "div_floor_cuda", [&]() {
aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationSiluKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/ActivationSiluKernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/Embedding.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/Embedding.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "embedding_renorm_cuda_", [&] {
aten/src/ATen/native/cuda/RreluWithNoise.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/layer_norm_kernel.cu:  if ((std::is_same<T, float>::value || std::is_same<T, at::Half>::value || std::is_same<T, at::BFloat16>::value) &&
aten/src/ATen/native/cuda/layer_norm_kernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/layer_norm_kernel.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu:        ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, res_values.scalar_type(),
aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/TensorTopK.cu:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "topk_out_cuda", [&] { \
aten/src/ATen/native/cuda/Dropout.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/Dropout.cu:   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, ret.scalar_type(), "masked_scale", [&] {
aten/src/ATen/native/cuda/ReduceArgMinKernel.cu:  if (iter.dtype(1) == kHalf) {
aten/src/ATen/native/cuda/ReduceArgMinKernel.cu:    argmin_kernel_cuda_impl<at::Half, float>(iter);
aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu:          at::ScalarType::Half,
aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu:          at::ScalarType::Half,
aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/CrossKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(kHalf, iter.common_dtype(), "cross_cuda", [&] {
aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu:        kHalf, kBFloat16, dtype, "div_trunc_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu:        kHalf, kBFloat16, dtype, "div_trunc_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu:        kComplexHalf, common_dtype, "atanh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu:        kComplexHalf, common_dtype, "atanh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/StepKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, kBFloat16, iter.dtype(), "heaviside_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sigmoid_backward_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sigmoid_backward_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, dtype, "sigmoid_backward_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:      at::ScalarType::Half,
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "tanh_backward_complex_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "tanh_backward_complex_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, dtype, "tanh_backward_cuda", [&]() {
aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf, kBFloat16, input.scalar_type(), "adaptive_max_pool2d_cuda", [&] {
aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf,
aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf,
aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf,
aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
aten/src/ATen/native/cuda/TensorModeKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, self.scalar_type(), "cuda_mode", [&] {
aten/src/ATen/native/cuda/TensorModeKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, self.scalar_type(), "cuda_mode", [&] {
aten/src/ATen/native/cuda/fused_adam_impl.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, params[0].scalar_type(),
aten/src/ATen/native/cuda/LinearAlgebra.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf,
aten/src/ATen/native/cuda/RangeFactories.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, r.scalar_type(), "linspace_cuda", [&]() {
aten/src/ATen/native/cuda/RangeFactories.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, r.scalar_type(), "logspace_cuda", [&]() {
aten/src/ATen/native/cuda/RangeFactories.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, result.scalar_type(), "range_cuda", [&]() {
aten/src/ATen/native/cuda/RangeFactories.cu:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, result.scalar_type(), "arange_cuda", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu:        kComplexHalf, common_dtype, "tan_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu:        kComplexHalf, common_dtype, "tan_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu:        ScalarType::Half,
aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu:      kHalf, kBFloat16, iter.common_dtype(), "log_sigmoid_forward_cuda", [&] {
aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu:      kHalf, kBFloat16, iter.common_dtype(), "log_sigmoid_backward_cuda", [&] {
aten/src/ATen/native/cuda/CumminmaxKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/CumminmaxKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu:        kComplexHalf, common_dtype, "asin_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu:        kComplexHalf, common_dtype, "asin_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu:        kHalf, kBFloat16, common_dtype, "asin_cuda", [&]() {
aten/src/ATen/native/cuda/MultiMarginLoss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "multi_margin_loss_cuda", [&] {
aten/src/ATen/native/cuda/MultiMarginLoss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/AveragePool2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/AveragePool2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/BinaryMulKernel.cu:  if (common_dtype == kComplexHalf) {
aten/src/ATen/native/cuda/BinaryMulKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/BinaryMulKernel.cu:        kHalf, kBFloat16, kBool, iter.common_dtype(), "mul_cuda", [&]() {
aten/src/ATen/native/cuda/Sorting.cu:      at::ScalarType::Half, self.scalar_type(), "kthvalue_cuda", [&] {
aten/src/ATen/native/cuda/Sorting.cu:      at::ScalarType::Half, self.scalar_type(), "median_out_impl", [&] {
aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu:  if (iter.common_dtype() == kComplexHalf) {
aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu:    using scalar_t = c10::complex<at::Half>;
aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu:        kHalf, kBFloat16, common_dtype, "div_true_cuda", [&]() {
aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu:        kHalf, kBFloat16, common_dtype, "div_true_cuda", [&]() {
aten/src/ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
aten/src/ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
aten/src/ATen/native/cuda/Lerp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
aten/src/ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
aten/src/ATen/native/cuda/Lerp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu:      kHalf, kBFloat16, input_.scalar_type(), "adaptive_max_pool3d_cuda", [&] {
aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu:        kHalf,
aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu:        kHalf,
aten/src/ATen/native/cuda/Normalization.cuh:  if (mean_.scalar_type() == at::ScalarType::Half || mean_.scalar_type() == at::ScalarType::BFloat16) {
aten/src/ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_forward", [&] {
aten/src/ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_forward", [&] {
aten/src/ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_reduce", [&] {
aten/src/ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_reduce", [&] {
aten/src/ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_element", [&] {
aten/src/ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "batchnorm_backward_element", [&] {
aten/src/ATen/native/cuda/Normalization.cuh:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_element", [&] {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(ScalarType::Half,  tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(ScalarType::Half, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(ScalarType::Half, at::ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(ScalarType::Half, at::ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_TYPES_AND(ScalarType::Half,  tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,  tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
aten/src/ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_zero_cuda_", [&]() {
aten/src/ATen/native/cuda/UpSampleNearest1d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, input.scalar_type(), "upsample_nearest1d_out_frame", [&] {
aten/src/ATen/native/cuda/UpSampleNearest1d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest1d_backward_out_frame", [&] {
aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "acosh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "acosh_name", [&]() {
aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/cuda/DistributionTemplates.h:#include <c10/util/Half.h>
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "random_from_to_kernel_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, iter.dtype(), "random_kernel_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "normal_kernel_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "uniform_kernel_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "log_normal_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "geometric_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "exponential_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "cauchy_cuda", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:    at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, self.scalar_type(), "bernoulli_tensor_cuda_self_", [&] {
aten/src/ATen/native/cuda/DistributionTemplates.h:    at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, iter.dtype(), "bernoulli_scalar_cuda_", [&] {
aten/src/ATen/native/cuda/Indexing.cu:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16,
aten/src/ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::ComplexHalf, result.scalar_type(), "index_add", [&] {
aten/src/ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "index_add", [&] {
aten/src/ATen/native/cuda/Indexing.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, result.scalar_type(), "index_reduce", [&] {
aten/src/ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "index_reduce", [&] {
aten/src/ATen/native/cuda/Indexing.cu:        at::ScalarType::ComplexHalf,
aten/src/ATen/native/cuda/Indexing.cu:        at::ScalarType::Half,
aten/src/ATen/native/cuda/Indexing.cu:      kBool, kHalf, kBFloat16, kComplexHalf, iter.common_dtype(), "masked_fill_", [&]() {
aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu:      kBFloat16, kHalf, kBool, iter.input_dtype(), "aminmax_all_cuda", [&] {
aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu:      kBFloat16, kHalf, kBool, iter.input_dtype(), "aminmax_cuda", [&]() {
aten/src/ATen/native/cuda/Bucketization.cu:    AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, input.scalar_type(), "searchsorted_out_cuda", [&] {
aten/src/ATen/native/cuda/Bucketization.cu:    AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, input.scalar_type(), "searchsorted_out_cuda", [&] {
aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu:    AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, ScalarType::BFloat16,
aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, ScalarType::BFloat16,
aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, ScalarType::BFloat16,
aten/src/ATen/native/cuda/Sort.cu:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, key.scalar_type(), "sortKeyValueInplace", [&]  {
aten/src/ATen/native/cuda/CompareKernels.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBFloat16, kBool, iter.common_dtype(), "compare_cuda", [&]() {
aten/src/ATen/native/cuda/ActivationHardswishKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "hardswish_cuda", [&]() {
aten/src/ATen/native/cuda/ActivationHardswishKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "hardswish_backward_cuda", [&]() {
aten/src/ATen/native/cuda/ConvolutionMM2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/ConvolutionMM2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/cuda/ConvolutionMM2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
aten/src/ATen/native/TensorConversions.cpp:  if ((self.dtype() == at::ScalarType::Half || self.dtype() == at::ScalarType::BFloat16) &&
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  if (dtype == kComplexHalf) {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    using scalar_t = c10::complex<Half>;
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), "abs_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "angle_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_CASE_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, [&] {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_CASE_COMPLEX_TYPES_AND(kComplexHalf, [&] {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "frac_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(1), "logical_not_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(0), "logical_not_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "reciprocal_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, kBFloat16, kHalf, iter.dtype(), "neg_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, ScalarType::Half, iter.dtype(), "sign_cpu", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:      AT_DISPATCH_CASE_FLOATING_TYPES_AND2(kBFloat16, ScalarType::Half, [&] {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  if (dtype == kComplexHalf) {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:    using scalar_t = c10::complex<Half>;
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "exp2", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "nan_to_num", [&]() {
aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf,
aten/src/ATen/native/cpu/Activation.cpp:    const Vec kOneHalfVec(one_half);
aten/src/ATen/native/cpu/Activation.cpp:            grad_val0 * ((self_val0 / kThreeVec) + kOneHalfVec),
aten/src/ATen/native/cpu/Activation.cpp:            grad_val1 * ((self_val1 / kThreeVec) + kOneHalfVec),
aten/src/ATen/native/cpu/Activation.cpp:    const Vec kOneHalfVec(one_half);
aten/src/ATen/native/cpu/Activation.cpp:            grad_val * ((self_val / kThreeVec) + kOneHalfVec),
aten/src/ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, self.scalar_type(), "min_cpu", [&] {
aten/src/ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, self.scalar_type(), "max_cpu", [&] {
aten/src/ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBFloat16, kBool,
aten/src/ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isposinf_cpu", [&]() {
aten/src/ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isneginf_cpu", [&]() {
aten/src/ATen/native/cpu/TensorCompareKernel.cpp:      kHalf, kBFloat16, kBool, self.scalar_type(), "mode_cpu", [&] {
aten/src/ATen/native/cpu/SumKernel.cpp:      ScalarType::BFloat16, ScalarType::Half, iter.dtype(), "sum_cpu", [&] {
aten/src/ATen/native/cpu/SumKernel.cpp:      ScalarType::BFloat16, ScalarType::Half, iter.dtype(), "nansum_cpu", [&] {
aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf,
aten/src/ATen/native/cpu/SortingKernel.cpp:    ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
aten/src/ATen/native/cpu/MultinomialKernel.cpp:      kHalf, kBFloat16, self.scalar_type(), "multinomial", [&] {
aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cpu/FillKernel.cpp:void fill_non_native_type<c10::complex<at::Half>>(TensorIterator& iter, const Scalar& value_scalar) {
aten/src/ATen/native/cpu/FillKernel.cpp:  static_assert(sizeof(c10::complex<at::Half>) == sizeof(int32_t), "Size of ComplexHalf should be 32-bits");
aten/src/ATen/native/cpu/FillKernel.cpp:  auto value = c10::complex<at::Half>(value_scalar.to<c10::complex<float>>());
aten/src/ATen/native/cpu/FillKernel.cpp:  if (iter.dtype() == ScalarType::Half) {
aten/src/ATen/native/cpu/FillKernel.cpp:    fill_non_native_type<at::Half>(iter, value_scalar);
aten/src/ATen/native/cpu/FillKernel.cpp:  } else if (iter.dtype() == ScalarType::ComplexHalf) {
aten/src/ATen/native/cpu/FillKernel.cpp:    fill_non_native_type<c10::complex<at::Half>>(iter, value_scalar);
aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "arange_cpu", [&]() {
aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, iter.dtype(), "linspace_cpu", [&]() {
aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, dtype, "huber_backward_cpu_out", [&] {
aten/src/ATen/native/cpu/SparseFactories.cpp:      at::ScalarType::Half,
aten/src/ATen/native/cpu/SparseFactories.cpp:      at::ScalarType::ComplexHalf,
aten/src/ATen/native/cpu/zmath.h:inline c10::complex<at::Half> conj_impl <c10::complex<at::Half>> (c10::complex<at::Half> z) {
aten/src/ATen/native/cpu/zmath.h:  return c10::complex<at::Half>{z.real(), -z.imag()};
aten/src/ATen/native/cpu/CopyKernel.cpp:  } else if (dtype == ScalarType::ComplexHalf) {
aten/src/ATen/native/cpu/CopyKernel.cpp:    cpu_kernel(iter, [=](c10::complex<at::Half> a) -> c10::complex<at::Half> { return a; });
aten/src/ATen/native/cpu/CopyKernel.cpp:        kBool, kHalf, kBFloat16, dtype, "copy_kernel", [&] {
aten/src/ATen/native/cpu/CopyKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::ComplexHalf, ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, dtype, "copy_", [&] {
aten/src/ATen/native/cpu/CopyKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::ComplexHalf, ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, iter.dtype(1), "copy_", [&] {
aten/src/ATen/native/cpu/BlasKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::kHalf, at::kBFloat16,
aten/src/ATen/native/cpu/BlasKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::kHalf, at::kBFloat16, type, "cpublas_axpy_impl",
aten/src/ATen/native/cpu/BlasKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(at::kComplexHalf, at::kHalf, at::kBFloat16, at::kBool, type, "cpublas_copy_impl",
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, kComplexHalf,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, kComplexHalf,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kBFloat16, kHalf,
aten/src/ATen/native/cpu/IndexKernel.cpp:      ScalarType::Half,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/IndexKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(), "flip_cpu",
aten/src/ATen/native/cpu/PowKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, dtype, "pow", [&]() {
aten/src/ATen/native/cpu/PowKernel.cpp:// to use this common-path. Half cannot currently use it, as AVX2 support for
aten/src/ATen/native/cpu/PowKernel.cpp:  } else if (dtype == ScalarType::Half) {
aten/src/ATen/native/cpu/PowKernel.cpp:          decltype(c10::impl::ScalarTypeToCPPType<ScalarType::Half>::t);
aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
aten/src/ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
aten/src/ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
aten/src/ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
aten/src/ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "min_all", [&] {
aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "max_all", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "std_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.dtype(), "min_values_cpu", [&iter] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.dtype(), "max_values_cpu", [&iter] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(1), "argmax_cpu", [&] {
aten/src/ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(1), "argmin_cpu", [&] {
aten/src/ATen/native/cpu/ComplexKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.input_dtype(), "complex_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  } else if (iter.dtype() == kComplexHalf) {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:        [=](c10::complex<at::Half> a,
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:            c10::complex<at::Half> b) -> c10::complex<at::Half> {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), "mul_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "div_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, dtype, "div_trunc_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, dtype, "div_floor_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_and_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), "logical_and_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_or_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_or_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_xor_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), "logical_xor_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "lt_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "lt_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "le_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "le_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "gt_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "gt_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "ge_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "ge_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kBFloat16, kHalf, iter.common_dtype(), "eq_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, kBFloat16, kHalf, iter.common_dtype(), "eq_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kBFloat16, kHalf, iter.common_dtype(), "ne_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, kBFloat16, kHalf, iter.common_dtype(), "ne_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "maximum_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "minimum_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmax_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmin_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:        kHalf, iter.dtype(), "smooth_l1_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "huber_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.dtype(), "sigmoid_backward_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  if (iter.dtype() == ScalarType::Half) {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "fmod_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "igamma_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "igammac_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, kBFloat16, iter.dtype(), "heaviside_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "copysign_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "xlogy_cpu", [&]() {
aten/src/ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "xlog1py_cpu", [&]() {
aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "random_from_to_kernel_cpu", [&] {
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, iter.dtype(), "random_kernel_cpu", [&] {
aten/src/ATen/native/cpu/DistributionTemplates.h:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, self.scalar_type(), "normal_kernel_cpu", [&] {
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "uniform_kernel_cpu", [&]() {
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "cauchy_cpu", [&]() {
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "log_normal_cpu", [&]() {
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "geometric_cpu", [&]() {
aten/src/ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "exponential_cpu", [&]() {
aten/src/ATen/native/cpu/DistributionKernels.cpp:    AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "exponential_cpu", [&] {
aten/src/ATen/native/SpectralOps.cpp:    TORCH_CHECK(type == kHalf || type == kFloat || type == kDouble, "Unsupported dtype ", type);
aten/src/ATen/native/SpectralOps.cpp:  case kHalf: return kComplexHalf;
aten/src/ATen/native/SoftMax.cpp:        input_dtype == ScalarType::Half) {
aten/src/ATen/native/SoftMax.cpp:      grad_input_options = grad_input_options.dtype(ScalarType::Half);
aten/src/ATen/native/SoftMax.cpp:        input_dtype == ScalarType::Half) {
aten/src/ATen/native/SoftMax.cpp:      grad_input_options = grad_input_options.dtype(ScalarType::Half);
aten/src/ATen/native/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
aten/src/ATen/native/SoftMax.cpp:  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&
aten/src/ATen/native/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
aten/src/ATen/native/SoftMax.cpp:  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&
aten/src/ATen/native/TensorAdvancedIndexing.h:  if (!(c10::isFloatingType(st)) || st == ScalarType::Half) {
aten/src/ATen/native/Bucketization.cpp:        ScalarType::Half,
aten/src/ATen/native/Bucketization.cpp:        ScalarType::Half,
aten/src/ATen/native/miopen/BatchNorm_miopen.cpp:  if (input->scalar_type() != ScalarType::Half) {
aten/src/ATen/native/miopen/BatchNorm_miopen.cpp:  if (input->scalar_type() == ScalarType::Half) {
aten/src/ATen/native/RNN.cpp:    bool is_miopen_acceptable = ((input.scalar_type() == at::kFloat)|| (input.scalar_type() == at::kHalf)) &&
aten/src/ATen/native/RNN.cpp:          result_dtype == at::kHalf,
aten/src/ATen/native/transformers/cuda/attention.cu:      ScalarType::Half,
aten/src/ATen/native/transformers/cuda/flash_attn/fmha_api.cpp:    TORCH_CHECK(q_dtype == at::kHalf || (is_sm8x && q_dtype == at::kBFloat16));
aten/src/ATen/native/transformers/cuda/sdp_utils.h:  bool is_half = (params.query.dtype() == at::kHalf) ||
aten/src/ATen/native/transformers/cuda/sdp_utils.h:    static const std::array<at::ScalarType, 2> sm80_flash_dtypes{at::kHalf, at::kBFloat16};
aten/src/ATen/native/transformers/cuda/sdp_utils.h:    static const std::array<at::ScalarType, 1> default_flash_dtypes{at::kHalf};
aten/src/ATen/native/transformers/cuda/sdp_utils.h:      at::kHalf, at::kFloat, at::kBFloat16};
aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:  static constexpr bool kIsHalf = cutlass::sizeof_bits<scalar_t>::value <= 16;
aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:  static constexpr bool kOutputInRF = kIsHalf && kMaxK <= kBlockSizeI;
aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:      kIsHalf && ArchTag::kMinComputeCapability >= 80 && kOutputInRF;
aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:      kIsHalf && (kOutputInRF || ArchTag::kMinComputeCapability != 70);
aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm_kernel_utils.h:    } else if (query.scalar_type() == at::ScalarType::Half) {               \
aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm_kernel_utils.h:    return at::ScalarType::Half;
aten/src/ATen/native/transformers/attention.cpp:      ScalarType::Half,
aten/src/ATen/native/ComplexHelper.h:    self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,
aten/src/ATen/native/Col2Im.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf,
aten/src/ATen/native/Copy.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kHalf, kBool, kBFloat16, kComplexHalf, self.scalar_type(), "copy_", [&] {
aten/src/ATen/native/Copy.cpp:    if (((self.dtype() == at::kFloat && src.dtype() == at::kHalf) ||
aten/src/ATen/native/Copy.cpp:         (self.dtype() == at::kHalf && src.dtype() == at::kFloat)) &&
aten/src/ATen/native/Copy.cpp:      if (src.dtype() == at::kFloat && self.dtype() == at::kHalf) {
aten/src/ATen/native/Copy.cpp:            reinterpret_cast<fbgemm::float16*>(self.data_ptr<at::Half>());
aten/src/ATen/native/Copy.cpp:            src.data_ptr<at::Half>());
aten/src/ATen/native/Scalar.cpp:    kComplexHalf, kHalf, kBool, kBFloat16, self.scalar_type(), "_local_scalar_dense_cpu", [&] {
aten/src/ATen/native/TensorTransformations.cpp:  return self.to(kComplexHalf, false, false, memory_format);
aten/src/ATen/native/RangeFactories.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, result.scalar_type(), "arange_cpu", [&]() {
aten/src/ATen/native/TensorAdvancedIndexing.cpp:        // scatter_add does not support ComplexHalf
aten/src/ATen/native/TensorAdvancedIndexing.cpp:        source.scalar_type() != ScalarType::ComplexHalf &&
aten/src/ATen/native/TensorAdvancedIndexing.cpp:        result.scalar_type() != ScalarType::ComplexHalf) {
aten/src/ATen/native/TensorAdvancedIndexing.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, ScalarType::ComplexHalf,
aten/src/ATen/native/TensorAdvancedIndexing.cpp:      at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/TensorAdvancedIndexing.cpp:    AT_DISPATCH_ALL_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/TensorAdvancedIndexing.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::ComplexHalf, ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16,
aten/src/ATen/native/TensorAdvancedIndexing.cpp:    at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool,
aten/src/ATen/native/TensorAdvancedIndexing.cpp:      kComplexHalf, kHalf, kBFloat16, kBool, self.scalar_type(), "nonzero_count_cpu", [&] {
aten/src/ATen/native/TensorAdvancedIndexing.cpp:      kComplexHalf, kHalf, kBFloat16, kBool, self.scalar_type(), "nonzero_count_cpu", [&] {
aten/src/ATen/native/TensorAdvancedIndexing.cpp:      kComplexHalf, kHalf, kBFloat16, kBool, self.scalar_type(), "nonzero_cpu", [&] {
aten/src/ATen/native/BlasKernel.cpp:INSTANTIATE_DOT_IMPL(c10::Half);
aten/src/ATen/native/Math.h:#include <c10/util/Half.h>
aten/src/ATen/native/Math.h:C10_UNUSED c10::Half calc_igamma<c10::Half>(c10::Half a, c10::Half x) {
aten/src/ATen/native/Math.h:C10_UNUSED c10::Half calc_igammac<c10::Half>(c10::Half a, c10::Half x) {
aten/src/ATen/native/TensorFactories.cpp:#include <ATen/ops/_cast_Half_native.h>
aten/src/ATen/native/TensorFactories.cpp:  TORCH_CHECK((a.scalar_type() == kFloat || a.scalar_type() == kDouble || a.scalar_type() == kHalf) &&
aten/src/ATen/native/TensorFactories.cpp:              (b.scalar_type() == kFloat || b.scalar_type() == kDouble || b.scalar_type() == kHalf),
aten/src/ATen/native/TensorFactories.cpp:              "Expected both inputs to be Half, Float or Double tensors but got ",
aten/src/ATen/native/TensorFactories.cpp:// AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, DEFINE_CAST_OP)
aten/src/ATen/native/TensorFactories.cpp:AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16, Float8, DEFINE_CAST_OP)
aten/src/ATen/native/TensorFactories.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::ScalarType::Half, at::ScalarType::Bool, result.scalar_type(), "eye", [&]() -> void {
aten/src/ATen/native/TensorFactories.cpp:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, result.scalar_type(), "randperm", [&]() -> void {
aten/src/ATen/native/SegmentReduce.cpp:      kBFloat16, kHalf, data.scalar_type(), "_segment_reduce_cpu", [&]() {
aten/src/ATen/native/SegmentReduce.cpp:      kHalf,
aten/src/ATen/native/sparse/SparseTensor.cpp:      at::ScalarType::ComplexHalf, at::ScalarType::BFloat16, at::ScalarType::Half, at::ScalarType::Bool,
aten/src/ATen/native/sparse/cuda/SparseMatMul.cu:      std::is_same<c10::Half, scalar_t>::value ||
aten/src/ATen/native/sparse/cuda/SparseMatMul.cu:    std::is_same<c10::Half, scalar_t>::value ||
aten/src/ATen/native/sparse/cuda/SparseMatMul.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, mat1_.scalar_type(), "sparse_matmul", [&] {
aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:  if (mat1.scalar_type() == ScalarType::Half || mat1.scalar_type() == ScalarType::BFloat16) {
aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:      kHalf,
aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:      kHalf,
aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp:  if (dispatch_scalar_type == at::ScalarType::Half ||
aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu:        at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu:        at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, commonDtype, "add_out_dense_sparse_cuda", [&] {
aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu:    at::ScalarType::Half, at::ScalarType::BFloat16, commonDtype, "add_out_sparse_cuda", [&] {
aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu:      AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf, grad_values.scalar_type(), "_sparse_sum_backward_cuda", [&] {
aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu:      kComplexHalf, kHalf, kBool, kBFloat16,
aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_sum_cuda",
aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_prod_cuda",
aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu:      at::ScalarType::ComplexHalf, at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool,
aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
aten/src/ATen/native/sparse/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
aten/src/ATen/native/sparse/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
aten/src/ATen/native/sparse/SparseBlasImpl.cpp:  const auto mm_dtype = (result.scalar_type() == kHalf || result.scalar_type() == kBFloat16)
aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:      kComplexHalf,
aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:      kHalf,
aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_sum_cpu",
aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_prod_cpu",
aten/src/ATen/native/sparse/SparseTensorMath.cpp:          at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
aten/src/ATen/native/sparse/SparseTensorMath.cpp:          at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
aten/src/ATen/native/sparse/SparseTensorMath.cpp:        at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
aten/src/ATen/native/sparse/SparseTensorMath.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::ScalarType::BFloat16, at::ScalarType::Half,
aten/src/ATen/native/sparse/SparseBinaryOpIntersectionKernel.cpp:        ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, res_values.scalar_type(),
aten/src/ATen/native/UpSampleBicubic2d.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,
aten/src/ATen/native/Convolution.cpp:                             input.scalar_type() == kHalf && // only for FP16
aten/src/ATen/native/Convolution.cpp:                             weight.scalar_type() == kHalf &&
aten/src/ATen/native/Convolution.cpp:                           input.scalar_type() == kHalf && // only for FP16
aten/src/ATen/native/Convolution.cpp:                           weight.scalar_type() == kHalf &&
aten/src/ATen/native/Convolution.cpp:    return ((input.scalar_type() == at::kFloat) || (input.scalar_type() == at::kHalf) || (input.scalar_type() == at::kBFloat16))
aten/src/ATen/native/Im2Col.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf,
aten/src/ATen/native/Normalization.cpp:      && (input.scalar_type() != at::kHalf
aten/src/ATen/native/Normalization.cpp:               && (weight.scalar_type() != at::kHalf)
aten/src/ATen/native/nested/NestedTensorBackward.cpp:    ScalarType::Half, ScalarType::BFloat16, self_grad_buffer.scalar_type(), "nested_sum_dim_cpu", [&]() {
aten/src/ATen/native/nested/cuda/NestedTensorBinaryOps.cu:    ScalarType::Half, ScalarType::BFloat16, self.scalar_type(), "_nested_op_dense_esuhm", [&]() {
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:    if (padded.dtype() != kFloat && padded.dtype() != kHalf) {
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:    } else if (padded.dtype() == kHalf) {
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            padded_contiguous.data_ptr<c10::Half>(),
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            output.data_ptr<c10::Half>(),
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            padded_contiguous.data_ptr<c10::Half>(),
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            output.data_ptr<c10::Half>(),
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:       t.dtype() == at::kHalf)) {
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:template void remove_padding_kernelLauncher<c10::Half>(
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    const c10::Half* input,
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* output,
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:template void remove_padding_transform0213_kernelLauncher<c10::Half>(
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    const c10::Half* input,
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* output,
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:template void add_padding_kernelLauncher<c10::Half>(
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* input,
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* output,
aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half padding_value,
aten/src/ATen/native/nested/cuda/NestedTensorMatmul.cu:    const std::vector<c10::Half*>& aptr_,
aten/src/ATen/native/nested/cuda/NestedTensorMatmul.cu:    const std::vector<c10::Half*>& bptr_,
aten/src/ATen/native/nested/cuda/NestedTensorMatmul.cu:    const std::vector<c10::Half*>& dptr_,
aten/src/ATen/native/nested/NestedTensorUtils.h:        at::ScalarType::Half,
aten/src/ATen/native/nested/NestedTensorMath.cpp:    ScalarType::Half, ScalarType::BFloat16, buffer.scalar_type(), "nested_sum_dim_cpu", [&]() {
aten/src/ATen/native/TensorProperties.cpp:  if (!(st == kDouble || st == kFloat || st == kHalf)) return false;
aten/src/ATen/native/Distributions.cpp:  // Half is not supported on CPU.
aten/src/ATen/native/Distributions.cpp:      !(self.device().is_cpu() && self.scalar_type() == ScalarType::Half),
aten/src/ATen/native/cudnn/Conv_v8.cpp:  if (scalar_type == kBFloat16 || scalar_type == kHalf) {
aten/src/ATen/native/cudnn/Conv_v7.cpp:    const auto kAccType = (grad_weight.scalar_type() == kHalf || grad_weight.scalar_type() == kBFloat16)
aten/src/ATen/native/cudnn/BatchNorm.cpp:  if (input->scalar_type() == ScalarType::Half) {
aten/src/ATen/native/cudnn/BatchNorm.cpp:  if (input->scalar_type() == ScalarType::Half) {
aten/src/ATen/native/native_functions.yaml:- func: _cast_Half(Tensor self, bool non_blocking=False) -> Tensor
aten/src/ATen/native/native_functions.yaml:    Generic: add (AllAndComplex, BFloat16, Half, ComplexHalf)
aten/src/ATen/native/TensorCompare.cpp:  return AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, self.scalar_type(), "isinf", [&]() {
aten/src/ATen/native/TensorCompare.cpp:  return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, self.scalar_type(), "isfinite", [&]() {
aten/src/ATen/native/EmbeddingBag.cpp:#include <c10/util/Half.h>
aten/src/ATen/native/EmbeddingBag.cpp:  return (src.scalar_type() == kFloat || src.scalar_type() == kHalf) && src.strides()[1] == 1 && output.strides()[1] == 1 && padding_idx < static_cast<index_t>(0);
aten/src/ATen/native/EmbeddingBag.cpp:  return (src.scalar_type() == kFloat || src.scalar_type() == kHalf) && src.strides()[1] == 1 && output.strides()[1] == 1 && scale.strides()[0] == 1 && padding_idx < static_cast<index_t>(0);
aten/src/ATen/native/EmbeddingBag.cpp:typename std::enable_if<!std::is_same<data_t, float>::value && !std::is_same<data_t, at::Half>::value, void>::type
aten/src/ATen/native/EmbeddingBag.cpp:typename std::enable_if<std::is_same<data_t, at::Half>::value, void>::type
aten/src/ATen/native/EmbeddingBag.cpp:  auto* output_data = output.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:    auto* src_data = src_contig.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:              (output_data + i * ddim)[d] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
aten/src/ATen/native/EmbeddingBag.cpp:    auto* src_data = src.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:        (output_data + output_stride0 * i)[d * output_stride1] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
aten/src/ATen/native/EmbeddingBag.cpp:static typename std::enable_if<!std::is_same<data_t, float>::value && !std::is_same<data_t, at::Half>::value, void>::type
aten/src/ATen/native/EmbeddingBag.cpp:typename std::enable_if<std::is_same<data_t, at::Half>::value, void>::type
aten/src/ATen/native/EmbeddingBag.cpp:  auto* scale_data = scale.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:  auto* output_data = output.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:    auto* src_data = src_contig.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:              (output_data + i * ddim)[d] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
aten/src/ATen/native/EmbeddingBag.cpp:    auto* src_data = src.data_ptr<at::Half>();
aten/src/ATen/native/EmbeddingBag.cpp:        (output_data + output_stride0 * i)[d * output_stride1] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
aten/src/ATen/native/EmbeddingBag.cpp:  checkScalarTypes("embedding_bag", weight_arg, {kHalf, kFloat, kDouble});
aten/src/ATen/native/EmbeddingBag.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, weight.scalar_type(), "embedding_bag_no_grad_cpu_out",
aten/src/ATen/native/EmbeddingBag.cpp:  checkScalarTypes("embedding_bag", grad_arg, {kHalf, kFloat, kDouble});
aten/src/ATen/native/EmbeddingBag.cpp:      at::ScalarType::Half,
aten/src/ATen/native/EmbeddingBag.cpp:      at::ScalarType::Half,
aten/src/ATen/native/ReduceOpsUtils.h:    self.is_cuda() && (self.scalar_type() == kHalf || self.scalar_type() == kBFloat16) && out_dtype == kFloat);
aten/src/ATen/native/ReduceOpsUtils.h:      (self.is_cuda() && self.scalar_type() == kHalf && dtype1 == kFloat)) {
aten/src/ATen/native/ReduceOpsUtils.h:      (self.is_cuda() && self.scalar_type() == kHalf && dtype1 == kFloat)) {
aten/src/ATen/native/ReduceOpsUtils.h:       (self.scalar_type() == kHalf || self.scalar_type() == kBFloat16) &&
aten/src/ATen/native/WeightNorm.cpp:  auto has_half_dtype = v.scalar_type() == at::ScalarType::Half
aten/src/ATen/native/WeightNorm.cpp:    || g.scalar_type() == at::ScalarType::Half;
aten/src/ATen/native/TensorFactories.h:    case at::ScalarType::Half:
aten/src/ATen/native/TensorFactories.h:      TORCH_CHECK(n <= (int64_t(1) << 11) + 1, "n cannot be greater than 2049 for Half type.");
aten/src/ATen/native/ReduceOps.cpp:      iter.common_dtype() != kBFloat16 && iter.common_dtype() != kHalf) {
aten/src/ATen/native/ReduceOps.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), "equal_cpu", [&] {
aten/src/ATen/native/ForeachUtils.h:      kComplexHalf,
aten/src/ATen/native/ForeachUtils.h:      kHalf,
aten/src/ATen/native/mps/TensorFactory.h:      AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__))
aten/src/ATen/native/mps/OperationUtils.h:    at::Half h;
aten/src/ATen/native/mps/operations/ReduceOps.mm:             (dtype.value() == kFloat || dtype.value() == kHalf || dtype.value() == kInt)) {
aten/src/ATen/native/mps/operations/ReduceOps.mm:              input_t.scalar_type() != ScalarType::Half) {
aten/src/ATen/native/mps/operations/ReduceOps.mm:              input_t.scalar_type() != ScalarType::Half) {
aten/src/ATen/native/mps/operations/ReduceOps.mm:              input_t.scalar_type() != ScalarType::Half) {
aten/src/ATen/native/mps/operations/Indexing.mm:                inputTensor.scalar_type() == ScalarType::Half,
aten/src/ATen/native/mps/operations/Distributions.mm:              return (self.scalar_type() == ScalarType::Half) ? MPSDataTypeFloat16 : MPSDataTypeFloat32;
aten/src/ATen/native/mps/operations/Distributions.mm:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input_dtype, "random_update_from_to", [&] {
aten/src/ATen/native/mps/operations/Distributions.mm:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input_dtype, "random_from_to_range_calc", [&] {
aten/src/ATen/native/mps/operations/LossOps.mm:          MPSGraphTensor *mpsGraphHalfTensor = [mpsGraph constantWithScalar: 0.5
aten/src/ATen/native/mps/operations/LossOps.mm:          MPSGraphTensor *diffSquareMulHalfTensor = [mpsGraph multiplicationWithPrimaryTensor: diffSquare
aten/src/ATen/native/mps/operations/LossOps.mm:                                                                              secondaryTensor: mpsGraphHalfTensor
aten/src/ATen/native/mps/operations/LossOps.mm:          MPSGraphTensor *loss1Temp = [mpsGraph divisionWithPrimaryTensor: diffSquareMulHalfTensor
aten/src/ATen/native/mps/operations/TensorCompare.mm:    AT_DISPATCH_FLOATING_TYPES_AND(kHalf, self.scalar_type(), "nan_to_num_mps", [&]() {
aten/src/ATen/native/mps/operations/SummaryOps.mm:       weights.scalar_type() != ScalarType::Half) {
aten/src/ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* firstHalf = outputTensorsArray[0];
aten/src/ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* secondHalf = [mpsGraph sigmoidWithTensor:outputTensorsArray[1]
aten/src/ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* outputTensor = [mpsGraph multiplicationWithPrimaryTensor:firstHalf
aten/src/ATen/native/mps/operations/Activation.mm:                                                   secondaryTensor:secondHalf
aten/src/ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* firstHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : sigmoidOutputTensor
aten/src/ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* secondHalfOutputTensor = [mpsGraph subtractionWithPrimaryTensor : one_val
aten/src/ATen/native/mps/operations/Activation.mm:          secondHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : secondHalfOutputTensor
aten/src/ATen/native/mps/operations/Activation.mm:          secondHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : secondHalfOutputTensor
aten/src/ATen/native/mps/operations/Activation.mm:          secondHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : secondHalfOutputTensor
aten/src/ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* outputTensor = [mpsGraph concatTensor : firstHalfOutputTensor
aten/src/ATen/native/mps/operations/Activation.mm:                                                     withTensor : secondHalfOutputTensor
aten/src/ATen/native/mps/operations/Linear.mm:              input.scalar_type() == ScalarType::Half, "MPS device does not support linear for non-float inputs");
aten/src/ATen/native/mps/operations/Linear.mm:             (weight.scalar_type() == kFloat || (weight.scalar_type() == kHalf)),
aten/src/ATen/native/mps/operations/Linear.mm:              || grad_output.scalar_type() == ScalarType::Half, "MPS device does not support linear backward for non-float inputs");
aten/src/ATen/native/mps/operations/Linear.mm:              grad_output.scalar_type() == ScalarType::Half, "MPS device does not support linear backward for non-float inputs");
aten/src/ATen/native/mps/operations/LinearAlgebra.mm:              || self.scalar_type() == ScalarType::Half, "MPS device does not support mm for non-float inputs");
aten/src/ATen/native/mps/operations/LinearAlgebra.mm:              || self.scalar_type() == ScalarType::Half, "MPS device does not support addmm for non-float input");
aten/src/ATen/native/mps/operations/LinearAlgebra.mm:              || batch1.scalar_type() == ScalarType::Half, "MPS device does not support bmm for non-float inputs");
aten/src/ATen/native/mps/operations/LinearAlgebra.mm:              || batch1.scalar_type() == ScalarType::Half, "MPS device does not support addbmm or baddbmm for non-float inputs");
aten/src/ATen/native/mps/operations/Scalar.mm:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, self.scalar_type(), "_local_scalar_dense_mps", [&] {
aten/src/ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
aten/src/ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
aten/src/ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
aten/src/ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
aten/src/ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:  return {.value.h = scalar.to<at::Half>(), .size = sizeof(short)  , .type = type};
aten/src/ATen/native/DistributionTemplates.h:    std::is_same<scalar_t, at::Half>::value ||
aten/src/ATen/native/DistributionTemplates.h:    std::is_same<scalar_t, at::Half>::value ||
aten/src/ATen/native/DistributionTemplates.h:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, scalar_type, "check_random_fp_bounds", [&] {
aten/src/ATen/native/DistributionTemplates.h:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "random_update_from_to", [&] {
aten/src/ATen/native/DistributionTemplates.h:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "random_from_to_range_calc", [&] {
aten/src/ATen/native/DistributionTemplates.h:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "check_uniform_bounds", [&] {
aten/src/ATen/native/BatchLinearAlgebra.cpp:  // Half optimisation half precondition for some parts of the LAPACK / cuSOLVER
aten/src/ATen/native/quantized/QTensor.cpp:  TORCH_CHECK( (dtype == ScalarType::QInt8 || dtype == ScalarType::QUInt8 || dtype == ScalarType::Half), "dtype ", dtype, "not supported");
aten/src/ATen/native/quantized/QTensor.cpp:  if (dtype == ScalarType::Half) {
aten/src/ATen/native/quantized/QTensor.cpp:    return input_contig.to(ScalarType::Half);
aten/src/ATen/native/quantized/QTensor.cpp:  xmin = static_cast<at::Half>(xmin);
aten/src/ATen/native/quantized/QTensor.cpp:      : static_cast<float>(static_cast<at::Half>(data_range / qmax));
aten/src/ATen/native/quantized/cuda/EmbeddingBag.cu:      static_cast<std::int64_t>(input_columns - 2 * sizeof(at::Half)) *
aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:    fbgemm::Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<float>(
aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:      static_cast<std::int64_t>(input_columns - 2 * sizeof(at::Half)) *
aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:    fbgemm::FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<float>(
aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:    const at::Half* input_row_scale_zp = reinterpret_cast<const at::Half*>(
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:            weight_data + (idx + 1) * weight_size - 2 * sizeof(at::Half);
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:        at::Half scale_val = (reinterpret_cast<at::Half*>(&scale_val_int16))[0];
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:        at::Half bias_val = (reinterpret_cast<at::Half*>(&bias_val_int16))[0];
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:      (weight_size - 2 * sizeof(at::Half)) * NUM_ELEM_PER_BYTE; // NB: 2-byte fp16 scale and 2-byte zero_offset
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:         per_sample_weights_.value().scalar_type() == at::kHalf),
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:         per_sample_weights_.value().scalar_type() == at::kHalf),
aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp:         per_sample_weights_.value().scalar_type() == at::kHalf),
aten/src/ATen/native/quantized/cpu/LinearUnpackImpl.cpp:      at::empty({ncols, nrows}, at::kHalf, c10::MemoryFormat::Contiguous);
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:        4; // extra 4 bytes to store at::Half scale and bias per row.
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:            at::Half* output_row_scale_bias =
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:                reinterpret_cast<at::Half*>(output_row + embedding_cols);
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          weight.scalar_type() == at::ScalarType::Half,
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:  if (weight_contig->scalar_type() == at::ScalarType::Half) {
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:  const Tensor& float_weight = weight_contig->scalar_type() == at::ScalarType::Half
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          weight.scalar_type() == at::ScalarType::Half,
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          2 * sizeof(at::Half))};
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:    if (weight_contig.scalar_type() == at::ScalarType::Half) {
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:            fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:            fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float>(
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:        weight_contig.scalar_type() == at::ScalarType::Half
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:      Xmin = static_cast<at::Half>(Xmin);
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:      at::Half scale = range == 0 ? 1.0f : range / ((1 << bit_width) - 1);
aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:      at::Half* output_row_scale_zp = reinterpret_cast<at::Half*>(
aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp:  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Int || zero_point.scalar_type() == ScalarType::Float || zero_point.scalar_type() == ScalarType::Half,
aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp:              "Zero-point must be Int32, Float or Half, found ", zero_point.scalar_type());
aten/src/ATen/native/mkldnn/RNN.cpp:          tensor.scalar_type() == at::ScalarType::Half,
aten/src/ATen/native/vulkan/ops/Copy.cpp:  } else if (src.dtype() == at::kHalf) {
aten/src/ATen/native/vulkan/ops/Copy.cpp:    memcpy_to_mapping_impl<c10::Half>(src, dst_mapping);
aten/src/ATen/native/vulkan/ops/Copy.cpp:        " at::kHalf or at::Float but got ",
aten/src/ATen/native/vulkan/ops/Copy.cpp:  } else if (dst.dtype() == at::kHalf) {
aten/src/ATen/native/vulkan/ops/Copy.cpp:    memcpy_from_mapping_impl<c10::Half>(src_mapping, dst);
aten/src/ATen/native/vulkan/ops/Copy.cpp:        " at::kHalf or at::Float but got ",
aten/src/ATen/native/vulkan/ops/Copy.cpp:    // If the dtype() of src is at::kHalf, then first convert it to 32 bit
aten/src/ATen/native/vulkan/ops/Copy.cpp:    if (src.dtype() == at::kHalf) {
aten/src/ATen/native/vulkan/ops/Copy.cpp:    // If the dtype() of dst is at::kHalf, then copy the data into a float
aten/src/ATen/native/vulkan/ops/Copy.cpp:    if (dst.dtype() == at::kHalf) {
aten/src/ATen/native/vulkan/ops/Copy.cpp:      dst = dst_float.to(at::kHalf);
aten/src/ATen/native/vulkan/api/Resource.cpp: * TODO: enable proper format selection between kFloat and kHalf.
aten/src/ATen/native/vulkan/api/Resource.cpp: * always created with the corresponding VkFormat. Consequently, kHalf tensors
aten/src/ATen/native/vulkan/api/Resource.cpp:      return c10::kHalf;
aten/src/ATen/native/vulkan/api/Utils.h:#include <c10/util/Half.h> // For c10::overflows
aten/src/ATen/NumericUtils.h:#include <c10/util/Half.h>
aten/src/ATen/NumericUtils.h:    typename std::enable_if<std::is_same<T, at::Half>::value, int>::type = 0>
aten/src/ATen/NumericUtils.h:inline C10_HOST_DEVICE bool _isinf(at::Half val) {
aten/src/ATen/Utils.cpp:// AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, TENSOR)
aten/src/ATen/Utils.cpp:AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16, Float8, TENSOR)
aten/src/ATen/ops/tensor.h:// AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, TENSOR)
aten/src/ATen/ops/tensor.h:AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16,Float8, TENSOR)
aten/src/ATen/EmptyTensor.cpp:  if (dtype == kComplexHalf) {
aten/src/ATen/EmptyTensor.cpp:        "ComplexHalf support is experimental and many operators don't support it yet.");
aten/src/ATen/cudnn/Types.cpp:  } else if (dtype == at::kHalf) {
aten/src/ATen/cudnn/AutocastRNN.cpp:                                 is_eligible(weight_buf) && (weight_buf.scalar_type() != at::kHalf) :
aten/src/ATen/cudnn/AutocastRNN.cpp:                                 is_eligible(weight[0]) && (weight[0].scalar_type() != at::kHalf));
aten/src/ATen/cudnn/AutocastRNN.cpp:            /*flat_buf_datatype=*/at::native::getCudnnDataTypeFromScalarType(at::kHalf), // could just hardcode CUDNN_DATA_HALF
aten/src/ATen/cudnn/AutocastRNN.cpp:            /*flat_buf_options=*/weight[0].options().dtype(at::kHalf),
aten/src/ATen/cudnn/AutocastRNN.cpp:      cached_cast(at::kHalf, input),
aten/src/ATen/cudnn/AutocastRNN.cpp:      cached_cast(at::kHalf, hx),
aten/src/ATen/cudnn/AutocastRNN.cpp:      cached_cast(at::kHalf, cx),
aten/src/ATen/cudnn/Descriptors.cpp:  } else if (scalar_type == at::kHalf) {
aten/src/ATen/AccumulateType.h:#include <c10/util/Half.h>
aten/src/ATen/AccumulateType.h:struct AccumulateType<Half, true> {
aten/src/ATen/AccumulateType.h:struct AccumulateType<Half, false> {
aten/src/ATen/AccumulateType.h:struct AccumulateType<c10::complex<Half>, false> {
aten/src/ATen/AccumulateType.h:struct AccumulateType<c10::complex<Half>, true> {
aten/src/ATen/SparseCsrTensorUtils.h:          kComplexHalf, kHalf, kBool, kBFloat16, __VA_ARGS__))
aten/src/ATen/autocast_mode.cpp:thread_local at::ScalarType autocast_gpu_dtype = at::kHalf;
aten/src/ATen/test/basic.cpp:  if (type.backend() != Backend::CPU || type.scalarType() != kHalf) {
aten/src/ATen/test/basic.cpp:  if (type.backend() == Backend::CPU && type.scalarType() == kHalf) {
aten/src/ATen/test/basic.cpp:  if (type.backend() != Backend::CPU || type.scalarType() != kHalf) {
aten/src/ATen/test/basic.cpp:  if (type.backend() != Backend::CPU || type.scalarType() != kHalf) {
aten/src/ATen/test/basic.cpp:TEST(BasicTest, BasicTestHalfCPU) {
aten/src/ATen/test/basic.cpp:  test(CPU(kHalf));
aten/src/ATen/test/basic.cpp:  at::Tensor tensor1 = at::empty({4}, at::TensorOptions().dtype(at::kHalf));
aten/src/ATen/test/basic.cpp:  ASSERT_EQ(tensor1.dtype(), at::kHalf);
aten/src/ATen/test/basic.cpp:  tensor1 = at::empty({4}, at::TensorOptions().dtype(at::kHalf).layout(at::kSparse));
aten/src/ATen/test/basic.cpp:  ASSERT_EQ(tensor1.dtype(), at::kHalf);
aten/src/ATen/test/basic.cpp:    tensor1 = at::empty({4}, at::TensorOptions().dtype(at::kHalf).device(at::kCUDA).layout(at::kSparse).requires_grad(false));
aten/src/ATen/test/basic.cpp:    ASSERT_EQ(tensor1.dtype(), at::kHalf);
aten/src/ATen/test/mps_test_print.cpp:TEST(MPSPrintTest, PrintHalf4DTensor) {
aten/src/ATen/test/mps_test_print.cpp:  ss << torch::randn({2, 2, 2, 2}, at::device(at::kMPS).dtype(at::kHalf));
aten/src/ATen/test/mps_test_print.cpp:  ASSERT_TRUE (ends_with(ss.str(), "[ MPSHalfType{2,2,2,2} ]")) << " got " << ss.str();
aten/src/ATen/test/scalar_test.cpp:struct Foo<Half> {
aten/src/ATen/test/scalar_test.cpp:  s1.toHalf();
aten/src/ATen/test/scalar_test.cpp:  ASSERT_THROW(s1.toHalf(), std::runtime_error);
aten/src/ATen/test/scalar_test.cpp:  Half h = bar.toHalf();
aten/src/ATen/test/scalar_test.cpp:  if (x.scalar_type() != ScalarType::Half) {
aten/src/ATen/test/scalar_test.cpp:  ASSERT_EQ(float_one.item<at::Half>(), 1);
aten/src/ATen/test/cuda_half_test.cu:  assert(Half(3) == Half(3.0f));
aten/src/ATen/test/cuda_half_test.cu:  assert(static_cast<Half>(3.0f) == Half(3.0f));
aten/src/ATen/test/cuda_half_test.cu:  assert(static_cast<Half>(3.0f) == 3.0f);
aten/src/ATen/test/cuda_half_test.cu:  __half c = a - Half(b);
aten/src/ATen/test/cuda_half_test.cu:  assert(static_cast<Half>(c) == Half(1.0));
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::lgamma(Half(10.0)) - ::lgamma(10.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::exp(Half(1.0)) - ::exp(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::log(Half(1.0)) - ::log(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::log10(Half(1000.0)) - ::log10(1000.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::log1p(Half(0.0)) - ::log1p(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::log2(Half(1000.0)) - ::log2(1000.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::expm1(Half(1.0)) - ::expm1(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::cos(Half(0.0)) - ::cos(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::sin(Half(0.0)) - ::sin(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::sqrt(Half(100.0)) - ::sqrt(100.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::ceil(Half(2.4)) - ::ceil(2.4f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::floor(Half(2.7)) - ::floor(2.7f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::trunc(Half(2.7)) - ::trunc(2.7f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::acos(Half(-1.0)) - ::acos(-1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::cosh(Half(1.0)) - ::cosh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::acosh(Half(1.0)) - ::acosh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::acosh(Half(1.0)) - ::acosh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::asinh(Half(1.0)) - ::asinh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::atanh(Half(1.0)) - ::atanh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::asin(Half(1.0)) - ::asin(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::sinh(Half(1.0)) - ::sinh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::asinh(Half(1.0)) - ::asinh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::tan(Half(0.0)) - ::tan(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::atan(Half(1.0)) - ::atan(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::tanh(Half(1.0)) - ::tanh(1.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::erf(Half(10.0)) - ::erf(10.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::erfc(Half(10.0)) - ::erfc(10.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::abs(Half(-3.0)) - ::abs(-3.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::round(Half(2.3)) - ::round(2.3f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::pow(Half(2.0), Half(10.0)) - ::pow(2.0f, 10.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:      ::abs(::atan2(Half(7.0), Half(0.0)) - ::atan2(7.0f, 0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::isnan((float)Half(0.0)) - ::isnan(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::isinf((float)Half(0.0)) - ::isinf(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::isnan(Half(0.0)) - ::isnan(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  assert(::abs(::isinf(Half(0.0)) - ::isinf(0.0f)) <= threshold);
aten/src/ATen/test/cuda_half_test.cu:  Half real = 3.0f;
aten/src/ATen/test/cuda_half_test.cu:  Half imag = -10.0f;
aten/src/ATen/test/cuda_half_test.cu:  auto complex = c10::complex<Half>(real, imag);
aten/src/ATen/test/cuda_half_test.cu:TEST(HalfCuda, HalfCuda) {
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, Arithmetic) {
aten/src/ATen/test/half_test.cpp:  Half zero = 0;
aten/src/ATen/test/half_test.cpp:  Half one = 1;
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(one + one, Half(2));
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, Comparisions) {
aten/src/ATen/test/half_test.cpp:  Half zero = 0;
aten/src/ATen/test/half_test.cpp:  Half one = 1;
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, Cast) {
aten/src/ATen/test/half_test.cpp:  Half value = 1.5f;
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ((bool)Half(0.0f), false);
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, Construction) {
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half((short)3), Half(3.0f));
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half((unsigned short)3), Half(3.0f));
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half(3), Half(3.0f));
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half(3U), Half(3.0f));
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half(3LL), Half(3.0f));
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half(3ULL), Half(3.0f));
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(Half(3.5), Half(3.5f));
aten/src/ATen/test/half_test.cpp:static std::string to_string(const Half& h) {
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, Half2String) {
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(to_string(Half(3.5f)), "3.5");
aten/src/ATen/test/half_test.cpp:  ASSERT_EQ(to_string(Half(-100.0f)), "-100");
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, HalfNumericLimits) {
aten/src/ATen/test/half_test.cpp:  using limits = std::numeric_limits<Half>;
aten/src/ATen/test/half_test.cpp:// Check the declared type of members of numeric_limits<Half> matches
aten/src/ATen/test/half_test.cpp:          decltype(std::numeric_limits<Half>::name),          \
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, CommonMath) {
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::lgamma(Half(10.0)) - std::lgamma(10.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::exp(Half(1.0)) - std::exp(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::log(Half(1.0)) - std::log(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::log10(Half(1000.0)) - std::log10(1000.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::log1p(Half(0.0)) - std::log1p(0.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::log2(Half(1000.0)) - std::log2(1000.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::expm1(Half(1.0)) - std::expm1(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::cos(Half(0.0)) - std::cos(0.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::sin(Half(0.0)) - std::sin(0.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::sqrt(Half(100.0)) - std::sqrt(100.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::ceil(Half(2.4)) - std::ceil(2.4f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::floor(Half(2.7)) - std::floor(2.7f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::trunc(Half(2.7)) - std::trunc(2.7f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::acos(Half(-1.0)) - std::acos(-1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::cosh(Half(1.0)) - std::cosh(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::acosh(Half(1.0)) - std::acosh(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::asin(Half(1.0)) - std::asin(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::sinh(Half(1.0)) - std::sinh(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::asinh(Half(1.0)) - std::asinh(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::tan(Half(0.0)) - std::tan(0.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::atan(Half(1.0)) - std::atan(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::tanh(Half(1.0)) - std::tanh(1.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::erf(Half(10.0)) - std::erf(10.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::erfc(Half(10.0)) - std::erfc(10.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::abs(Half(-3.0)) - std::abs(-3.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::round(Half(2.3)) - std::round(2.3f)) <= threshold);
aten/src/ATen/test/half_test.cpp:      std::abs(std::pow(Half(2.0), Half(10.0)) - std::pow(2.0f, 10.0f)) <=
aten/src/ATen/test/half_test.cpp:      std::abs(std::atan2(Half(7.0), Half(0.0)) - std::atan2(7.0f, 0.0f)) <=
aten/src/ATen/test/half_test.cpp:  // @TODO: can macos do implicit conversion of Half?
aten/src/ATen/test/half_test.cpp:      std::abs(std::isnan(static_cast<float>(Half(0.0))) - std::isnan(0.0f)) <=
aten/src/ATen/test/half_test.cpp:      std::abs(std::isinf(static_cast<float>(Half(0.0))) - std::isinf(0.0f)) <=
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::isnan(Half(0.0)) - std::isnan(0.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:  assert(std::abs(std::isinf(Half(0.0)) - std::isinf(0.0f)) <= threshold);
aten/src/ATen/test/half_test.cpp:TEST(TestHalf, ComplexHalf) {
aten/src/ATen/test/half_test.cpp:  Half real = 3.0f;
aten/src/ATen/test/half_test.cpp:  Half imag = -10.0f;
aten/src/ATen/test/half_test.cpp:  auto complex = c10::complex<Half>(real, imag);
aten/src/ATen/test/reduce_ops_test.cpp:    for (const auto dtype : {kHalf, kFloat, kDouble, kShort, kInt, kLong}) {
aten/src/ATen/test/reduce_ops_test.cpp:      auto a = at::rand({H, W}, TensorOptions(kCUDA).dtype(at::kHalf));
aten/src/ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(Interleave FirstHalf), std::get<0>(cc), vec::loadu(interleaved)).check(true);
aten/src/ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(Interleave SecondHalf), std::get<1>(cc), vec::loadu(interleaved + vec::size())).check(true);
aten/src/ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(DeInterleave FirstHalf), std::get<0>(cc), vec::loadu(vals)).check(true);
aten/src/ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(DeInterleave SecondHalf), std::get<1>(cc), vec::loadu(vals + vec::size())).check(true);
aten/src/ATen/test/cuda_atomic_ops_test.cu:  test_atomic_add<at::Half>();
aten/src/ATen/test/cuda_atomic_ops_test.cu:  test_atomic_mul<at::Half>();
aten/src/ATen/test/cuda_atomic_ops_test.cu:  test_atomic_max<at::Half>();
aten/src/ATen/test/cuda_atomic_ops_test.cu:  test_atomic_min<at::Half>();




################################################################################################


benchmarks/operator_benchmark/pt/tensor_to_test.py:class FloatToHalfTensorConversionBenchmark(op_bench.TorchBenchmarkBase):
benchmarks/operator_benchmark/pt/tensor_to_test.py:class HalfToFloatTensorConversionBenchmark(op_bench.TorchBenchmarkBase):
benchmarks/operator_benchmark/pt/tensor_to_test.py:op_bench.generate_pt_test(tensor_conversion_short_configs, FloatToHalfTensorConversionBenchmark)
benchmarks/operator_benchmark/pt/tensor_to_test.py:op_bench.generate_pt_test(tensor_conversion_long_configs, FloatToHalfTensorConversionBenchmark)
benchmarks/operator_benchmark/pt/tensor_to_test.py:op_bench.generate_pt_test(tensor_conversion_short_configs, HalfToFloatTensorConversionBenchmark)
benchmarks/operator_benchmark/pt/tensor_to_test.py:op_bench.generate_pt_test(tensor_conversion_long_configs, HalfToFloatTensorConversionBenchmark)
benchmarks/static_runtime/test_static_runtime.cc:          at::ScalarType::Half,
benchmarks/static_runtime/test_static_runtime.cc:          at::ScalarType::Half,
benchmarks/cpp/nvfuser/softmax_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/softmax_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/softmax_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/softmax_backward.cpp:    grad_input = castOp(DataType::Half, grad_input);
benchmarks/cpp/nvfuser/softmax_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/softmax_backward.cpp:  Baseline_Softmax_BWD(benchmark_state, DataType::Half, 0);
benchmarks/cpp/nvfuser/softmax_backward.cpp:  Baseline_Softmax_BWD(benchmark_state, DataType::Half, 1);
benchmarks/cpp/nvfuser/softmax_backward.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/softmax_backward.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/matmul.cpp:  auto options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/matmul.cpp:  auto a = makeContigTensor(2, DataType::Half);
benchmarks/cpp/nvfuser/matmul.cpp:  auto b = makeContigTensor(2, DataType::Half);
benchmarks/cpp/nvfuser/matmul.cpp:      dataTypeSize(DataType::Half) * stage_number;
benchmarks/cpp/nvfuser/layer_norm.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/layer_norm.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/layer_norm.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/layer_norm.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/layer_norm.cpp:  Baseline_LayerNorm(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/layer_norm.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/reduction.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/reduction.cpp:    tv1_cast = castOp(DataType::Half, tv1);
benchmarks/cpp/nvfuser/reduction.cpp:  Baseline_Reduction(benchmark_state, DataType::Half, 0);
benchmarks/cpp/nvfuser/reduction.cpp:  Baseline_Reduction(benchmark_state, DataType::Half, 1);
benchmarks/cpp/nvfuser/reduction.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/reduction.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:    output = castOp(DataType::Half, output);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  Baseline_BatchNorm(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  Baseline_BatchNorm(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:  Baseline_BatchNorm(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/broadcast.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/broadcast.cpp:    tv3 = castOp(DataType::Half, tv3);
benchmarks/cpp/nvfuser/broadcast.cpp:  Baseline_Broadcast(benchmark_state, DataType::Half, 0);
benchmarks/cpp/nvfuser/broadcast.cpp:  Baseline_Broadcast(benchmark_state, DataType::Half, 1);
benchmarks/cpp/nvfuser/broadcast.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/broadcast.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/gelu_backward.cpp:  auto t0 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/gelu_backward.cpp:  auto t2 = makeContigTensor(1, DataType::Half);
benchmarks/cpp/nvfuser/gelu_backward.cpp:  auto t4 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/gelu_backward.cpp:  auto t27 = castOp(DataType::Half, t26);
benchmarks/cpp/nvfuser/gelu_backward.cpp:  auto options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/transpose.cpp:      DataType::Half,
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:    grad_input = castOp(DataType::Half, grad_input);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:    grad_weight = castOp(DataType::Half, grad_weight);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:    grad_bias = castOp(DataType::Half, grad_bias);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  Baseline_BatchNorm_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  Baseline_BatchNorm_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:  Baseline_BatchNorm_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_first_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/instance_norm.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/instance_norm.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/instance_norm.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/instance_norm.cpp:    output = castOp(DataType::Half, output);
benchmarks/cpp/nvfuser/instance_norm.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/instance_norm.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/instance_norm.cpp:  Baseline_InstanceNorm(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/instance_norm.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:    output = castOp(DataType::Half, output);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  Baseline_BatchNorm_nhwc(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  Baseline_BatchNorm_nhwc(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:  Baseline_BatchNorm_nhwc(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:    scale_bias_relu = castOp(DataType::Half, scale_bias_relu);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:    scale_bias_relu = castOp(DataType::Half, scale_bias_relu);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  Baseline_SBR(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/scale_bias_relu.cpp:  Baseline_SBR_Norm(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/layer_norm_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/layer_norm_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/layer_norm_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/layer_norm_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/layer_norm_backward.cpp:  Baseline_LayerNorm_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/layer_norm_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t7 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t39 = castOp(DataType::Half, t11);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t3 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t34 = castOp(DataType::Half, t33);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t0 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t6 = castOp(DataType::Half, t5);
benchmarks/cpp/nvfuser/timm.cpp:  auto t7 = castOp(DataType::Half, t3);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t0 = makeContigTensor(4, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t19 = castOp(DataType::Half, t18);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t0 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t18 = castOp(DataType::Half, t17);
benchmarks/cpp/nvfuser/timm.cpp:  auto t19 = castOp(DataType::Half, t3);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t0 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t18 = castOp(DataType::Half, t17);
benchmarks/cpp/nvfuser/timm.cpp:  auto t19 = castOp(DataType::Half, t3);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t1 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t3 = makeContigTensor(3, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t7 = makeContigTensor(1, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t9 = makeContigTensor(1, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t40 = castOp(DataType::Half, t12);
benchmarks/cpp/nvfuser/timm.cpp:  auto t42 = castOp(DataType::Half, t20);
benchmarks/cpp/nvfuser/timm.cpp:  auto t38 = castOp(DataType::Half, t15);
benchmarks/cpp/nvfuser/timm.cpp:  auto t44 = castOp(DataType::Half, t17);
benchmarks/cpp/nvfuser/timm.cpp:  auto t33 = castOp(DataType::Half, t32);
benchmarks/cpp/nvfuser/timm.cpp:  auto t34 = castOp(DataType::Half, t31);
benchmarks/cpp/nvfuser/timm.cpp:  auto t35 = castOp(DataType::Half, t25);
benchmarks/cpp/nvfuser/timm.cpp:  auto t36 = castOp(DataType::Half, t27);
benchmarks/cpp/nvfuser/timm.cpp:  auto t37 = castOp(DataType::Half, t28);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:  auto t2 = makeContigTensor(4, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t5 = makeContigTensor(4, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t7 = makeContigTensor(4, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t9 = makeContigTensor(4, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t4 = makeConcreteTensor({}, DataType::Half);
benchmarks/cpp/nvfuser/timm.cpp:  auto t29 = castOp(DataType::Half, t17);
benchmarks/cpp/nvfuser/timm.cpp:  auto t30 = castOp(DataType::Half, t19);
benchmarks/cpp/nvfuser/timm.cpp:  auto fp16_options = at::TensorOptions().dtype(at::kHalf).device(at::kCUDA, 0);
benchmarks/cpp/nvfuser/timm.cpp:// Norm inner dim Half version of vit_base_patch16_224_norm_inner3
benchmarks/cpp/nvfuser/rms_norm_backward.cpp:      dtype == DataType::Float || dtype == DataType::Half ||
benchmarks/cpp/nvfuser/rms_norm_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/rms_norm_backward.cpp:      dtype == DataType::Float || dtype == DataType::Half ||
benchmarks/cpp/nvfuser/rms_norm_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    grad_input = castOp(DataType::Half, grad_input);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    grad_weight = castOp(DataType::Half, grad_weight);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    grad_bias = castOp(DataType::Half, grad_bias);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  Baseline_BatchNorm_nhwc_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  Baseline_BatchNorm_nhwc_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:  Baseline_BatchNorm_nhwc_BWD(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/batch_norm_channels_last_backward.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/bert.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/bert.cpp:    tv14 = castOp(DataType::Half, tv14);
benchmarks/cpp/nvfuser/bert.cpp:    tv10 = castOp(DataType::Half, tv10);
benchmarks/cpp/nvfuser/bert.cpp:    tv3 = castOp(DataType::Half, tv3);
benchmarks/cpp/nvfuser/bert.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/bert.cpp:    tv10 = castOp(DataType::Half, tv10);
benchmarks/cpp/nvfuser/bert.cpp:    tv11 = castOp(DataType::Half, tv11);
benchmarks/cpp/nvfuser/bert.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/bert.cpp:    tv11 = castOp(DataType::Half, tv11);
benchmarks/cpp/nvfuser/bert.cpp:    tv14 = castOp(DataType::Half, tv14);
benchmarks/cpp/nvfuser/bert.cpp:    tv21 = castOp(DataType::Half, tv21);
benchmarks/cpp/nvfuser/bert.cpp:    tv26 = castOp(DataType::Half, tv26);
benchmarks/cpp/nvfuser/bert.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/bert.cpp:    tv24 = castOp(DataType::Half, tv24);
benchmarks/cpp/nvfuser/bert.cpp:    tv23 = castOp(DataType::Half, tv23);
benchmarks/cpp/nvfuser/bert.cpp:    tv8 = castOp(DataType::Half, tv8);
benchmarks/cpp/nvfuser/bert.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/bert.cpp:    tv21 = castOp(DataType::Half, tv21);
benchmarks/cpp/nvfuser/bert.cpp:  bool is_fp16 = dtype == DataType::Half;
benchmarks/cpp/nvfuser/bert.cpp:    tv26 = castOp(DataType::Half, tv27);
benchmarks/cpp/nvfuser/bert.cpp:    tv27 = castOp(DataType::Half, tv27);
benchmarks/cpp/nvfuser/bert.cpp:  MagicScheduler_DivMaxSoftDropFwd(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/bert.cpp:  MagicScheduler_DivMaxSoftDropBwd(benchmark_state, DataType::Half);
benchmarks/cpp/nvfuser/rms_norm.cpp:      dtype == DataType::Float || dtype == DataType::Half ||
benchmarks/cpp/nvfuser/rms_norm.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/rms_norm.cpp:      dtype == DataType::Float || dtype == DataType::Half ||
benchmarks/cpp/nvfuser/rms_norm.cpp:    DataType::Half);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/softmax_dropout.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/softmax_dropout.cpp:    attention_scores = castOp(DataType::Half, attention_scores);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:    attention_probs = castOp(DataType::Half, attention_probs);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:    output = castOp(DataType::Half, output);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:  Baseline_Softmax_Dropout(benchmark_state, 3, DataType::Half);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:  Baseline_Softmax_Dropout(benchmark_state, 1, DataType::Half);
benchmarks/cpp/nvfuser/softmax_dropout.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/softmax_dropout.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/softmax.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/softmax.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/softmax.cpp:  if (dtype == DataType::Half) {
benchmarks/cpp/nvfuser/softmax.cpp:    output = castOp(DataType::Half, output);
benchmarks/cpp/nvfuser/softmax.cpp:  TORCH_INTERNAL_ASSERT(dtype == DataType::Float || dtype == DataType::Half);
benchmarks/cpp/nvfuser/softmax.cpp:  Baseline_Softmax(benchmark_state, DataType::Half, 0);
benchmarks/cpp/nvfuser/softmax.cpp:  Baseline_Softmax(benchmark_state, DataType::Half, 1);
benchmarks/cpp/nvfuser/softmax.cpp:    DataType::Half,
benchmarks/cpp/nvfuser/softmax.cpp:    DataType::Half,



####################################################################################################


c10/test/util/Half_test.cpp:#include <c10/util/Half.h>
c10/test/util/Half_test.cpp:TEST(HalfDoubleConversionTest, Half2Double) {
c10/test/util/typeid_test.cpp:  EXPECT_NE(TypeMeta::Id<uint16_t>(), TypeMeta::Id<at::Half>());

####################################################################################################


caffe2/core/nomnigraph/include/nomnigraph/Representations/NeuralNet.h:  enum class DataType { Generic, Float, Half, Int8 };
caffe2/core/common_cudnn.h:class cudnnTypeWrapper<at::Half> {
caffe2/core/types.cc:  } else if (meta_id == TypeMeta::Id<at::Half>()) {
caffe2/core/types.cc:      return TypeMeta::Make<at::Half>();
caffe2/core/blob_test.cc:  // at::Half is smaller, so still should share buffer
caffe2/core/blob_test.cc:  EXPECT_TRUE(tensor.mutable_data<at::Half>() == (at::Half*)ptr);
caffe2/core/blob_test.cc:  EXPECT_TRUE(tensor.data<at::Half>() == (const at::Half*)ptr);
caffe2/core/blob_test.cc:  EXPECT_TRUE(tensor.dtype().Match<at::Half>());
caffe2/core/blob_test.cc:TEST(TensorTest, Half) {
caffe2/core/blob_test.cc:    tensor->mutable_data<at::Half>()[i].x = i % 10000;
caffe2/core/blob_test.cc:      tensor_proto.data_type(), TypeMetaToDataType(TypeMeta::Make<at::Half>()));
caffe2/core/blob_test.cc:      auto value = tensor->mutable_data<at::Half>()[i].x;
caffe2/core/blob_test.cc:    EXPECT_EQ(new_tensor.data<at::Half>()[i].x, i % 10000);
caffe2/core/types.h:#include <c10/util/Half.h>
caffe2/core/types.h:// at::Half is defined in c10/util/Half.h. Currently half float operators are
caffe2/core/types.h:inline bool fp16_type<at::Half>() {
caffe2/core/blob_serialization.cc:void SerializeTensorData(const SerializeParams<at::Half>& params) {
caffe2/core/blob_serialization.cc:    SERIALIZE_TYPE_CASE(FLOAT16, at::Half)
caffe2/core/blob_serialization.cc:DESERIALIZE_IMPL(at::Half, FMT_PROTOBUF) {
caffe2/core/blob_serialization.cc:  DeserializeFromBytesOrInt32<uint16_t, at::Half>(
caffe2/core/blob_serialization.cc:    DESERIALIZE_TYPE_CASE(FLOAT16, at::Half);
caffe2/core/hip/common_miopen.h:class miopenTypeWrapper<at::Half>
caffe2/core/hip/common_miopen.h:    static const miopenDataType_t type = miopenHalf;
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:      FloatToFusedNBitRowwiseQuantizedSBHalf(
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        at::Half* output_row_scale = reinterpret_cast<at::Half*>(
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        at::Half* output_row_bias = reinterpret_cast<at::Half*>(
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:            sizeof(at::Half));
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        Xmin = static_cast<at::Half>(Xmin);
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        at::Half scale = range == 0 ? 1.0f : range / ((1 << BIT_RATE) - 1);
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        static_cast<std::int64_t>(input_columns - 2 * sizeof(at::Half)) *
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:      FusedNBitRowwiseQuantizedSBHalfToFloat(
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        float scale = *reinterpret_cast<const at::Half*>(
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:        float bias = *reinterpret_cast<const at::Half*>(
caffe2/operators/fused_rowwise_nbit_conversion_ops.h:            sizeof(at::Half));
caffe2/operators/cast_op.cc:      CAFFE_THROW("Casting to and from at::Half on CPU is not supported yet");
caffe2/operators/cast_op.cc:    FLOAT16 = 12;  // at::Half
caffe2/operators/boolean_mask_ops.cu:    return DispatchHelper<TensorTypes<at::Half, float>>::call(this, Input(0));
caffe2/operators/max_pool_with_index.cu:  } else if (X.IsType<at::Half>()) {
caffe2/operators/max_pool_with_index.cu:    return DoRunWithType<at::Half>();
caffe2/operators/max_pool_with_index.cu:  } else if (X.IsType<at::Half>()) {
caffe2/operators/max_pool_with_index.cu:    return DoRunWithType<at::Half>();
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:        sizeof(at::Half) + sizeof(at::Half),
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:        static_cast<int64_t>(data.size(1) - 2 * sizeof(at::Half)) *
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:        const at::Half* scale_bias = reinterpret_cast<const at::Half*>(
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:            input_data + (idx + 1) * data.size(1) - 2 * sizeof(at::Half));
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:        sizeof(at::Half) + sizeof(at::Half),
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:            2 * (BIT_RATE == 8 ? sizeof(float) : sizeof(at::Half))) *
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:            2 * (BIT_RATE == 8 ? sizeof(float) : sizeof(at::Half));
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:          scale = weight * reinterpret_cast<const at::Half*>(scale_bias)[0];
caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h:          bias = weight * reinterpret_cast<const at::Half*>(scale_bias)[1];
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h:void convertfp16fp32(float* dst, const at::Half* src, size_t N);
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h:    if (!std::is_same<T, float>::value && !std::is_same<T, at::Half>::value) {
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h:      minimum_element = static_cast<at::Half>(minimum_element);
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h:          : static_cast<float>(static_cast<at::Half>(
caffe2/operators/softmax_op_cudnn.cc:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/softmax_op_cudnn.cc:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/copy_rows_to_tensor_op.h:        TensorTypes<at::Half, float, double, int32_t, int64_t>>::
caffe2/operators/copy_rows_to_tensor_op.h:        TensorTypes<at::Half, float, double, int32_t, int64_t>>::
caffe2/operators/tile_op.cc:      at::Half,
caffe2/operators/half_float_ops.cc:#include <c10/util/Half.h>
caffe2/operators/half_float_ops.cc:    at::Half* out,
caffe2/operators/half_float_ops.cc:inline void Float16ToFloat_ref(const at::Half* in, float* out, size_t N) {
caffe2/operators/half_float_ops.cc:bool FloatToHalfOp<CPUContext>::RunOnDevice() {
caffe2/operators/half_float_ops.cc:  auto* output = Output(0, input.sizes(), at::dtype<at::Half>());
caffe2/operators/half_float_ops.cc:  at::Half* out = output->template mutable_data<at::Half>();
caffe2/operators/half_float_ops.cc:bool HalfToFloatOp<CPUContext>::RunOnDevice() {
caffe2/operators/half_float_ops.cc:  const at::Half* data = input.template data<at::Half>();
caffe2/operators/half_float_ops.cc:REGISTER_CPU_OPERATOR(FloatToHalf, FloatToHalfOp<CPUContext>);
caffe2/operators/half_float_ops.cc:REGISTER_CPU_OPERATOR(HalfToFloat, HalfToFloatOp<CPUContext>);
caffe2/operators/half_float_ops.cc:OPERATOR_SCHEMA(FloatToHalf)
caffe2/operators/half_float_ops.cc:OPERATOR_SCHEMA(HalfToFloat)
caffe2/operators/half_float_ops.cc:  auto* output = Output(0, shape_, at::dtype<at::Half>());
caffe2/operators/half_float_ops.cc:  at::Half givenFp16Value = givenValue;
caffe2/operators/half_float_ops.cc:    at::Half* out = output->template mutable_data<at::Half>();
caffe2/operators/half_float_ops.cc:  auto* output = Output(0, shape_, at::dtype<at::Half>());
caffe2/operators/half_float_ops.cc:  at::Half* out = output->template mutable_data<at::Half>();
caffe2/operators/half_float_ops.cc:class GetFloatToHalfGradient : public GradientMakerBase {
caffe2/operators/half_float_ops.cc:        "HalfToFloat", "", vector<string>{GO(0)}, vector<string>{GI(0)});
caffe2/operators/half_float_ops.cc:REGISTER_GRADIENT(FloatToHalf, GetFloatToHalfGradient);
caffe2/operators/half_float_ops.cc:class GetHalfToFloatGradient : public GradientMakerBase {
caffe2/operators/half_float_ops.cc:        "FloatToHalf", "", vector<string>{GO(0)}, vector<string>{GI(0)});
caffe2/operators/half_float_ops.cc:REGISTER_GRADIENT(HalfToFloat, GetHalfToFloatGradient);
caffe2/operators/utility_ops.h:                   &ScatterAssignOp::DoRun<int32_t, at::Half>},
caffe2/operators/utility_ops.h:                   &ScatterAssignOp::DoRun<int64_t, at::Half>},
caffe2/operators/batch_matmul_op.cu:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/batch_matmul_op.cu:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/elu_op_cudnn.cc:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/elu_op_cudnn.cc:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/segment_reduction_op_gpu.cu:      return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/operators/segment_reduction_op_gpu.cu:      return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/operators/segment_reduction_op_gpu.cuh:struct SharedMemory<at::Half> {
caffe2/operators/segment_reduction_op_gpu.cuh:  __device__ at::Half* getPointer() {
caffe2/operators/segment_reduction_op_gpu.cuh:    extern __shared__ at::Half s_half[];
caffe2/operators/segment_reduction_op_gpu.cuh:__device__ inline float convert_type<at::Half, float>(const at::Half in) {
caffe2/operators/lengths_reducer_ops.h:  // Currently, we support float and at::Half inputs for input data type, and
caffe2/operators/lengths_reducer_ops.h:        CAFFE_ENFORCE((std::is_same<InputType, at::Half>::value));
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:  xmin = static_cast<at::Half>(xmin);
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:      : static_cast<float>(static_cast<at::Half>(data_range / qmax));
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:void convertfp16fp32(float* dst, const at::Half* src, size_t N) {
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:    HalfToFused4BitFakeRowwiseQuantized,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:OPERATOR_SCHEMA(HalfToFused4BitFakeRowwiseQuantized)
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:NO_GRADIENT(HalfToFused4BitFakeRowwiseQuantized);
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:    HalfToFused4BitFakeRowwiseQuantized,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:    HalfToFused2BitFakeRowwiseQuantized,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:OPERATOR_SCHEMA(HalfToFused2BitFakeRowwiseQuantized)
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:NO_GRADIENT(HalfToFused2BitFakeRowwiseQuantized);
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:    HalfToFused2BitFakeRowwiseQuantized,
caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc:        at::Half,
caffe2/operators/half_float_ops.h:class FloatToHalfOp : public Operator<Context> {
caffe2/operators/half_float_ops.h:  explicit FloatToHalfOp(const OperatorDef& operator_def, Workspace* ws)
caffe2/operators/half_float_ops.h:class HalfToFloatOp : public Operator<Context> {
caffe2/operators/half_float_ops.h:  USE_SIMPLE_CTOR_DTOR(HalfToFloatOp);
caffe2/operators/utility_ops.cu:  } else if (Input(0).IsType<at::Half>()) {
caffe2/operators/utility_ops.cu:    return DoRunWithType<at::Half>();
caffe2/operators/utility_ops.cu:  return DispatchHelper<TensorTypes<float, at::Half, int32_t, int64_t>>::call(
caffe2/operators/activation_ops_cudnn.h:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/activation_ops_cudnn.h:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/conv_op_cudnn.cc:    if (X.template IsType<at::Half>()) {
caffe2/operators/conv_op_cudnn.cc:                << "supported, input data is Half - using Half "
caffe2/operators/conv_op_cudnn.cc:                << "not supported, input data is Half - using float32 "
caffe2/operators/conv_op_cudnn.cc:                << "input data is Half - using float32 compute.";
caffe2/operators/conv_op_cudnn.cc:  } else if (Input(0).IsType<at::Half>()) {
caffe2/operators/conv_op_cudnn.cc:        at::Half, // X
caffe2/operators/conv_op_cudnn.cc:        at::Half, // W
caffe2/operators/conv_op_cudnn.cc:        at::Half, // B
caffe2/operators/conv_op_cudnn.cc:        at::Half>(); // Y
caffe2/operators/conv_op_cudnn.cc:    LOG(FATAL) << "Only float (32bit) and Half are supported by "
caffe2/operators/conv_op_cudnn.cc:  } else if (Input(0).IsType<at::Half>()) {
caffe2/operators/conv_op_cudnn.cc:        at::Half, //  X
caffe2/operators/conv_op_cudnn.cc:        at::Half, // dY
caffe2/operators/conv_op_cudnn.cc:        at::Half, //  W
caffe2/operators/conv_op_cudnn.cc:        at::Half, //  b
caffe2/operators/conv_op_cudnn.cc:        at::Half, // dX
caffe2/operators/conv_op_cudnn.cc:        at::Half, // dW
caffe2/operators/conv_op_cudnn.cc:        at::Half>(); // db
caffe2/operators/rnn/recurrent_network_op_gpu.cu:    const at::Half* src,
caffe2/operators/rnn/recurrent_network_op_gpu.cu:    at::Half* dst,
caffe2/operators/rnn/recurrent_network_op_gpu.cu:    initRecurrentInput_kernel<at::Half><<<repeat_n, CAFFE_CUDA_NUM_THREADS, 0, context->cuda_stream()>>>(
caffe2/operators/rnn/recurrent_network_op_gpu.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/rnn/recurrent_network_op_gpu.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/rnn/recurrent_network_op_gpu.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(1));
caffe2/operators/rnn/recurrent_network_op_gpu.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(1));
caffe2/operators/transpose_op_cudnn.cc:        std::is_same<T, at::Half>::value;
caffe2/operators/relu_op.cu:bool ReluFunctor<CUDAContext>::operator()<at::Half>(
caffe2/operators/relu_op.cu:    const at::Half* X,
caffe2/operators/relu_op.cu:    at::Half* Y,
caffe2/operators/relu_op.cu:bool ReluGradientFunctor<CUDAContext>::Forward<at::Half>(
caffe2/operators/relu_op.cu:    const at::Half* Y,
caffe2/operators/relu_op.cu:    const at::Half* dY,
caffe2/operators/relu_op.cu:    at::Half* dX,
caffe2/operators/relu_op.cu:        TensorTypes<float, at::Half>,
caffe2/operators/relu_op.cu:        TensorTypes<float, at::Half>,
caffe2/operators/stats_put_ops.h:        at::Half,
caffe2/operators/local_response_normalization_op_cudnn.cc:  } else if (X.IsType<at::Half>()) {
caffe2/operators/local_response_normalization_op_cudnn.cc:    return DoRunWithType<at::Half, float>();
caffe2/operators/local_response_normalization_op_cudnn.cc:  } else if (dY.IsType<at::Half>()) {
caffe2/operators/local_response_normalization_op_cudnn.cc:    return DoRunWithType<at::Half, float>();
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:void convertfp16fp32(float* dst, const at::Half* src, size_t N) {
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:void convertfp32fp16(at::Half* dst, const float* src, size_t N) {
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:    FloatToFused8BitRowwiseQuantizedHalfScaleBias,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:OPERATOR_SCHEMA(FloatToFused8BitRowwiseQuantizedHalfScaleBias)
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:          X.dims(X.dims().size() - 1) + 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:NO_GRADIENT(FloatToFused8BitRowwiseQuantizedHalfScaleBias);
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:    HalfFloatToFused8BitRowwiseQuantized,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:OPERATOR_SCHEMA(HalfFloatToFused8BitRowwiseQuantized)
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:NO_GRADIENT(HalfFloatToFused8BitRowwiseQuantized);
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:    HalfFloatToFused8BitRowwiseQuantizedHalfScaleBias,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:OPERATOR_SCHEMA(HalfFloatToFused8BitRowwiseQuantizedHalfScaleBias)
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:          X.dims(X.dims().size() - 1) + 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:NO_GRADIENT(HalfFloatToFused8BitRowwiseQuantizedHalfScaleBias);
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:    Fused8BitRowwiseQuantizedHalfScaleBiasToFloat,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:OPERATOR_SCHEMA(Fused8BitRowwiseQuantizedHalfScaleBiasToFloat)
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:          X.dims(X.dims().size() - 1) - 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:NO_GRADIENT(Fused8BitRowwiseQuantizedHalfScaleBiasToFloat);
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:    Fused8BitRowwiseQuantizedToHalfFloat,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:OPERATOR_SCHEMA(Fused8BitRowwiseQuantizedToHalfFloat)
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:HalfFloatToFused8BitRowwiseQuantized operator. The input is expected to
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:NO_GRADIENT(Fused8BitRowwiseQuantizedToHalfFloat);
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:    Fused8BitRowwiseQuantizedHalfScaleBiasToHalfFloat,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:OPERATOR_SCHEMA(Fused8BitRowwiseQuantizedHalfScaleBiasToHalfFloat)
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:          X.dims(X.dims().size() - 1) - 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_8bit_conversion_ops.cc:NO_GRADIENT(Fused8BitRowwiseQuantizedHalfScaleBiasToHalfFloat);
caffe2/operators/reduction_ops.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/elementwise_ops.cu:void device_reduce<at::Half>(
caffe2/operators/elementwise_ops.cu:    const at::Half* in,
caffe2/operators/elementwise_ops.cu:    at::Half* out,
caffe2/operators/elementwise_ops.cu:    math::Set<at::Half, CUDAContext>(
caffe2/operators/elementwise_ops.cu:        convert::To<float, at::Half>(1.),
caffe2/operators/elementwise_ops.cu:        buffer->template mutable_data<at::Half>(),
caffe2/operators/elementwise_ops.cu:      reinterpret_cast<const rocblas_half*>(buffer->data<at::Half>()),
caffe2/operators/elementwise_ops.cu:    math::Set<at::Half, CUDAContext>(
caffe2/operators/elementwise_ops.cu:        convert::To<float, at::Half>(1.),
caffe2/operators/elementwise_ops.cu:        buffer->template mutable_data<at::Half>(),
caffe2/operators/elementwise_ops.cu:      buffer->data<at::Half>(),
caffe2/operators/elementwise_ops.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/filler_op.cc:    FLOAT16 = 12;  // at::Half
caffe2/operators/order_switch_ops_cudnn.cc:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/order_switch_ops_cudnn.cc:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:void convertfp32fp16(at::Half* dst, const float* src, size_t N) {
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) + 1) / 2 + 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    HalfToFused4BitRowwiseQuantized,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    FloatToFusedNBitRowwiseQuantizedOp<4, at::Half, internal::convertfp16fp32>);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:OPERATOR_SCHEMA(HalfToFused4BitRowwiseQuantized)
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) + 1) / 2 + 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:NO_GRADIENT(HalfToFused4BitRowwiseQuantized);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) - 2 * sizeof(at::Half)) * 2);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    Fused4BitRowwiseQuantizedToHalf,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    FusedNBitRowwiseQuantizedToFloatOp<4, at::Half, internal::convertfp32fp16>);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:OPERATOR_SCHEMA(Fused4BitRowwiseQuantizedToHalf)
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) - 2 * sizeof(at::Half)) * 2);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:NO_GRADIENT(Fused4BitRowwiseQuantizedToHalf);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    HalfToFused4BitRowwiseQuantized,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:        at::Half,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) + 3) / 4 + 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    HalfToFused2BitRowwiseQuantized,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    FloatToFusedNBitRowwiseQuantizedOp<2, at::Half, internal::convertfp16fp32>);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:OPERATOR_SCHEMA(HalfToFused2BitRowwiseQuantized)
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) + 3) / 4 + 2 * sizeof(at::Half));
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:NO_GRADIENT(HalfToFused2BitRowwiseQuantized);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) - 2 * sizeof(at::Half)) * 4);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    Fused2BitRowwiseQuantizedToHalf,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    FusedNBitRowwiseQuantizedToFloatOp<2, at::Half, internal::convertfp32fp16>);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:OPERATOR_SCHEMA(Fused2BitRowwiseQuantizedToHalf)
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:          (X.dims(X.dims().size() - 1) - 2 * sizeof(at::Half)) * 4);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:NO_GRADIENT(Fused2BitRowwiseQuantizedToHalf);
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:    HalfToFused2BitRowwiseQuantized,
caffe2/operators/fused_rowwise_nbit_conversion_ops.cc:        at::Half,
caffe2/operators/half_float_ops.cu:__global__ void FloatToHalfKernel(const int N, const float* X, half* Y) {
caffe2/operators/half_float_ops.cu:__global__ void HalfToFloatKernel(const int N, const half* X, float* Y) {
caffe2/operators/half_float_ops.cu:bool FloatToHalfOp<CUDAContext>::RunOnDevice() {
caffe2/operators/half_float_ops.cu:  auto* Y = Output(0, X.sizes(), at::dtype<at::Half>());
caffe2/operators/half_float_ops.cu:  FloatToHalfKernel<<<
caffe2/operators/half_float_ops.cu:      reinterpret_cast<half*>(Y->template mutable_data<at::Half>()));
caffe2/operators/half_float_ops.cu:bool HalfToFloatOp<CUDAContext>::RunOnDevice() {
caffe2/operators/half_float_ops.cu:  HalfToFloatKernel<<<
caffe2/operators/half_float_ops.cu:      reinterpret_cast<const half*>(X.data<at::Half>()),
caffe2/operators/half_float_ops.cu:  auto* output = Output(0, shape_, at::dtype<at::Half>());
caffe2/operators/half_float_ops.cu:  at::Half* out = output->template mutable_data<at::Half>();
caffe2/operators/half_float_ops.cu:    FloatToHalfKernel<<<
caffe2/operators/half_float_ops.cu:REGISTER_CUDA_OPERATOR(FloatToHalf, FloatToHalfOp<CUDAContext>);
caffe2/operators/half_float_ops.cu:REGISTER_CUDA_OPERATOR(HalfToFloat, HalfToFloatOp<CUDAContext>);
caffe2/operators/dropout_op_cudnn.cc:  } else if (X.IsType<at::Half>()) {
caffe2/operators/dropout_op_cudnn.cc:    return DoRunWithType<at::Half, float>();
caffe2/operators/dropout_op_cudnn.cc:  } else if (dY.IsType<at::Half>()) {
caffe2/operators/dropout_op_cudnn.cc:    return DoRunWithType<at::Half, float>();
caffe2/operators/fully_connected_op_gpu.cc:  } else if (op->Input(0).template IsType<at::Half>()) {
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // X
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // W
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // B
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // Y
caffe2/operators/fully_connected_op_gpu.cc:            at::Half>(); // Math
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // X
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // W
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // B
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // Y
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // X
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // W
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // B
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // Y
caffe2/operators/fully_connected_op_gpu.cc:  } else if (op->Input(0).template IsType<at::Half>()) {
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, //  X
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, //  W
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dY
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, //  B
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dX
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dW
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dB
caffe2/operators/fully_connected_op_gpu.cc:            at::Half>(); // Math
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, //  X
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, //  W
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dY
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, //  B
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dX
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dW
caffe2/operators/fully_connected_op_gpu.cc:            at::Half, // dB
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, //  X
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, //  W
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // dY
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, //  B
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // dX
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // dW
caffe2/operators/fully_connected_op_gpu.cc:          at::Half, // dB
caffe2/operators/lengths_reducer_ops.cc:    CPUSparseLengthsReductionOp<float, TensorTypes<float, at::Half>, 0, 0>;
caffe2/operators/lengths_reducer_ops.cc:    CPUSparseLengthsReductionOp<float, TensorTypes<float, at::Half>, 1, 0>;
caffe2/operators/lengths_reducer_ops.cc:    CPUSparseLengthsReductionOp<float, TensorTypes<float, at::Half>, 0, 1>;
caffe2/operators/lengths_reducer_ops.cc:    CPUSparseLengthsReductionOp<float, TensorTypes<float, at::Half>, 1, 0, 1>);
caffe2/operators/scale_blobs_op.cu:  return DispatchHelper<TensorTypes<at::Half, float>>::call(this, Input(0));
caffe2/operators/scale_op_gpu.cc:  return DispatchHelper<TensorTypes<at::Half, float>>::call(this, Input(0));
caffe2/operators/half_float_ops_test.cc:  def.set_type("FloatToHalf");
caffe2/operators/half_float_ops_test.cc:  EXPECT_NO_THROW(outputTensor.data<at::Half>());
caffe2/operators/half_float_ops_test.cc:  def2.set_type("HalfToFloat");
caffe2/operators/half_float_ops_test.cc:  const at::Half* data = resultTensor.data<at::Half>();
caffe2/operators/half_float_ops_test.cc:    float x = caffe2::convert::Get<float, at::Half>(data[i]);
caffe2/operators/spatial_batch_norm_op_cudnn.cu:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/spatial_batch_norm_op_cudnn.cu:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/sparse_normalize_op.cc:bool SparseNormalizeOp<c10::Half, CPUContext>::RunOnDevice() {
caffe2/operators/sparse_normalize_op.cc:inline void Float16ToFloat_ref(const at::Half* in, float* out, size_t N) {
caffe2/operators/sparse_normalize_op.cc:bool SparseNormalizeOp<c10::Half, CPUContext>::DoRunWithType() {
caffe2/operators/sparse_normalize_op.cc:  const auto* paramIn = Input(PARAM).template data<c10::Half>();
caffe2/operators/sparse_normalize_op.cc:  auto* paramOut = Output(OUTPUT_PARAM)->template mutable_data<c10::Half>();
caffe2/operators/sparse_normalize_op.cc:    EigenVectorArrayMap<c10::Half>(Y, block_size) *=
caffe2/operators/sparse_normalize_op.cc:REGISTER_CPU_OPERATOR(Float16SparseNormalize, SparseNormalizeOp<c10::Half, CPUContext>);
caffe2/operators/cast_op.cu:          at::Half,
caffe2/operators/cast_op.cu:bool CastOp<CUDAContext>::DoRunWithDstType<at::Half>() {
caffe2/operators/cast_op.cu:          at::Half>,
caffe2/operators/cast_op.cu:      at::Half /* DstType */>::call(this, Input(0));
caffe2/operators/cast_op.cu:      body_ = &CastOp<CUDAContext>::DoRunWithDstType<at::Half>;
caffe2/operators/hip/local_response_normalization_op_miopen.hip:  } else if (X.IsType<at::Half>()) {
caffe2/operators/hip/local_response_normalization_op_miopen.hip:    return DoRunWithType<at::Half, float>();
caffe2/operators/hip/local_response_normalization_op_miopen.hip:  } else if (dY.IsType<at::Half>()) {
caffe2/operators/hip/local_response_normalization_op_miopen.hip:    return DoRunWithType<at::Half, float>();
caffe2/operators/hip/activation_ops_miopen.h:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/hip/activation_ops_miopen.h:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/hip/elu_op_miopen.hip:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/hip/elu_op_miopen.hip:    return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/hip/conv_op_miopen.hip:  } else if (Input(0).IsType<at::Half>()) {
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // X
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // W
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // B
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // Math
caffe2/operators/hip/conv_op_miopen.hip:        at::Half>(); // Y
caffe2/operators/hip/conv_op_miopen.hip:    LOG(FATAL) << "Only float (32bit) and Half are supported by "
caffe2/operators/hip/conv_op_miopen.hip:  } else if (Input(0).IsType<at::Half>()) {
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, //  X
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // dY
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, //  W
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, //  b
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // Math
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // dX
caffe2/operators/hip/conv_op_miopen.hip:        at::Half, // dW
caffe2/operators/hip/conv_op_miopen.hip:        at::Half>(); // db
caffe2/operators/hip/spatial_batch_norm_op_miopen.hip:  } else if (Input(0).IsType<at::Half>()){
caffe2/operators/hip/spatial_batch_norm_op_miopen.hip:    return DoRunWithType<at::Half, float>();
caffe2/operators/hip/spatial_batch_norm_op_miopen.hip:  } else if (Input(0).IsType<at::Half>()) {
caffe2/operators/hip/spatial_batch_norm_op_miopen.hip:    return DoRunWithType<at::Half, float>();
caffe2/operators/hip/pool_op_miopen.hip:    } else if (X.IsType<at::Half>()){
caffe2/operators/hip/pool_op_miopen.hip:      return DoRunWithType<at::Half, float>();
caffe2/operators/hip/pool_op_miopen.hip:    } else if (X.IsType<at::Half>()) {
caffe2/operators/hip/pool_op_miopen.hip:      return DoRunWithType<at::Half, float>();
caffe2/operators/lstm_unit_op_gpu.cu:void LSTMUnit<at::Half, CUDAContext>(
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* H_prev,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* C_prev,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* X,
caffe2/operators/lstm_unit_op_gpu.cu:    at::Half* C,
caffe2/operators/lstm_unit_op_gpu.cu:    at::Half* H,
caffe2/operators/lstm_unit_op_gpu.cu:  LSTMUnitKernel<at::Half, float><<<
caffe2/operators/lstm_unit_op_gpu.cu:void LSTMUnitGradient<at::Half, CUDAContext>(
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* C_prev,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* X,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* C,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* H,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* C_diff,
caffe2/operators/lstm_unit_op_gpu.cu:    const at::Half* H_diff,
caffe2/operators/lstm_unit_op_gpu.cu:    at::Half* H_prev_diff,
caffe2/operators/lstm_unit_op_gpu.cu:    at::Half* C_prev_diff,
caffe2/operators/lstm_unit_op_gpu.cu:    at::Half* X_diff,
caffe2/operators/lstm_unit_op_gpu.cu:  LSTMUnitGradientKernel<at::Half, float><<<
caffe2/operators/lstm_unit_op_gpu.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/lstm_unit_op_gpu.cu:  return DispatchHelper<TensorTypes<float, at::Half>>::call(this, Input(0));
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:    bool out_sb_half = std::is_same<Tsb, at::Half>::value;
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:        FloatToFusedNBitRowwiseQuantizedSBHalf(
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      bool is_half = std::is_same<T, at::Half>::value;
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:          FloatToFusedNBitRowwiseQuantizedSBHalf(
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:    bool in_sb_half = std::is_same<Tsb, at::Half>::value;
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:        FusedNBitRowwiseQuantizedSBHalfToFloat(
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:      bool is_half = std::is_same<T, at::Half>::value;
caffe2/operators/fused_rowwise_8bit_conversion_ops.h:          FusedNBitRowwiseQuantizedSBHalfToFloat(
caffe2/python/operator_test/shape_inference_test.py:        model.FloatToFused8BitRowwiseQuantizedHalfScaleBias('x', 'x_8bit')
caffe2/python/operator_test/shape_inference_test.py:        model.Fused8BitRowwiseQuantizedHalfScaleBiasToFloat('x_8bit', 'x_recovered')
caffe2/python/operator_test/shape_inference_test.py:    def testHalfInt8Conversion(self):
caffe2/python/operator_test/shape_inference_test.py:        model.HalfFloatToFused8BitRowwiseQuantized('x', 'x_8bit')
caffe2/python/operator_test/shape_inference_test.py:        model.Fused8BitRowwiseQuantizedToHalfFloat('x_8bit', 'x_recovered')
caffe2/python/operator_test/shape_inference_test.py:        model.HalfFloatToFused8BitRowwiseQuantizedHalfScaleBias('x', 'x_8bit')
caffe2/python/operator_test/shape_inference_test.py:        model.Fused8BitRowwiseQuantizedHalfScaleBiasToHalfFloat('x_8bit', 'x_recovered')
caffe2/python/operator_test/fused_nbit_rowwise_test.cc:TEST(OperatorSchemaTest, TensorInferenceNbitHalf) {
caffe2/python/operator_test/fused_nbit_rowwise_test.cc:        "HalfToFused" + std::to_string(bit_rate) + "BitRowwiseQuantized");
caffe2/python/operator_test/fused_nbit_rowwise_test.cc:        "HalfToFused" + std::to_string(bit_rate) + "BitRowwiseQuantized",
caffe2/python/operator_test/fused_nbit_rowwise_test.cc:TEST(OperatorSchemaTest, TensorInferenceNbitHalfBack) {
caffe2/python/operator_test/fused_nbit_rowwise_test.cc:        "Fused" + std::to_string(bit_rate) + "BitRowwiseQuantizedToHalf");
caffe2/python/operator_test/fused_nbit_rowwise_test.cc:        "Fused" + std::to_string(bit_rate) + "BitRowwiseQuantizedToHalf",
caffe2/python/pybind_state_dlpack.cc:      {TypeMeta::Id<at::Half>(), DLDataType{2, 16, 1}},
caffe2/python/pybind_state_dlpack.cc:             {16, TypeMeta::Make<at::Half>()},
caffe2/python/benchmarks/sparse_normalize_benchmark.py:        op = core.CreateOperator("FloatToHalf", "X", "X_fp16")
caffe2/python/layers_test.py:    def testHalfToFloatTypeInference(self):
caffe2/python/layers_test.py:        output = self.model.FloatToHalf(input, 1)
caffe2/python/layers_test.py:        output = self.model.HalfToFloat(output, 1)
caffe2/python/lengths_reducer_fused_8bit_rowwise_ops_test.py:            quantized_data = net.HalfFloatToFused8BitRowwiseQuantized(
caffe2/python/lengths_reducer_fused_8bit_rowwise_ops_test.py:            dequantized_data = net.Fused8BitRowwiseQuantizedToHalfFloat(
caffe2/python/lengths_reducer_fused_8bit_rowwise_ops_test.py:            quantized_data = net.HalfFloatToFused8BitRowwiseQuantized(
caffe2/python/lengths_reducer_fused_8bit_rowwise_ops_test.py:            dequantized_data = net.Fused8BitRowwiseQuantizedToHalfFloat(
caffe2/python/optimizer.py:        grad_fp32 = net.HalfToFloat(grad, grad + "_fp32")
caffe2/python/optimizer.py:        net.FloatToHalf(param_fp32, param)
caffe2/python/optimizer.py:        momentum_data = param_init_net.FloatToHalf(
caffe2/python/layers/sparse_lookup.py:            return net.HalfToFloat(gathered_w, out)
caffe2/python/optimizer_test_util.py:            out = model.HalfToFloat(out, out + "_fp32")
caffe2/python/optimizer_test_util.py:                fc_fp32_for_host = model.HalfToFloat('fc', 'fc_fp32_for_host')
caffe2/python/examples/imagenet_trainer.py:        model.param_init_net.FloatToHalf("data" + suffix, "data")
caffe2/python/examples/imagenet_trainer.py:            pred = model.net.HalfToFloat(pred, pred + '_fp32')
caffe2/python/examples/imagenet_trainer.py:            pred = model.net.HalfToFloat(pred, pred + '_fp32')
caffe2/python/examples/imagenet_trainer.py:                model.param_init_net.HalfToFloat(
caffe2/python/test/executor_test_util.py:                model.param_init_net.HalfToFloat(
caffe2/python/modeling/initializers.py:        param = init_net.FloatToHalf(
caffe2/python/modeling/initializers.py:        param_fp16 = init_net.FloatToHalf(
caffe2/python/pybind_state.cc:      {TypeMeta::Id<at::Half>(), NPY_FLOAT16},
caffe2/python/pybind_state.cc:      {NPY_FLOAT16, TypeMeta::Make<at::Half>()},
caffe2/python/serialized_test/SerializedTestCoverage.md:* FloatToFused8BitRowwiseQuantizedHalfScaleBias
caffe2/python/serialized_test/SerializedTestCoverage.md:* FloatToHalf
caffe2/python/serialized_test/SerializedTestCoverage.md:* Fused2BitRowwiseQuantizedToHalf
caffe2/python/serialized_test/SerializedTestCoverage.md:* Fused4BitRowwiseQuantizedToHalf
caffe2/python/serialized_test/SerializedTestCoverage.md:* Fused8BitRowwiseQuantizedHalfScaleBiasToFloat
caffe2/python/serialized_test/SerializedTestCoverage.md:* Fused8BitRowwiseQuantizedHalfScaleBiasToHalfFloat
caffe2/python/serialized_test/SerializedTestCoverage.md:* Fused8BitRowwiseQuantizedToHalfFloat
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfFloatToFused8BitRowwiseQuantized
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfFloatToFused8BitRowwiseQuantizedHalfScaleBias
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfToFloat
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfToFused2BitFakeRowwiseQuantized
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfToFused2BitRowwiseQuantized
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfToFused4BitFakeRowwiseQuantized
caffe2/python/serialized_test/SerializedTestCoverage.md:* HalfToFused4BitRowwiseQuantized
caffe2/opt/custom/in_batch_broadcast.cc:          "FloatToHalf",
caffe2/opt/custom/in_batch_broadcast.cc:          "HalfToFloat",
caffe2/opt/custom/in_batch_broadcast_test.cc:      CreateOperatorDef("FloatToHalf", "", {"blob"}, {"blob_half"}, {}));
caffe2/opt/custom/in_batch_broadcast_test.cc:      "FloatToHalf",
caffe2/opt/custom/in_batch_broadcast_test.cc:      "HalfToFloat",
caffe2/opt/custom/in_batch_broadcast_test.cc:      CreateOperatorDef("FloatToHalf", "", {"blob_tile"}, {"blob_half"}, {}));
caffe2/opt/custom/in_batch_broadcast_test.cc:      "FloatToHalf",
caffe2/opt/custom/in_batch_broadcast_test.cc:      "HalfToFloat",
caffe2/opt/onnxifi_transformer.cc:        "HalfToFloat",
caffe2/opt/onnxifi_transformer.cc:    op2.set_type("FloatToHalf");
caffe2/opt/onnxifi_transformer.cc:          "HalfToFloat",
caffe2/opt/bound_shape_inferencer.cc:      op.type() == "HalfFloatToFused8BitRowwiseQuantized" ||
caffe2/opt/bound_shape_inferencer.cc:      op.type() == "HalfToFused4BitRowwiseQuantized" ||
caffe2/opt/bound_shape_inferencer.cc:      op.type() == "FloatToHalf" || op.type() == "FbGemmPack") {
caffe2/opt/onnxifi_op.cc:  } else if (cpu_tensor.template IsType<c10::Half>()) {
caffe2/opt/onnxifi_op.cc:    desc->buffer = reinterpret_cast<onnxPointer>(cpu_tensor.data<c10::Half>());
caffe2/opt/onnxifi_op.cc:      {ONNXIFI_DATATYPE_FLOAT16, TypeMeta::Make<c10::Half>()},
caffe2/proto/caffe2.proto:    FLOAT16 = 12; // at::Half
caffe2/sgd/fp16_momentum_sgd_op.cu:    const at::Half* g,
caffe2/sgd/fp16_momentum_sgd_op.cu:    const at::Half* m,
caffe2/sgd/fp16_momentum_sgd_op.cu:    at::Half* ng,
caffe2/sgd/fp16_momentum_sgd_op.cu:    at::Half* nm,
caffe2/sgd/fp16_momentum_sgd_op.cu:    at::Half* param,
caffe2/sgd/fp16_momentum_sgd_op.cu:    FP16MomentumSGDUpdateOp<at::Half, CUDAContext>);
caffe2/sgd/adagrad_fused_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/adagrad_fused_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/adagrad_fused_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/adagrad_fused_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/adagrad_fused_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/rowwise_adagrad_fused.h:    const at::Half* w,
caffe2/sgd/rowwise_adagrad_fused.h:        } else if (std::is_same<Tdata, at::Half>::value) {
caffe2/sgd/adagrad_fused_op_gpu.cuh:inline __device__ randFactor<at::Half, float, STOCHASTIC>::randFactor(
caffe2/sgd/adagrad_fused_op_gpu.cuh:randFactor<at::Half, float, NEAREST>::convertTypeFromParamToTarget(
caffe2/sgd/adagrad_fused_op_gpu.cuh:    at::Half param) {
caffe2/sgd/adagrad_fused_op_gpu.cuh:randFactor<at::Half, float, STOCHASTIC>::convertTypeFromParamToTarget(
caffe2/sgd/adagrad_fused_op_gpu.cuh:    at::Half param) {
caffe2/sgd/adagrad_fused_op_gpu.cuh:inline __device__ at::Half
caffe2/sgd/adagrad_fused_op_gpu.cuh:randFactor<at::Half, float, STOCHASTIC>::convertTypeFromTargetToParam(
caffe2/sgd/adagrad_fused_op_gpu.cuh:inline __device__ at::Half
caffe2/sgd/adagrad_fused_op_gpu.cuh:randFactor<at::Half, float, NEAREST>::convertTypeFromTargetToParam(
caffe2/sgd/adagrad_fused_op_gpu.cuh:static inline __device__ void gpuAtomicAdd(c10::Half* address, c10::Half val) {
caffe2/sgd/adagrad_fused_op_gpu.cuh:    at::Half hsum;
caffe2/sgd/adagrad_op_gpu.cu:template <typename SIndex, typename THalf>
caffe2/sgd/adagrad_op_gpu.cu:    THalf* param,
caffe2/sgd/adagrad_op_gpu.cu:    THalf* param_mom,
caffe2/sgd/adagrad_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/adagrad_op_gpu.cu:  template <typename IndexType, typename THalf>
caffe2/sgd/adagrad_op_gpu.cu:    const auto* paramIn = Input(PARAM).template data<THalf>();
caffe2/sgd/adagrad_op_gpu.cu:    const auto* momentIn = Input(MOMENT_1).template data<THalf>();
caffe2/sgd/adagrad_op_gpu.cu:    auto* paramOut = Output(OUTPUT_PARAM)->template mutable_data<THalf>();
caffe2/sgd/adagrad_op_gpu.cu:    auto* momentOut = Output(OUTPUT_MOMENT_1)->template mutable_data<THalf>();
caffe2/sgd/adagrad_op_gpu.cu:    SparseAdagradKernel<IndexType, THalf>
caffe2/sgd/adagrad_op_gpu.cu:            Output(OUTPUT_PARAM)->template mutable_data<THalf>(),
caffe2/sgd/adagrad_op_gpu.cu:            Output(OUTPUT_MOMENT_1)->template mutable_data<THalf>(),
caffe2/sgd/fp16_momentum_sgd_op.h:    const at::Half* g,
caffe2/sgd/fp16_momentum_sgd_op.h:    const at::Half* m,
caffe2/sgd/fp16_momentum_sgd_op.h:    at::Half* ng,
caffe2/sgd/fp16_momentum_sgd_op.h:    at::Half* nm,
caffe2/sgd/fp16_momentum_sgd_op.h:    at::Half* param,
caffe2/sgd/math_lp.cc:void dot<float, at::Half, float>(
caffe2/sgd/math_lp.cc:    const at::Half* y,
caffe2/sgd/adadelta_op_gpu.cu:template <typename SIndex, typename THalf>
caffe2/sgd/adadelta_op_gpu.cu:    THalf* param,
caffe2/sgd/adadelta_op_gpu.cu:    THalf* param_mom,
caffe2/sgd/adadelta_op_gpu.cu:    THalf* param_mom_delta) {
caffe2/sgd/adadelta_op_gpu.cu:    return DispatchHelper<TensorTypes2<float, at::Half>, IndexType>::call(
caffe2/sgd/adadelta_op_gpu.cu:  template <typename IndexType, typename THalf>
caffe2/sgd/adadelta_op_gpu.cu:    const auto* paramIn = Input(PARAM).template data<THalf>();
caffe2/sgd/adadelta_op_gpu.cu:    const auto* momentIn = Input(MOMENT_GRAD).template data<THalf>();
caffe2/sgd/adadelta_op_gpu.cu:    const auto* momentDeltaIn = Input(MOMENT_DELTA).template data<THalf>();
caffe2/sgd/adadelta_op_gpu.cu:    auto* paramOut = Output(OUTPUT_PARAM)->template mutable_data<THalf>();
caffe2/sgd/adadelta_op_gpu.cu:        Output(OUTPUT_MOMENT_GRAD)->template mutable_data<THalf>();
caffe2/sgd/adadelta_op_gpu.cu:        Output(OUTPUT_MOMENT_DELTA)->template mutable_data<THalf>();
caffe2/sgd/adadelta_op_gpu.cu:    SparseAdadeltaKernel<IndexType, THalf>
caffe2/sgd/math_lp.h:void dot<float, at::Half, float>(
caffe2/sgd/math_lp.h:    const at::Half* y,
caffe2/image/image_input_op_gpu.cc:        OperatorBase::OutputTensor(0, dims, at::dtype<at::Half>().device(type));
caffe2/image/image_input_op_gpu.cc:    TransformOnGPU<uint8_t, at::Half, CUDAContext>(
caffe2/image/transform_gpu.cu:template bool TransformOnGPU<uint8_t, at::Half, CUDAContext>(
caffe2/utils/math/half_utils.h:struct HalfAddFunctor {
caffe2/utils/math/half_utils.h:  MATH_UTILS_DECL at::Half operator()(const at::Half a, const at::Half b)
caffe2/utils/math/half_utils.h:    return convert::To<float, at::Half>(
caffe2/utils/math/half_utils.h:        convert::To<at::Half, float>(a) + convert::To<at::Half, float>(b));
caffe2/utils/math/half_utils.h:struct HalfSubFunctor {
caffe2/utils/math/half_utils.h:  MATH_UTILS_DECL at::Half operator()(const at::Half a, const at::Half b)
caffe2/utils/math/half_utils.h:    return convert::To<float, at::Half>(
caffe2/utils/math/half_utils.h:        convert::To<at::Half, float>(a) - convert::To<at::Half, float>(b));
caffe2/utils/math/half_utils.h:struct HalfMulFunctor {
caffe2/utils/math/half_utils.h:  MATH_UTILS_DECL at::Half operator()(const at::Half a, const at::Half b)
caffe2/utils/math/half_utils.h:    return convert::To<float, at::Half>(
caffe2/utils/math/half_utils.h:        convert::To<at::Half, float>(a) * convert::To<at::Half, float>(b));
caffe2/utils/math/half_utils.h:struct HalfDivFunctor {
caffe2/utils/math/half_utils.h:  MATH_UTILS_DECL at::Half operator()(const at::Half a, const at::Half b)
caffe2/utils/math/half_utils.h:    return convert::To<float, at::Half>(
caffe2/utils/math/half_utils.h:        convert::To<at::Half, float>(a) / convert::To<at::Half, float>(b));
caffe2/utils/math/elementwise.cu:  __global__ void AxpyCUDAKernel<TAlpha, at::Half>(                    \
caffe2/utils/math/elementwise.cu:      const at::Half* X,                                               \
caffe2/utils/math/elementwise.cu:      at::Half* Y) {                                                   \
caffe2/utils/math/elementwise.cu:      Y[index] = convert::To<TAlpha, at::Half>(FMAFunc(                \
caffe2/utils/math/elementwise.cu:          convert::To<at::Half, TAlpha>(X[index]),                     \
caffe2/utils/math/elementwise.cu:          convert::To<at::Half, TAlpha>(Y[index])));                   \
caffe2/utils/math/elementwise.cu:  __global__ void AxpyCUDAKernel<TAlpha, at::Half>(                    \
caffe2/utils/math/elementwise.cu:      const at::Half* X,                                               \
caffe2/utils/math/elementwise.cu:      at::Half* Y) {                                                   \
caffe2/utils/math/elementwise.cu:      Y[index] = convert::To<TAlpha, at::Half>(FMAFunc(                \
caffe2/utils/math/elementwise.cu:          convert::To<at::Half, TAlpha>(X[index]),                     \
caffe2/utils/math/elementwise.cu:          convert::To<at::Half, TAlpha>(Y[index])));                   \
caffe2/utils/math/elementwise.cu:  __global__ void AxpbyCUDAKernel<TAlpha, at::Half>(                   \
caffe2/utils/math/elementwise.cu:      const at::Half* X,                                               \
caffe2/utils/math/elementwise.cu:      at::Half* Y) {                                                   \
caffe2/utils/math/elementwise.cu:      Y[index] = convert::To<TAlpha, at::Half>(FMAFunc(                \
caffe2/utils/math/elementwise.cu:          convert::To<at::Half, TAlpha>(X[index]),                     \
caffe2/utils/math/elementwise.cu:          beta * convert::To<at::Half, TAlpha>(Y[index])));            \
caffe2/utils/math/elementwise.cu:  __global__ void AxpbyCUDAKernel<TAlpha, at::Half>(                   \
caffe2/utils/math/elementwise.cu:      const at::Half* X,                                               \
caffe2/utils/math/elementwise.cu:      at::Half* Y) {                                                   \
caffe2/utils/math/elementwise.cu:      Y[index] = convert::To<TAlpha, at::Half>(FMAFunc(                \
caffe2/utils/math/elementwise.cu:          convert::To<at::Half, TAlpha>(X[index]),                     \
caffe2/utils/math/elementwise.cu:          b * convert::To<at::Half, TAlpha>(Y[index])));               \
caffe2/utils/math/elementwise.cu:  __global__ void ScaleCUDAKernel<TAlpha, at::Half>(                   \
caffe2/utils/math/elementwise.cu:      const at::Half* X,                                               \
caffe2/utils/math/elementwise.cu:      at::Half* Y) {                                                   \
caffe2/utils/math/elementwise.cu:      Y[index] = convert::To<TAlpha, at::Half>(                        \
caffe2/utils/math/elementwise.cu:          alpha * convert::To<at::Half, TAlpha>(X[index]));            \
caffe2/utils/math/elementwise.cu:  __global__ void ScaleCUDAKernel<TAlpha, at::Half>(                   \
caffe2/utils/math/elementwise.cu:      const at::Half* X,                                               \
caffe2/utils/math/elementwise.cu:      at::Half* Y) {                                                   \
caffe2/utils/math/elementwise.cu:      Y[index] = convert::To<TAlpha, at::Half>(                        \
caffe2/utils/math/elementwise.cu:          a * convert::To<at::Half, TAlpha>(X[index]));                \
caffe2/utils/math/elementwise.cu:CAFFE2_SPECIALIZED_CUDA_SET(at::Half)
caffe2/utils/math/elementwise.cu:DELEGATE_CUDA_SCALE_EX(float, at::Half, CUDA_R_32F, CUDA_R_16F, CUDA_R_32F)
caffe2/utils/math/elementwise.cu:CAFFE2_SPECIALIZED_CUDA_SCALE(float, at::Half)
caffe2/utils/math/elementwise.cu:DELEGATE_SIMPLE_CUDA_BINARY_FUNCTION(at::Half, Add, utils::HalfAddFunctor())
caffe2/utils/math/elementwise.cu:DELEGATE_SIMPLE_CUDA_BINARY_FUNCTION(at::Half, Sub, utils::HalfSubFunctor())
caffe2/utils/math/elementwise.cu:DELEGATE_SIMPLE_CUDA_BINARY_FUNCTION(at::Half, Mul, utils::HalfMulFunctor())
caffe2/utils/math/elementwise.cu:DELEGATE_SIMPLE_CUDA_BINARY_FUNCTION(at::Half, Div, utils::HalfDivFunctor())
caffe2/utils/math/elementwise.cu:DELEGATE_CUDA_AXPY_EX(float, at::Half, CUDA_R_32F, CUDA_R_16F, CUDA_R_32F)
caffe2/utils/math/elementwise.cu:CAFFE2_SPECIALIZED_CUDA_AXPY(float, at::Half)
caffe2/utils/math/elementwise.cu:CAFFE2_SPECIALIZED_CUDA_AXPBY(float, at::Half)
caffe2/utils/math_gpu.cu:  struct Func##Functor<at::Half> {                                      \
caffe2/utils/math_gpu.cu:    inline __host__ __device__ at::Half operator()(                     \
caffe2/utils/math_gpu.cu:        const at::Half& lhs,                                            \
caffe2/utils/math_gpu.cu:        const at::Half& rhs) const {                                    \
caffe2/utils/math_gpu.cu:      return convert::To<float, at::Half>(convert::To<at::Half, float>( \
caffe2/utils/math_gpu.cu:          lhs) expr convert::To<at::Half, float>(rhs));                 \
caffe2/utils/math_gpu.cu:  DELEGATE_2D_BROADCAST_CUDA_BINARY_FUNCTION(at::Half, at::Half, Func, Op)
caffe2/utils/math_gpu.cu:  DELEGATE_BROADCAST_CUDA_BINARY_FUNCTION(at::Half, at::Half, Func, Op)
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void Gemm<at::Half, CUDAContext>(
caffe2/utils/math_gpu.cu:    const at::Half* A,
caffe2/utils/math_gpu.cu:    const at::Half* B,
caffe2/utils/math_gpu.cu:    at::Half* C,
caffe2/utils/math_gpu.cu:    const __half alpha_fp16 = at::Half(alpha);
caffe2/utils/math_gpu.cu:    const __half beta_fp16 = at::Half(beta);
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void GemmBatched<at::Half, CUDAContext>(
caffe2/utils/math_gpu.cu:    const at::Half** A,
caffe2/utils/math_gpu.cu:    const at::Half** B,
caffe2/utils/math_gpu.cu:    at::Half** C,
caffe2/utils/math_gpu.cu:    const __half alpha_fp16 = at::Half(alpha);
caffe2/utils/math_gpu.cu:    const __half beta_fp16 = at::Half(beta);
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void GemmStridedBatched<at::Half, CUDAContext>(
caffe2/utils/math_gpu.cu:    const at::Half* A,
caffe2/utils/math_gpu.cu:    const at::Half* B,
caffe2/utils/math_gpu.cu:    at::Half* C,
caffe2/utils/math_gpu.cu:    const __half alpha_fp16 = at::Half(alpha);
caffe2/utils/math_gpu.cu:    const __half beta_fp16 = at::Half(beta);
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void Gemv<at::Half, CUDAContext>(
caffe2/utils/math_gpu.cu:    const at::Half* A,
caffe2/utils/math_gpu.cu:    const at::Half* x,
caffe2/utils/math_gpu.cu:    at::Half* y,
caffe2/utils/math_gpu.cu:    const __half alpha_fp16 = at::Half(alpha);
caffe2/utils/math_gpu.cu:    const __half beta_fp16 = at::Half(beta);
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void Gemm<at::Half, CUDAContext, TensorCoreEngine>(
caffe2/utils/math_gpu.cu:    const at::Half* A,
caffe2/utils/math_gpu.cu:    const at::Half* B,
caffe2/utils/math_gpu.cu:    at::Half* C,
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void GemmBatched<at::Half, CUDAContext, TensorCoreEngine>(
caffe2/utils/math_gpu.cu:    const at::Half** A,
caffe2/utils/math_gpu.cu:    const at::Half** B,
caffe2/utils/math_gpu.cu:    at::Half** C,
caffe2/utils/math_gpu.cu:  GemmBatched<at::Half, CUDAContext, DefaultEngine>(
caffe2/utils/math_gpu.cu:GemmStridedBatched<at::Half, CUDAContext, TensorCoreEngine>(
caffe2/utils/math_gpu.cu:    const at::Half* A,
caffe2/utils/math_gpu.cu:    const at::Half* B,
caffe2/utils/math_gpu.cu:    at::Half* C,
caffe2/utils/math_gpu.cu:  GemmStridedBatched<at::Half, CUDAContext, DefaultEngine>(
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void Gemv<at::Half, CUDAContext, TensorCoreEngine>(
caffe2/utils/math_gpu.cu:    const at::Half* A,
caffe2/utils/math_gpu.cu:    const at::Half* x,
caffe2/utils/math_gpu.cu:    at::Half* y,
caffe2/utils/math_gpu.cu:  Gemv<at::Half, CUDAContext, DefaultEngine>(
caffe2/utils/math_gpu.cu:CAFFE2_SPECIALIZED_CUDA_ADD_STRIPED_BATCH(at::Half);
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void Dot<at::Half, CUDAContext>(
caffe2/utils/math_gpu.cu:    const at::Half* a,
caffe2/utils/math_gpu.cu:    const at::Half* b,
caffe2/utils/math_gpu.cu:    at::Half* y,
caffe2/utils/math_gpu.cu:CAFFE2_MATH_SUM_FUNC(at::Half)
caffe2/utils/math_gpu.cu:CAFFE2_MATH_SUMSQR_FUNC(at::Half)
caffe2/utils/math_gpu.cu:CAFFE2_CUDA_EXPORT void Select<at::Half, CUDAContext>(
caffe2/utils/math_gpu.cu:    const at::Half* x,
caffe2/utils/math_gpu.cu:    at::Half* y,
caffe2/utils/math_gpu.cu:  SelectKernel<at::Half>
caffe2/utils/math_test.cc:TEST(MathTest, FloatToHalfConversion) {
caffe2/utils/math_test.cc:  float converted_a = static_cast<float>(at::Half(a));
caffe2/utils/math_test.cc:  float converted_b = static_cast<float>(at::Half(b));
caffe2/utils/math_test.cc:  float converted_c = static_cast<float>(at::Half(c));
caffe2/quantization/server/fb_fc_packed_op.cc:  return TypeMeta::Make<at::Half>();
caffe2/contrib/fakelowp/lengths_reducer_fused_4bit_rowwise_fp16_fake_op.h:        sizeof(at::Half) * 2,
caffe2/contrib/fakelowp/lengths_reducer_fused_4bit_rowwise_fp16_fake_op.h:        static_cast<int64_t>(data.size(1) - 2 * sizeof(at::Half)) *
caffe2/contrib/fakelowp/lengths_reducer_fused_4bit_rowwise_fp16_fake_op.h:    const auto scale_bias_offset = 2 * sizeof(at::Half);
caffe2/contrib/fakelowp/lengths_reducer_fused_4bit_rowwise_fp16_fake_op.h:        const at::Half* scale_bias = reinterpret_cast<const at::Half*>(
caffe2/contrib/fakelowp/lengths_reducer_ops.h:  // Currently, we support float and at::Half inputs for input data type, and
caffe2/contrib/fakelowp/lengths_reducer_ops.h:          if (std::is_same<InputType, at::Half>::value) {
caffe2/contrib/fakelowp/lengths_reducer_ops.h:          if (std::is_same<InputType, at::Half>::value) {
caffe2/contrib/fakelowp/lengths_reducer_ops.h:          if (std::is_same<InputType, at::Half>::value) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:auto sig_lut = std::vector<at::Half>{
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:at::Half CalcSigmoidByLUT(at::Half x) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half a = -18.0;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half b = 10.0;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half delta = (b - a) / (at::Half)nBins;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half one_over_delta = 1 / delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half a_one_over_delta = a * one_over_delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half bin_calc = std::fma(x, one_over_delta, -a_one_over_delta);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  // Use MAC bin_x = a + delta * at::Half(bin);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half bin_x = std::fma(delta, at::Half(bin), a);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half p = at::Half(x - bin_x) * one_over_delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half res1 = sig_lut[bin + 1] * p;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half res2 = std::fma(-p, sig_lut[bin], sig_lut[bin]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  return at::Half(res1 + res2);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.02831274f), at::Half(0.02928850f), at::Half(0.03026419f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.03123983f), at::Half(0.03319093f), at::Half(0.03514177f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.03709235f), at::Half(0.03904264f), at::Half(0.04099264f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.04294232f), at::Half(0.04489168f), at::Half(0.04684070f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.04878936f), at::Half(0.05073764f), at::Half(0.05268555f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.05463305f), at::Half(0.05658013f), at::Half(0.05852679f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.06047300f), at::Half(0.06241875f), at::Half(0.06630881f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.07019686f), at::Half(0.07408277f), at::Half(0.07796644f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.08184774f), at::Half(0.08572657f), at::Half(0.08960279f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.09347630f), at::Half(0.09734699f), at::Half(0.10121473f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.10507942f), at::Half(0.10894093f), at::Half(0.11279916f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.11665399f), at::Half(0.12050531f), at::Half(0.12435300f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.13203707f), at::Half(0.13970530f), at::Half(0.14735681f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.15499073f), at::Half(0.16260618f), at::Half(0.17020231f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.17777826f), at::Half(0.18533320f), at::Half(0.19286629f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.20037672f), at::Half(0.20786367f), at::Half(0.21532634f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.22276395f), at::Half(0.23017571f), at::Half(0.23756087f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.24491866f), at::Half(0.25954921f), at::Half(0.27406159f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.28845021f), at::Half(0.30270973f), at::Half(0.31683500f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.33082112f), at::Half(0.34466340f), at::Half(0.35835740f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.37189891f), at::Half(0.38528397f), at::Half(0.39850884f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.41157006f), at::Half(0.42446437f), at::Half(0.43718879f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.44974055f), at::Half(0.46211716f), at::Half(0.48633602f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.50982997f), at::Half(0.53258729f), at::Half(0.55459972f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.57586239f), at::Half(0.59637356f), at::Half(0.61613443f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.63514895f), at::Half(0.65342359f), at::Half(0.67096707f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.68779021f), at::Half(0.70390560f), at::Half(0.71932750f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.73407152f), at::Half(0.74815447f), at::Half(0.76159416f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.78661881f), at::Half(0.80930107f), at::Half(0.82980191f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.84828364f), at::Half(0.86490662f), at::Half(0.87982670f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.89319334f), at::Half(0.90514825f), at::Half(0.91582454f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.92534623f), at::Half(0.93382804f), at::Half(0.94137554f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.94808529f), at::Half(0.95404526f), at::Half(0.95933529f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.96402758f), at::Half(0.97187275f), at::Half(0.97802611f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.98284503f), at::Half(0.98661430f), at::Half(0.98955975f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99185972f), at::Half(0.99365463f), at::Half(0.99505475f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99614653f), at::Half(0.99699764f), at::Half(0.99766098f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99817790f), at::Half(0.99858066f), at::Half(0.99889444f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99913889f), at::Half(0.99932930f), at::Half(0.99959315f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99975321f), at::Half(0.99985031f), at::Half(0.99990920f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99994493f), at::Half(0.99996660f), at::Half(0.99997974f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99998771f), at::Half(0.99999255f), at::Half(0.99999548f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.99999726f), at::Half(0.99999834f)};
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00001525f), at::Half(0.00001525f), at::Half(0.00001524f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00003049f), at::Half(0.00003048f), at::Half(0.00003048f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00003047f), at::Half(0.00003047f), at::Half(0.00003046f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00003046f), at::Half(0.00003045f), at::Half(0.00003045f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00003044f), at::Half(0.00003044f), at::Half(0.00003043f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00003042f), at::Half(0.00003042f), at::Half(0.00003041f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00003040f), at::Half(0.00006078f), at::Half(0.00006075f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00006072f), at::Half(0.00006068f), at::Half(0.00006065f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00006061f), at::Half(0.00006057f), at::Half(0.00006052f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00006048f), at::Half(0.00006043f), at::Half(0.00006039f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00006034f), at::Half(0.00006028f), at::Half(0.00006023f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00006018f), at::Half(0.00006012f), at::Half(0.00012006f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00011982f), at::Half(0.00011955f), at::Half(0.00011928f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00011899f), at::Half(0.00011869f), at::Half(0.00011837f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00011805f), at::Half(0.00011770f), at::Half(0.00011735f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00011698f), at::Half(0.00011660f), at::Half(0.00011621f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00011581f), at::Half(0.00011539f), at::Half(0.00011497f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00022860f), at::Half(0.00022676f), at::Half(0.00022482f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00022281f), at::Half(0.00022071f), at::Half(0.00021853f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00021629f), at::Half(0.00021397f), at::Half(0.00021159f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00020914f), at::Half(0.00020664f), at::Half(0.00020408f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00020147f), at::Half(0.00019882f), at::Half(0.00019612f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00019338f), at::Half(0.00037842f), at::Half(0.00036709f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00035558f), at::Half(0.00034394f), at::Half(0.00033223f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00032049f), at::Half(0.00030876f), at::Half(0.00029710f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00028554f), at::Half(0.00027412f), at::Half(0.00026286f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00025180f), at::Half(0.00024097f), at::Half(0.00023038f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00022005f), at::Half(0.00021000f), at::Half(0.00039101f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00035441f), at::Half(0.00032033f), at::Half(0.00028878f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00025973f), at::Half(0.00023313f), at::Half(0.00020885f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00018680f), at::Half(0.00016682f), at::Half(0.00014878f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00013253f), at::Half(0.00011793f), at::Half(0.00010484f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00009312f), at::Half(0.00008266f), at::Half(0.00007332f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00012258f), at::Half(0.00009615f), at::Half(0.00007530f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00005889f), at::Half(0.00004602f), at::Half(0.00003594f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00002805f), at::Half(0.00002188f), at::Half(0.00001706f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00001330f), at::Half(0.00001036f), at::Half(0.00000808f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00000629f), at::Half(0.00000490f), at::Half(0.00000382f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00000298f), at::Half(0.00000000f), at::Half(0.00000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00000000f), at::Half(0.00000000f), at::Half(0.00000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00000000f), at::Half(0.00000000f), at::Half(0.00000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00000000f), at::Half(0.00000000f), at::Half(0.00000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    at::Half(0.00000000f), at::Half(0.00000000f)};
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:at::Half CalcTanhByLUT(at::Half input) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half err_f16;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half output = at::Half(0.0f);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:        at::Half(tanh_error_lut[(InputInU16 - TANH_LINEAR_MAX_VALUE) / 64]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    output = at::Half(tanh_lut[(InputInU16 - TANH_LINEAR_MAX_VALUE) / 64]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    output = at::Half(std::fma(err_f16, index, output));
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  output = (*((at::Half*)&outputInU16_temp));
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:at::Half CalcTanhByPolynomial(at::Half input) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  static const at::Half aCoefficient[64] = {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.423340f), at::Half(-0.352783f), at::Half(-0.411377f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.284424f), at::Half(-0.335938f), at::Half(-0.333740f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.333252f), at::Half(-0.332275f), at::Half(-0.333252f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.333252f), at::Half(-0.333252f), at::Half(-0.333252f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.333252f), at::Half(-0.333252f), at::Half(-0.333252f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.333252f), at::Half(-0.333008f), at::Half(-0.333008f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.332764f), at::Half(-0.332275f), at::Half(-0.331055f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.329346f), at::Half(-0.325195f), at::Half(-0.317383f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.301758f), at::Half(-0.273438f), at::Half(-0.219360f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.136108f), at::Half(-0.018677f), at::Half(0.080872f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.107056f),  at::Half(0.063110f),  at::Half(0.017731f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.002533f),  at::Half(0.000147f),  at::Half(0.000003f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f)};
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  static const at::Half bCoefficient[64] = {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000004f),  at::Half(0.000002f),  at::Half(0.000017f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000016f), at::Half(0.000001f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(-0.000001f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000001f), at::Half(-0.000002f), at::Half(-0.000007f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000020f), at::Half(-0.000054f), at::Half(-0.000158f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000433f), at::Half(-0.001253f), at::Half(-0.003410f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.009712f), at::Half(-0.025681f), at::Half(-0.068665f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.162354f), at::Half(-0.346680f), at::Half(-0.566406f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.640137f), at::Half(-0.439941f), at::Half(-0.161255f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.030548f), at::Half(-0.002459f), at::Half(-0.000061f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f)};
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  static const at::Half cCoefficient[64] = {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f), at::Half(1.000000f), at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000977f), at::Half(1.003906f), at::Half(1.014648f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.050781f), at::Half(1.147461f), at::Half(1.309570f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.378906f), at::Half(1.073242f), at::Half(0.500488f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.124329f), at::Half(0.013718f), at::Half(0.000464f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000004f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f), at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f), at::Half(0.000000f)};
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  static const at::Half dCoefficient[64] = {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(0.000000f),  at::Half(0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.000000f),  at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000000f), at::Half(-0.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000000f), at::Half(-0.000001f), at::Half(-0.000008f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.000045f), at::Half(-0.000237f), at::Half(-0.001252f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.005722f), at::Half(-0.022766f), at::Half(-0.062866f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(-0.084229f), at::Half(0.071167f),  at::Half(0.466064f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.828125f),  at::Half(0.974121f),  at::Half(0.998535f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(0.999512f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f),  at::Half(1.000000f),  at::Half(1.000000f),
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      at::Half(1.000000f)};
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half absInput = (input < 0) ? (input * at::Half(-1)) : input;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half a = aCoefficient[index];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half b = bCoefficient[index];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half c = cCoefficient[index];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half d = dCoefficient[index];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half tanhResult = d + c * absInput;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:at::Half CalcSwishByLUT(at::Half x) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  const at::Half a = (at::Half)(-20.5);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  const at::Half b = (at::Half)(8.0);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half delta = (b - a) / (at::Half)nBins;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half one_over_delta = at::Half(1) / delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half a_one_over_delta = a * one_over_delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half bin_calc = f_a_one_over_delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  // Use MAC bin_x = a + delta * at::Half(bin);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  float f_bin = at::Half(bin);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half bin_x = at::Half(f_a);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half p = at::Half(x - bin_x) * one_over_delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half res1 = swishLutKnot[bin + 1] * p;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  float lutVal = at::Half(swishLutKnot[bin]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half res2 = lutVal;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  return at::Half(res1 + res2);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:at::Half CalcSwishByLUTCubic(at::Half x) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half x_min = (at::Half)SWISH_KNOT_RANGE_MIN;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half x_max = (at::Half)SWISH_KNOT_RANGE_MAX;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half delta = (at::Half)SWISH_KNOT_LUT_DELTA;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half bias = SWISH_KNOT_LUT_BIAS;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half one_over_delta = at::Half(1) / delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half x_over_delta = x * one_over_delta;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half x_over_delta_int = std::round(x_over_delta);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half p = x_over_delta - x_over_delta_int;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half y_left = swishLutKnotCub[k_bin - 1];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half y_mid = swishLutKnotCub[k_bin];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half y_right = swishLutKnotCub[k_bin + 1];
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half a = y_mid + y_mid;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half c = (y_right + y_left);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half b = y_right - y_left;
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  at::Half result = std::fma(p, c, b);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  if (x == (at::Half)0.0f) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:at::Half CalcLogit(at::Half input, float eps) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  float x = at::Half(input);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  if (at::Half(input) < at::Half(eps)) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    x = at::Half(eps);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:  if (at::Half(input) > at::Half(1 - eps)) {
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    x = at::Half(1 - eps);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:    return at::Half(NAN);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      return at::Half(lower_bound);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      return at::Half(upper_bound);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      return at::Half(log((x / (1 - x))));
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      Y[i] = CalcSigmoidByLUT((at::Half)X[i]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      Y[i] = CalcTanhByLUT((at::Half)X[i]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      Y[i] = CalcSwishByLUT((at::Half)X[i]);
caffe2/contrib/fakelowp/unary_fp16_fake_op.cc:      Y_data[i] = CalcLogit((at::Half)X_data[i], eps_);
caffe2/contrib/fakelowp/unary_fp16_fake_op.h:at::Half CalcSigmoidByLUT(at::Half x);
caffe2/contrib/fakelowp/unary_fp16_fake_op.h:at::Half CalcSwishByLUT(at::Half x);
caffe2/contrib/fakelowp/unary_fp16_fake_op.h:at::Half CalcSwishByLUTCubic(at::Half x);
caffe2/contrib/fakelowp/unary_fp16_fake_op.h:at::Half CalcTanhByLUT(at::Half input);
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:    SparseLengthsReductionFakeFp16Op<TensorTypes<float, at::Half>, 0, 0>;
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:    SparseLengthsReductionFakeFp16Op<TensorTypes<float, at::Half>, 1, 0>;
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:    SparseLengthsReductionFakeFp16Op<TensorTypes<float, at::Half>, 0, 1>;
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:    SparseLengthsReductionFakeFp16Op<TensorTypes<float, at::Half>, 0, 0, 0, 1>;
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:    SparseLengthsReductionFakeFp16Op<TensorTypes<float, at::Half>, 1, 0, 0, 1>;
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:    SparseLengthsReductionFakeFp16Op<TensorTypes<float, at::Half>, 0, 1, 0, 1>;
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:        TensorTypes<float, at::Half>,
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:        TensorTypes<float, at::Half>,
caffe2/contrib/fakelowp/lengths_reducer_ops.cc:        TensorTypes<float, at::Half>,
caffe2/contrib/aten/aten_op_template.h:using at::Half; // for AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, ...)
caffe2/contrib/aten/aten_op_template.h:      // AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, DEFINE_CASE)
caffe2/contrib/aten/aten_op_template.h:      AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16, Float8, DEFINE_CASE)
caffe2/contrib/aten/aten_op_template.h:      // AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, DEFINE_CASE)
caffe2/contrib/aten/aten_op_template.h:      AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16, Float8, DEFINE_CASE)
caffe2/contrib/aten/aten_op.cc:void Set<at::Half, CPUContext>(
caffe2/contrib/aten/aten_op.cc:    const at::Half h,
caffe2/contrib/aten/aten_op.cc:    at::Half* v,
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:    } else if (AllInputsAre<at::Half>(this)) {
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:      nccl::NCCL<at::Half>::AllReduce(getNCCLElements(this, context_));
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:    } else if (AllInputsAre<at::Half>(this)) {
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:      nccl::NCCL<at::Half>::Broadcast(getNCCLElements(this, context_));
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:    } else if (AllInputsAre<at::Half>(this)) {
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:      nccl::NCCL<at::Half>::Reduce(ex);
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:    } else if (AllInputsAre<at::Half>(this)) {
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:      nccl::NCCL<at::Half>::AllGather(getNCCLElements(this, context_));
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:    } else if (AllInputsAre<at::Half>(this)) {
caffe2/contrib/nccl/cuda_nccl_op_gpu.cc:      nccl::NCCL<at::Half>::ReduceScatter(getNCCLElements(this, context_));
caffe2/contrib/nccl/cuda_nccl_gpu.cc:class ncclTypeWrapper<at::Half> {
caffe2/contrib/nccl/cuda_nccl_gpu.cc:  static const ncclDataType_t type = ncclHalf;
caffe2/contrib/nccl/cuda_nccl_gpu.cc:template class NCCL<at::Half>;
caffe2/contrib/gloo/broadcast_ops_gpu.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/allreduce_ops_gpu.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/allreduce_ops_gpu.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/allreduce_ops_gpu.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/allreduce_ops_gpu.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/reduce_scatter_ops.cc:  } else if (init_.template IsType<::at::Half>()) {
caffe2/contrib/gloo/broadcast_ops.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/allgather_ops.cc:  } else if (init_.template IsType<at::Half>()) {
caffe2/contrib/gloo/allreduce_ops.cc:  } else if (init_.template IsType<::at::Half>()) {
caffe2/contrib/gloo/allreduce_ops.cc:  } else if (init_.template IsType<::at::Half>()) {
caffe2/contrib/gloo/allreduce_ops.cc:  } else if (init_.template IsType<::at::Half>()) {
caffe2/contrib/gloo/allreduce_ops.cc:  } else if (init_.template IsType<::at::Half>()) {
caffe2/perfkernels/embedding_lookup_idx.cc:#include <c10/util/Half.h>
caffe2/perfkernels/embedding_lookup_idx.cc:EMBEDDING_IDX_SPECIALIZATION(int32_t, half, at::Half, float, false);
caffe2/perfkernels/embedding_lookup_idx.cc:EMBEDDING_IDX_SPECIALIZATION(int64_t, half, at::Half, float, false);
caffe2/perfkernels/embedding_lookup_idx.cc:EMBEDDING_IDX_SPECIALIZATION(int32_t, half, at::Half, float, true);
caffe2/perfkernels/embedding_lookup_idx.cc:EMBEDDING_IDX_SPECIALIZATION(int64_t, half, at::Half, float, true);
caffe2/perfkernels/embedding_lookup_avx2.cc:#include <c10/util/Half.h>
caffe2/perfkernels/embedding_lookup_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_avx2.cc:    const at::Half* input,
caffe2/perfkernels/adagrad_avx2.cc:// Compute adagrad sparse, assumes embedding and momentum are at::Half
caffe2/perfkernels/adagrad_avx2.cc:    const at::Half* w,
caffe2/perfkernels/adagrad_avx2.cc:    const at::Half* w_n, // prefetch ptr
caffe2/perfkernels/adagrad_avx2.cc:    const at::Half* h,
caffe2/perfkernels/adagrad_avx2.cc:    const at::Half* h_n, // prefetch ptr
caffe2/perfkernels/adagrad_avx2.cc:    at::Half* nw,
caffe2/perfkernels/adagrad_avx2.cc:    at::Half* nw_n, // prefetch ptr
caffe2/perfkernels/adagrad_avx2.cc:    at::Half* nh,
caffe2/perfkernels/adagrad_avx2.cc:    at::Half* nh_n, // prefetch ptr
caffe2/perfkernels/embedding_lookup.cc:EMBEDDING_SPECIALIZATION(int32_t, half, at::Half, float, false);
caffe2/perfkernels/embedding_lookup.cc:EMBEDDING_SPECIALIZATION(int64_t, half, at::Half, float, false);
caffe2/perfkernels/embedding_lookup.cc:EMBEDDING_SPECIALIZATION(int32_t, half, at::Half, float, true);
caffe2/perfkernels/embedding_lookup.cc:EMBEDDING_SPECIALIZATION(int64_t, half, at::Half, float, true);
caffe2/perfkernels/adagrad.cc:    const at::Half* w,
caffe2/perfkernels/adagrad.cc:    const at::Half* /* w_n */, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    const at::Half* h,
caffe2/perfkernels/adagrad.cc:    const at::Half* /* h_n */, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    at::Half* nw,
caffe2/perfkernels/adagrad.cc:    at::Half* /* nw_n */, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    at::Half* nh,
caffe2/perfkernels/adagrad.cc:    at::Half* /* nh_n */, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    const at::Half* w,
caffe2/perfkernels/adagrad.cc:    const at::Half* w_n, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    const at::Half* h,
caffe2/perfkernels/adagrad.cc:    const at::Half* h_n, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    at::Half* nw,
caffe2/perfkernels/adagrad.cc:    at::Half* nw_n, // prefetch ptr
caffe2/perfkernels/adagrad.cc:    at::Half* nh,
caffe2/perfkernels/adagrad.cc:    at::Half* nh_n, // prefetch ptr
caffe2/perfkernels/cvtsh_ss_bugfix.h:#include <c10/util/Half.h>
caffe2/perfkernels/cvtsh_ss_bugfix.h:  *reinterpret_cast<at::Half*>(&ret) = a;
caffe2/perfkernels/cvtsh_ss_bugfix.h:#include <c10/util/Half.h>
caffe2/perfkernels/cvtsh_ss_bugfix.h:  *reinterpret_cast<at::Half*>(&ret) = x;
caffe2/perfkernels/adagrad.h:#include <c10/util/Half.h>
caffe2/perfkernels/adagrad.h:    const at::Half* w,
caffe2/perfkernels/adagrad.h:    const at::Half* w_n, // prefetch ptr
caffe2/perfkernels/adagrad.h:    const at::Half* h,
caffe2/perfkernels/adagrad.h:    const at::Half* h_n, // prefetch ptr
caffe2/perfkernels/adagrad.h:    at::Half* nw,
caffe2/perfkernels/adagrad.h:    at::Half* nw_n, // prefetch ptr
caffe2/perfkernels/adagrad.h:    at::Half* nh,
caffe2/perfkernels/adagrad.h:    at::Half* nh_n, // prefetch ptr
caffe2/perfkernels/typed_axpy_avx2.cc:#include <c10/util/Half.h>
caffe2/perfkernels/typed_axpy_avx2.cc:void TypedAxpyHalffloat__avx2_fma(
caffe2/perfkernels/typed_axpy_avx2.cc:    const at::Half* x,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:#include <c10/util/Half.h>
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/typed_axpy_avx.cc:#include <c10/util/Half.h>
caffe2/perfkernels/typed_axpy_avx.cc:void TypedAxpyHalffloat__avx_f16c(
caffe2/perfkernels/typed_axpy_avx.cc:    const at::Half* x,
caffe2/perfkernels/hp_emblookup_codegen.py:sizeof = {"float": 4, "at::Half": 2, "at::BFloat16": 2, "uint8_t": 1}
caffe2/perfkernels/hp_emblookup_codegen.py:        elif InType == "at::Half":
caffe2/perfkernels/hp_emblookup_codegen.py:        elif InType == "at::Half":
caffe2/perfkernels/hp_emblookup_codegen.py:    if InType == "at::Half":
caffe2/perfkernels/hp_emblookup_codegen.py:        code.append("    alignas(64) at::Half vtmp1[8] = {0};")
caffe2/perfkernels/hp_emblookup_codegen.py:    elif InType == "at::Half":
caffe2/perfkernels/hp_emblookup_codegen.py:    ["int32_t", "int", "half", "at::Half", "float", "float"],
caffe2/perfkernels/hp_emblookup_codegen.py:    ["int64_t", "int64_t", "half", "at::Half", "float", "float"],
caffe2/perfkernels/hp_emblookup_codegen.py:code.append("#include <c10/util/Half.h>")
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:#include <c10/util/Half.h>
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_idx_avx2.cc:    const at::Half* input,
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:#include <c10/util/Half.h>
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:  fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:  fbgemm::Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<float>(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:void FloatToFusedNBitRowwiseQuantizedSBHalf__base(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:      2 * sizeof(at::Half);
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:    at::Half* output_row_scale_bias = reinterpret_cast<at::Half*>(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:    minimum_element = static_cast<at::Half>(minimum_element);
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:    at::Half scale = range == 0 ? 1.0f : range / ((1 << bit_rate) - 1);
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:void FusedNBitRowwiseQuantizedSBHalfToFloat__base(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:      (input_columns - 2 * sizeof(at::Half)) * num_elem_per_byte;
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:    const at::Half* input_row_scale_bias = reinterpret_cast<const at::Half*>(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:void FloatToFusedNBitRowwiseQuantizedSBHalf(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:  fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float>(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:  FloatToFusedNBitRowwiseQuantizedSBHalf__base(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:void FusedNBitRowwiseQuantizedSBHalfToFloat(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:  fbgemm::FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<float>(
caffe2/perfkernels/fused_nbit_rowwise_conversion.cc:  FusedNBitRowwiseQuantizedSBHalfToFloat__base(
caffe2/perfkernels/typed_axpy.cc:void TypedAxpyHalffloat__base(
caffe2/perfkernels/typed_axpy.cc:    const at::Half* x,
caffe2/perfkernels/typed_axpy.cc:decltype(TypedAxpyHalffloat__base) TypedAxpyHalffloat__avx2_fma;
caffe2/perfkernels/typed_axpy.cc:decltype(TypedAxpyHalffloat__base) TypedAxpyHalffloat__avx_f16c;
caffe2/perfkernels/typed_axpy.cc:void TypedAxpy<at::Half, float>(
caffe2/perfkernels/typed_axpy.cc:    const at::Half* x,
caffe2/perfkernels/typed_axpy.cc:  AVX2_FMA_DO(TypedAxpyHalffloat, N, a, x, y);
caffe2/perfkernels/typed_axpy.cc:  AVX_F16C_DO(TypedAxpyHalffloat, N, a, x, y);
caffe2/perfkernels/typed_axpy.cc:  BASE_DO(TypedAxpyHalffloat, N, a, x, y);
caffe2/perfkernels/fused_nbit_rowwise_conversion.h:void FloatToFusedNBitRowwiseQuantizedSBHalf(
caffe2/perfkernels/fused_nbit_rowwise_conversion.h:void FusedNBitRowwiseQuantizedSBHalfToFloat(
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:#include <c10/util/Half.h>
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    alignas(64) at::Half vtmp1[8] = {0};
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip = &input[idx * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:        const at::Half* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    const at::Half* input,
caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc:    const at::Half* input,
caffe2/onnx/offline_tensor.cc:  } else if (shape_tensor.template IsType<c10::Half>()) {


####################################################################################################

cmake/ProtoBufPatch.cmake:#     function "c10::operator+(int, c10::Half)"
cmake/ProtoBufPatch.cmake:#     function "c10::operator+(c10::Half, int)"
cmake/Dependencies.cmake:  message(STATUS "Found CUDA with FP16 support, compiling with torch.cuda.HalfTensor")

####################################################################################################

docs/source/distributions.rst::hidden:`HalfCauchy`
docs/source/distributions.rst:.. autoclass:: HalfCauchy
docs/source/distributions.rst::hidden:`HalfNormal`
docs/source/distributions.rst:.. autoclass:: HalfNormal
docs/source/tensor_attributes.rst:16-bit floating point [1]_ ``torch.float16`` or ``torch.half``           ``torch.*.HalfTensor``
docs/source/storage.rst:.. autoclass:: torch.HalfStorage
docs/source/tensors.rst:16-bit floating point [1]_              ``torch.float16`` or ``torch.half``         :class:`torch.HalfTensor`     :class:`torch.cuda.HalfTensor`
docs/source/notes/numerical_accuracy.rst:Half-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., ``inf`` values when the final result should be be representable in half-precision).
docs/source/conf.py:    "HalfStorage",
docs/source/conf.py:    "HalfTensor",
docs/cpp/source/Doxyfile:                         ../../../c10/util/Half.h \

####################################################################################################

test/distributions/test_distributions.py:                                 HalfCauchy, HalfNormal, Independent, Kumaraswamy,
test/distributions/test_distributions.py:    Example(HalfCauchy, [
test/distributions/test_distributions.py:    Example(HalfNormal, [
test/distributions/test_distributions.py:    Example(HalfCauchy, [
test/distributions/test_distributions.py:    Example(HalfNormal, [
test/distributions/test_distributions.py:        self.assertTrue(torch.isinf(HalfCauchy(scale_1d).mean).all())
test/distributions/test_distributions.py:        self.assertEqual(HalfCauchy(scale_1d).variance, inf)
test/distributions/test_distributions.py:        self.assertEqual(HalfCauchy(scale).sample().size(), (5, 5))
test/distributions/test_distributions.py:        self.assertEqual(HalfCauchy(scale).sample((7,)).size(), (7, 5, 5))
test/distributions/test_distributions.py:        self.assertEqual(HalfCauchy(scale_1d).sample().size(), (1,))
test/distributions/test_distributions.py:        self.assertEqual(HalfCauchy(scale_1d).sample((1,)).size(), (1, 1))
test/distributions/test_distributions.py:        self.assertEqual(HalfCauchy(1.0).sample((1,)).size(), (1,))
test/distributions/test_distributions.py:        self._gradcheck_log_prob(HalfCauchy, (scale,))
test/distributions/test_distributions.py:        self._gradcheck_log_prob(HalfCauchy, (1.0,))
test/distributions/test_distributions.py:        c = HalfCauchy(scale).rsample()
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(std).sample().size(), (5, 5))
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(std).sample((7,)).size(), (7, 5, 5))
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(std_1d).sample((1,)).size(), (1, 1))
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(std_1d).sample().size(), (1,))
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(.6).sample((1,)).size(), (1,))
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(50.0).sample((1,)).size(), (1,))
test/distributions/test_distributions.py:        self.assertEqual(HalfNormal(std_delta).sample(sample_shape=(1, 2)),
test/distributions/test_distributions.py:        self._gradcheck_log_prob(HalfNormal, (std,))
test/distributions/test_distributions.py:        self._gradcheck_log_prob(HalfNormal, (1.0,))
test/distributions/test_distributions.py:        dist = HalfNormal(torch.ones(2, 1, 4))
test/distributions/test_distributions.py:        self._check_log_prob(HalfNormal(std), ref_log_prob)
test/distributions/test_distributions.py:            self._check_sampler_sampler(HalfNormal(std),
test/distributions/test_distributions.py:                                        'HalfNormal(scale={})'.format(std))
test/distributions/test_distributions.py:        halfcauchy = HalfCauchy(1)
test/distributions/test_distributions.py:        halfcauchy = HalfCauchy(torch.tensor([1., 1.]))
test/distributions/test_distributions.py:        halfnormal = pairwise(HalfNormal, [1.0, 2.0, 1.0, 2.0])
test/distributions/test_distributions.py:                HalfCauchy(positive_var),
test/distributions/test_distributions.py:                HalfNormal(positive_var2),
test/distributions/test_distributions.py:            if isinstance(pytorch_dist, (Cauchy, HalfCauchy)):
test/distributions/test_distributions.py:                # Cauchy, HalfCauchy distributions' mean is nan, skipping check
test/distributions/test_distributions.py:            if isinstance(pytorch_dist, (Cauchy, HalfCauchy, VonMises)):
test/distributions/test_distributions.py:                # Cauchy, HalfCauchy distributions' standard deviation is nan, skipping check
test/distributions/test_distributions.py:                HalfCauchy,  # aten::cauchy(Double(2, 1), float, float, Generator)
test/distributions/test_distributions.py:                HalfCauchy,  # aten::cauchy(Double(2, 1), float, float, Generator)
test/distributions/test_distributions.py:            if Dist in [Cauchy, HalfCauchy]:
test/test_nvfuser_frontend.py:            t0 = fd.define_tensor(3, DataType.Half)
test/test_nvfuser_frontend.py:            t1 = fd.define_tensor(3, DataType.Half)
test/test_nvfuser_frontend.py:            t5 = fd.ops.cast(t4, DataType.Half)
test/test_nvfuser_frontend.py:            t0h = fd.ops.cast(t0, DataType.Half)
test/test_nvfuser_frontend.py:            t1h = fd.ops.cast(t1, DataType.Half)
test/test_nvfuser_frontend.py:            t4 = fd.ops.cast(t3, DataType.Half)
test/test_nvfuser_frontend.py:            t0 = fd.define_tensor(2, DataType.Half)
test/test_decomp.py:    # exp_vml_cpu not implemented for Half
test/test_decomp.py:    # sin_vml_cpu not implemented for Half
test/test_nestedtensor.py:    # cannot test torch.float16 because: RuntimeError: "bernoulli_scalar_cpu_" not implemented for 'Half'
test/test_nestedtensor.py:    # cannot test torch.float16 because: RuntimeError: "softmax_kernel_impl" not implemented for 'Half'
test/test_nestedtensor.py:    # cannot test torch.float16 because: RuntimeError: "addmm_impl_cpu_" not implemented for 'Half'
test/test_nestedtensor.py:    # cannot test torch.float16 because: RuntimeError: "addmm_impl_cpu_" not implemented for 'Half'
test/test_nestedtensor.py:    # cannot test torch.float16 because: RuntimeError: "bmm" not implemented for 'Half'
test/test_nestedtensor.py:    # cannot test torch.float16 because: RuntimeError: "bmm" not implemented for 'Half'
test/benchmark_utils/callgrind_artifacts.json:        "25000 build/../c10/util/typeid.cpp:caffe2::detail::TypeMetaData const* caffe2::TypeMeta::_typeMetaDataInstance<c10::Half>() [/data/users/test_user/repos/pytorch/torch/lib/libc10.so]",
test/benchmark_utils/callgrind_artifacts.json:        "25000 build/../c10/util/typeid.cpp:caffe2::detail::TypeMetaData const* caffe2::TypeMeta::_typeMetaDataInstance<c10::Half>() [/data/users/test_user/repos/pytorch/torch/lib/libc10.so]",
test/functorch/test_aotdispatch.py:    xfail('chalf'),  # RuntimeError: "sum_cpu" not implemented for 'ComplexHalf'
test/functorch/test_ops.py:        xfail('chalf', '', device_type='cpu'),  # RuntimeError: "sum_cpu" not implemented for 'ComplexHalf'
test/test_torch.py:        msg = 'ComplexHalf support is experimental'
test/test_torch.py:        self.assertEqual(torch.HalfTensor(10).to(device).is_signed(), True)
test/test_torch.py:        self.assertEqual(halfStorage.type(), 'torch.HalfStorage')
test/test_torch.py:            if t == torch.HalfTensor:
test/test_torch.py:                continue  # HalfTensor does not support fill
test/test_binary_ufuncs.py:                # Half Tensor with complex exponents leads to computation dtype
test/test_binary_ufuncs.py:                # of ComplexHalf for which this ops is not supported yet
test/test_binary_ufuncs.py:                with self.assertRaisesRegex(RuntimeError, "not implemented for 'ComplexHalf'"):
test/test_binary_ufuncs.py:            # Half Tensor with complex base leads to computation dtype
test/test_binary_ufuncs.py:            # of ComplexHalf for which this ops is not supported yet
test/test_binary_ufuncs.py:            # ComplexHalf
test/test_binary_ufuncs.py:                with self.assertRaisesRegex(RuntimeError, "not implemented for 'ComplexHalf'"):
test/test_binary_ufuncs.py:                            "result type (Half|Float|Double) "
test/test_jit.py:                       .check("Tensor = aten::tensor").check("Half(device=cpu) = prim::BailOut").run(g)
test/test_jit.py:                FileCheck().check("Half").check_same("aten::tensor").run(torch.jit.last_executed_optimized_graph())
test/test_sparse.py:        x = torch.sparse.HalfTensor(2, 3, 4)
test/test_sparse.py:        x = torch.cuda.sparse.HalfTensor(2, 3, 4)
test/test_sparse.py:        t = torch.sparse_coo_tensor(torch.tensor(([0], [2])), torch.HalfTensor(1, 0))
test/test_public_bindings.py:            "CudaHalfTensorBase",
test/test_reductions.py:        # mean_cuda is not implemented for ComplexHalf
test/test_reductions.py:        err_msg = "not implemented for 'ComplexHalf'"
test/dynamo/test_recompile_ux.py:            "tensor 'a' dtype mismatch. expected Float, actual Half",
test/dynamo/test_misc.py:        opt_fn(torch.float16, torch.HalfTensor)
test/distributed/fsdp/test_fsdp_mixed_precision.py:        # RuntimeError: Expected counts to have type Half but got Float
test/test_dataloader.py:            np.float16: torch.HalfTensor,
test/allowlist_for_publicAPI.json:    "HalfTensor",
test/allowlist_for_publicAPI.json:    "HalfStorage",
test/allowlist_for_publicAPI.json:    "HalfTensor",
test/allowlist_for_publicAPI.json:    "_cast_Half",
test/test_numpy_interop.py:            # TODO: change to tensor equality check once HalfTensor
test/test_numpy_interop.py:            torch.HalfTensor,
test/test_type_promotion.py:            # "_th_normal_ not supported on CPUType for Half" so simpler create and convert
test/test_type_promotion.py:            # "mul_cpu" / "div_cpu" not implemented for 'Half'
test/test_sparse_csr.py:                # ComplexHalf is experimental
test/jit/test_misc.py:                torch.HalfTensor([1]),
test/nn/test_packed_sequence.py:        # We leave out `'torch.HalfTensor': (torch.HalfTensor, 'half'),`
test/nn/test_packed_sequence.py:        # > AttributeError: 'torch.HalfTensor' object has no attribute 'fill_'
test/quantization/core/test_quantized_op.py:        conversion_op = "FloatToFused8BitRowwiseQuantized" if data_type == torch.float32 else "HalfFloatToFused8BitRowwiseQuantized"
test/quantization/core/test_quantized_op.py:            conversion_op = "FloatToFused4BitRowwiseQuantized" if data_type == torch.float32 else "HalfToFused4BitRowwiseQuantized"
test/quantization/core/test_quantized_op.py:            conversion_op = "FloatToFused2BitRowwiseQuantized" if data_type == torch.float32 else "HalfToFused2BitRowwiseQuantized"
test/test_numba_integration.py:            torch.HalfTensor,
test/test_numba_integration.py:            if tp not in (torch.HalfTensor,):
test/test_tensor_creation_ops.py:            error = r"Expected both inputs to be Half, Float or Double tensors but " \
test/test_tensor_creation_ops.py:            tensor = torch.HalfTensor(array).to(device)
test/test_tensor_creation_ops.py:        complexHalfTensor = torch.zeros(2, 2, device=device, dtype=torch.complex32)
test/test_tensor_creation_ops.py:        self.assertEqual(complexHalfTensor, expected)
test/test_tensor_creation_ops.py:            # Ensure both integer and floating-point numbers are tested. Half follows an execution path that is
Binary file test/onnx/assets/grace_hopper_517x606.jpg matches
test/onnx/test_pytorch_onnx_onnxruntime.py:        if op != torch.prod:  # torch.prod not implemented for Half
test/test_nn.py:        self.assertIsInstance(l.weight.data, torch.HalfTensor)
test/test_nn.py:        self.assertIsInstance(l.bias.data, torch.HalfTensor)
test/test_nn.py:            self.assertIsInstance(l.weight.data, torch.cuda.HalfTensor)
test/test_nn.py:            self.assertIsInstance(l.bias.data, torch.cuda.HalfTensor)
test/test_nn.py:        self.assertIsInstance(l.weight.data, torch.HalfTensor)
test/test_nn.py:        self.assertIsInstance(l.bias.data, torch.HalfTensor)
test/test_mps.py:        helper([9.0, 3.0, 5.0, 4.0], torch.HalfTensor)
test/cpp/tensorexpr/padded_buffer.h:struct DefaultPaddedValue<at::Half> {
test/cpp/tensorexpr/padded_buffer.h:  // at::Half ctor isn't constexpr, so just fill it with bits.
test/cpp/tensorexpr/test_cpp_codegen.cpp:TEST(CppPrinter, HalfImm) {
test/cpp/tensorexpr/test_cpp_codegen.cpp:  auto h = alloc<HalfImm>(10);
test/cpp/tensorexpr/test_cpp_codegen.cpp:TEST(CppPrinter, MaxHalf) {
test/cpp/tensorexpr/test_cpp_codegen.cpp:  auto max = alloc<Max>(alloc<HalfImm>(1), alloc<HalfImm>(2), false);
test/cpp/tensorexpr/test_expr.cpp:TEST(Expr, HalfTest) {
test/cpp/tensorexpr/test_expr.cpp:  VarHandle x("x", kHalf);
test/cpp/tensorexpr/test_expr.cpp:  ExprHandle body = ExprHandle((at::Half)2) +
test/cpp/tensorexpr/test_expr.cpp:      (x * ExprHandle((at::Half)3) + ExprHandle((at::Half)4));
test/cpp/tensorexpr/test_expr.cpp:  eval.bindVar(x, ExprHandle((at::Half)3));
test/cpp/tensorexpr/test_expr.cpp:  ASSERT_EQ(eval.value<at::Half>(), 2 + (3 * 3 + 4));
test/cpp/tensorexpr/test_simplify.cpp:  immediates.push_back(alloc<HalfImm>(1));
test/cpp/tensorexpr/test_cuda.cpp:#include <c10/util/Half.h>
test/cpp/tensorexpr/test_cuda.cpp:  testCudaTestVectorAdd01_impl<at::Half>();
test/cpp/tensorexpr/test_cuda.cpp:TEST(Cuda, HalfCast_CUDA) {
test/cpp/tensorexpr/test_cuda.cpp:  auto half = ToDtype<at::Half>();
test/cpp/tensorexpr/test_cuda.cpp:  std::vector<at::Half> aData(4, 2.0f);
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* aDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:TEST(Cuda, HalfSupport_CUDA) {
test/cpp/tensorexpr/test_cuda.cpp:  auto half = ToDtype<at::Half>();
test/cpp/tensorexpr/test_cuda.cpp:  std::vector<at::Half> aData(4, 2.0f);
test/cpp/tensorexpr/test_cuda.cpp:  std::vector<at::Half> dData(4, 0.0f);
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* aDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* bDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* cDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* dDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:TEST(Cuda, HalfPropagation_CUDA) {
test/cpp/tensorexpr/test_cuda.cpp:  auto half = ToDtype<at::Half>();
test/cpp/tensorexpr/test_cuda.cpp:    return Max::make(a.load(i), ExprHandle(alloc<HalfImm>(0)), true);
test/cpp/tensorexpr/test_cuda.cpp:  std::vector<at::Half> aData(4, 2.0f);
test/cpp/tensorexpr/test_cuda.cpp:  std::vector<at::Half> reluData(4, 0.0f);
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* aDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* reluDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:TEST(Cuda, UnusedHalfArgument_CUDA) {
test/cpp/tensorexpr/test_cuda.cpp:  auto half = ToDtype<at::Half>();
test/cpp/tensorexpr/test_cuda.cpp:  std::vector<at::Half> bData(4, 2.0f);
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* aDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* bDev = nullptr;
test/cpp/tensorexpr/test_cuda.cpp:  at::Half* reluDev = nullptr;
test/cpp/tensorexpr/test_llvm.cpp:  _(at::Half, Half, 0.128f)
test/cpp/tensorexpr/test_llvm.cpp:TEST(LLVM, HalfToLongCastTest) {
test/cpp/tensorexpr/test_llvm.cpp:  auto a = HalfImm::make(2.0);
test/cpp/tensorexpr/test_llvm.cpp:  at::Half reff16 = 1337.0f;
test/cpp/tensorexpr/test_llvm.cpp:    at::Half k_;
test/cpp/tensorexpr/test_llvm.cpp:    at::Half* k = &k_;
test/cpp/tensorexpr/test_llvm.cpp:    auto a = HalfImm::make(k);
test/cpp/tensorexpr/test_llvm.cpp:  VarHandle y("y", kHalf);
test/cpp/tensorexpr/test_ir_printer.cpp:  VarHandle x("x", kHalf);
test/cpp/tensorexpr/test_type.cpp:    ASSERT_EQ(kHalf, ToDtype<at::Half>());
test/cpp/tensorexpr/test_type.cpp:    ExprHandle y = bitcast<at::Half>(x);
test/cpp/tensorexpr/test_type.cpp:    ASSERT_EQ(y.dtype(), kHalf);
test/cpp/tensorexpr/test_type.cpp:    VarHandle x("x", kHalf);
test/cpp/tensorexpr/test_type.cpp:    at::Half k_;
test/cpp/tensorexpr/test_type.cpp:    at::Half* k = &k_;
test/cpp/tensorexpr/test_type.cpp:    auto a = HalfImm::make(*k);
test/cpp/tensorexpr/test_type.cpp:    ASSERT_ANY_THROW(ExprHandle y = bitcast<at::Half>(x));
test/cpp/tensorexpr/test_type.cpp:    VarHandle x("x", kHalf);
test/cpp/tensorexpr/test_type.cpp:        HalfImm::make(2.f) + (x * HalfImm::make(3) + HalfImm::make(4) * y);
test/cpp/tensorexpr/test_type.cpp:    VarHandle x("x", kHalf);
test/cpp/tensorexpr/test_type.cpp:        HalfImm::make(2) + (x * HalfImm::make(3) + HalfImm::make(4) * y);
test/cpp/tensorexpr/test_type.cpp:    ASSERT_EQ(body.dtype(), kHalf);
test/cpp/api/functional.cpp:  test_isfinite<torch::kFloat16, c10::Half>(device);
test/cpp/api/functional.cpp:  test_isinf<torch::kFloat16, c10::Half>(device);
test/cpp/api/functional.cpp:  test_allclose<torch::kFloat16, c10::Half>(device);
test/cpp/lazy/test_lazy_ops.cpp:TEST_F(LazyOpsTest, TestAddHalf) {
test/cpp/lazy/test_lazy_ops.cpp:      {2, 2}, torch::TensorOptions(torch::kHalf).device(DefaultDevice()));
test/cpp/lazy/test_lazy_ops.cpp:      {2, 2}, torch::TensorOptions(torch::kHalf).device(DefaultDevice()));
test/cpp/lazy/test_lazy_ops.cpp:      {2, 2}, torch::TensorOptions(torch::kHalf).device(DefaultDevice()));


####################################################################################################

third_party/cutlass/tools/profiler/src/symm_operation_profiler.cu:  // Half matrix including the diagonal will have (X*(X+1))/2 elements
third_party/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu:    // Half matrix including the diagonal will have (N*(N+1))/2 elements
third_party/cutlass/tools/profiler/src/trmm_operation_profiler.cu:      // Half matrix including the diagonal will have (M*(M+1))/2 elements
third_party/cutlass/tools/profiler/src/trmm_operation_profiler.cu:      // Half matrix including the diagonal will have (N*(N+1))/2 elements
third_party/cutlass/tools/profiler/src/rank_k_operation_profiler.cu:    // Half matrix including the diagonal will have (N*(N+1))/2 elements
third_party/cutlass/CHANGELOG.md:  * Half2 usage if epilogue compute type is fp16.
third_party/cutlass/CHANGELOG.md:  * [Half-precision GELU_taylor activation functions](/include/cutlass/epilogue/thread/activation.h#L196)
third_party/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h:  static int const kHalfThreadsPerRow = (kThreadsPerRow >> 1);
third_party/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h:    for (int i = kHalfThreadsPerRow; i > 0; i >>= 1) {
third_party/cutlass/examples/11_planar_complex_array/planar_complex_array.cu:  // Half-precision input and output
third_party/flatbuffers/grpc/tests/message_builder_test.cpp:/// MessageBuilder DURING a table construction. Half of the table is constructed
third_party/flatbuffers/grpc/tests/message_builder_test.cpp:/// MessageBuilder DURING a table construction. Half of the table is constructed
third_party/onnx-tensorrt/main.cpp:      trt_builder->setHalf2Mode(true);
third_party/onnx-tensorrt/onnx_trt_backend.cpp:// - setHalf2Mode (bool)
third_party/FP16/bench/from-ieee-array.cc:	#include <third-party/THHalf.h>
third_party/FP16/bench/to-ieee-array.cc:	#include <third-party/THHalf.h>
third_party/FP16/bench/ieee-element.cc:	#include <third-party/THHalf.h>
third_party/FP16/third-party/half.hpp:	/// Half-precision floating point type.
third_party/FP16/third-party/half.hpp:		/// Half literal.
third_party/FP16/third-party/half.hpp:			/// \return Half-precision sum stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision difference stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision product stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision quotient stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision division remainder stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision division remainder stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision division remainder stored in single-precision
third_party/FP16/third-party/half.hpp:			/// \return Half-precision quiet NaN
third_party/FP16/third-party/THHalf.h: *   File: torch/lib/TH/THHalf.c
third_party/FP16/third-party/eigen-half.h: *   File: Eigen/src/Core/arch/CUDA/Half.h
third_party/sleef/src/arch/helpervecext.h:static INLINE vint vreinterpretFirstHalf_vi_vi2(vint2 vi2) { return (vint){ vi2[0], vi2[1] }; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint2 vreinterpretFirstHalf_vi2_vi(vint vi) { return (vint2){ vi[0], vi[1], 0, 0 }; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint vreinterpretFirstHalf_vi_vi2(vint2 vi2) { return (vint){ vi2[0], vi2[1], vi2[2], vi2[3] }; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint2 vreinterpretFirstHalf_vi2_vi(vint vi) { return (vint2){ vi[0], vi[1], vi[2], vi[3], 0, 0, 0, 0 }; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint vreinterpretFirstHalf_vi_vi2(vint2 vi2) { return (vint){ vi2[0], vi2[1], vi2[2], vi2[3], vi2[4], vi2[5], vi2[6], vi2[7] }; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint2 vreinterpretFirstHalf_vi2_vi(vint vi) { return (vint2){ vi[0], vi[1], vi[2], vi[3], vi[4], vi[5], vi[6], vi[7], 0, 0, 0, 0, 0, 0, 0, 0 }; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint vreinterpretFirstHalf_vi_vi2(vint2 vi2) {
third_party/sleef/src/arch/helpervecext.h:static INLINE vint2 vreinterpretFirstHalf_vi2_vi(vint vi) {
third_party/sleef/src/arch/helpervecext.h:static INLINE vint vand_vi_vo_vi(vopmask x, vint y) { return vreinterpretFirstHalf_vi_vi2((vint2)x) & y; }
third_party/sleef/src/arch/helpervecext.h:static INLINE vint vandnot_vi_vo_vi(vopmask x, vint y) { return y & ~vreinterpretFirstHalf_vi_vi2((vint2)x); }
third_party/sleef/src/arch/helpervecext.h:static INLINE vopmask veq_vo_vi_vi(vint x, vint y) { return (vopmask)vreinterpretFirstHalf_vi2_vi(x == y); }
third_party/sleef/src/arch/helpervecext.h:static INLINE vopmask vgt_vo_vi_vi(vint x, vint y) { return (vopmask)vreinterpretFirstHalf_vi2_vi(x > y);}
third_party/sleef/src/arch/helpervecext.h:  return vor_vi_vi_vi(vand_vi_vi_vi(vreinterpretFirstHalf_vi_vi2((vint2)m), x),
third_party/sleef/src/arch/helpervecext.h:		      vandnot_vi_vi_vi(vreinterpretFirstHalf_vi_vi2((vint2)m), y));
third_party/sleef/src/arch/helperpurec.h:static INLINE vint vreinterpretFirstHalf_vi_vi2(vint2 vi2) {
third_party/sleef/src/arch/helperpurec.h:static INLINE vint2 vreinterpretFirstHalf_vi2_vi(vint vi) {
third_party/sleef/src/arch/helperpurec.h:static INLINE vint vand_vi_vo_vi(vopmask x, vint y)    { return vand_vi_vi_vi(vreinterpretFirstHalf_vi_vi2(x), y); }
third_party/sleef/src/arch/helperpurec.h:static INLINE vint vandnot_vi_vo_vi(vopmask x, vint y) { return vandnot_vi_vi_vi(vreinterpretFirstHalf_vi_vi2(x), y); }
third_party/sleef/src/arch/helperpurec.h:  return vor_vi_vi_vi(vand_vi_vi_vi(vreinterpretFirstHalf_vi_vi2(cnv.vi2), x),
third_party/sleef/src/arch/helperpurec.h:		      vandnot_vi_vi_vi(vreinterpretFirstHalf_vi_vi2(cnv.vi2), y));
third_party/sleef/src/arch/helpers390x_128.h:static INLINE vint vreinterpretFirstHalf_vi_vi2(vint2 vi2) { return (vint){ vi2[0], vi2[1] }; }
third_party/sleef/src/arch/helpers390x_128.h:static INLINE vint2 vreinterpretFirstHalf_vi2_vi(vint vi) { return (vint2){ vi[0], vi[1], 0, 0 }; }
third_party/sleef/src/arch/helpers390x_128.h:static INLINE vint vand_vi_vo_vi(vopmask x, vint y) { return vreinterpretFirstHalf_vi_vi2((vint2)x) & y; }
third_party/sleef/src/arch/helpers390x_128.h:static INLINE vint vandnot_vi_vo_vi(vopmask x, vint y) { return vec_andc(y, vreinterpretFirstHalf_vi_vi2((vint2)x)); }
third_party/sleef/src/arch/helpers390x_128.h:static INLINE vopmask veq_vo_vi_vi(vint x, vint y) { return (vopmask)vreinterpretFirstHalf_vi2_vi(vec_cmpeq(x, y)); }
third_party/sleef/src/arch/helpers390x_128.h:static INLINE vopmask vgt_vo_vi_vi(vint x, vint y) { return (vopmask)vreinterpretFirstHalf_vi2_vi(vec_cmpgt(x, y));}
third_party/sleef/src/arch/helpers390x_128.h:  return vor_vi_vi_vi(vand_vi_vi_vi(vreinterpretFirstHalf_vi_vi2((vint2)m), x),
third_party/sleef/src/arch/helpers390x_128.h:		      vandnot_vi_vi_vi(vreinterpretFirstHalf_vi_vi2((vint2)m), y));
</para></entry></row>
third_party/cudnn_frontend/docs/xml/d9/dcc/classnlohmann_1_1basic__json.xml:<entry thead="no"><para>Half-Precision Float </para></entry><entry thead="no"><para>number_float </para></entry><entry thead="no"><para>0xF9 </para></entry></row>
third_party/cudnn_frontend/docs/xml/d5/db8/json_8hpp.xml:<codeline lineno="8953"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">case</highlight><highlight class="normal"><sp/>0xF9:<sp/></highlight><highlight class="comment">//<sp/>Half-Precision<sp/>Float<sp/>(two-byte<sp/>IEEE<sp/>754)</highlight><highlight class="normal"></highlight></codeline>
third_party/cudnn_frontend/include/contrib/nlohmann/json/json.hpp:            case 0xF9: // Half-Precision Float (two-byte IEEE 754)
third_party/cudnn_frontend/include/contrib/nlohmann/json/json.hpp:    Half-Precision Float   | number_float    | 0xF9
third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [ RUN      ] UnorderedElementsAreTest.PerformanceHalfStrict
third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [       OK ] UnorderedElementsAreTest.PerformanceHalfStrict (4 ms)
third_party/googletest/googlemock/test/gmock-matchers_test.cc:TEST_F(UnorderedElementsAreTest, PerformanceHalfStrict) {
third_party/googletest/googlemock/test/gmock-matchers_test.cc:class IsHalfOfMatcher {
third_party/googletest/googlemock/test/gmock-matchers_test.cc:PolymorphicMatcher<IsHalfOfMatcher> IsHalfOf() {
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  return MakePolymorphicMatcher(IsHalfOfMatcher());
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = Pointwise(IsHalfOf(), rhs);
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = Pointwise(IsHalfOf(), rhs);
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(Pointwise(IsHalfOf(), rhs)));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Pointwise(IsHalfOf(), rhs));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_EQ("", Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(UnorderedPointwise(IsHalfOf(), rhs)));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(UnorderedPointwise(IsHalfOf(), rhs), lhs));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_avx.h:inline __m256i RoundingHalfSum(__m256i a, __m256i b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint.h:IntegerType RoundingHalfSum(IntegerType a, IntegerType b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint.h:inline std::int32_t RoundingHalfSum(std::int32_t a, std::int32_t b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint.h:inline std::int16_t RoundingHalfSum(std::int16_t a, std::int16_t b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint.h:MAKE_FIXEDPOINT_BINARY_FUNC(RoundingHalfSum, RoundingHalfSum)
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint.h:  F0 half_denominator = RoundingHalfSum(a, F0::One());
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint.h:  F0 half_denominator = RoundingHalfSum(a, F0::One());
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_neon.h:inline int32x4_t RoundingHalfSum(int32x4_t a, int32x4_t b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_neon.h:inline int16x8_t RoundingHalfSum(int16x8_t a, int16x8_t b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_sse.h:inline __m128i RoundingHalfSum(__m128i a, __m128i b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_sse.h:inline int16x8_m128i RoundingHalfSum(int16x8_m128i a, int16x8_m128i b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_msa.h:inline v4i32 RoundingHalfSum(v4i32 a, v4i32 b) {
third_party/gemmlowp/gemmlowp/fixedpoint/fixedpoint_msa.h:inline v8i16 RoundingHalfSum(v8i16 a, v8i16 b) {
third_party/ideep/mkl-dnn/third_party/oneDNN/src/cpu/x64/injectors/jit_uni_eltwise_injector.cpp:    //           responding XMM. Halfway through the copy we exchange Xmm and
third_party/ideep/mkl-dnn/third_party/oneDNN/src/common/float16.hpp:            // Half denormal -> float normal
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/amd/sycl_hip_utils.hpp:            *miopen_data_type = miopenDataType_t::miopenHalf;
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_pseudo.hpp:               RegData zero, RegData oneHalf, RegData one, const A &tmp, InstructionModifier cfmod = InstructionModifier())
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_pseudo.hpp:            madm<DT>(mod, TMP(0) | mme1,     zero | nomme,  oneHalf | nomme,     dst | mme0);
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_pseudo.hpp:            madm<DT>(mod, TMP(2) | mme3,  oneHalf | nomme,  -TMP(1) | mme2,   TMP(0) | mme1);
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_pseudo.hpp:            madm<DT>(mod, TMP(0) | mme1,     zero | mme0,   oneHalf | nomme,     dst | mme0);
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_pseudo.hpp:            madm<DT>(mod, TMP(2) | mme3,  oneHalf | nomme,  -TMP(1) | mme2,   TMP(0) | mme1);
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_pseudo.hpp:            madm<DT>(mod, TMP(3) | mme4,      one | nomme,  oneHalf | nomme,     dst | nomme);
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_interface.hpp:    bool needHalf = false;
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_interface.hpp:        case DataType::hf: needHalf = true;   break;
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_interface.hpp:    if (needHalf)   stream << "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n";
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:    uint8_t highHalf;
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:    constexpr block_oword(uint8_t count_, bool highHalf_) : count(count_), highHalf(highHalf_) {}
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:    block_oword(int count_ = 1) : count(count_), highHalf(false) {}
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:        desc.block.elements = (count == 1) ? highHalf : (1 + utils::log2(count));
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:    uint8_t highHalf;
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:    constexpr aligned_block_oword(uint8_t count_, bool highHalf_) : count(count_), highHalf(highHalf_) {}
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:    aligned_block_oword(int count_ = 1) : count(count_), highHalf(false) {}
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/ngen/ngen_core.hpp:        desc.block.elements = (count == 1) ? highHalf : (1 + utils::log2(count));
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/gemm/xe_hp_systolic_gemm_kernel.cpp:        // Half-precision C must be upconverted to single precision separately (no hf/f mixed mode support in XeHP).
third_party/ideep/mkl-dnn/third_party/oneDNN/src/gpu/jit/conv/message_support.cpp:    // XXX: Half-GRF stores result in correctness issues on XeHPC.
third_party/ideep/mkl-dnn/third_party/oneDNN/doc/programming_model/data_types.md:| f16       | [IEEE half precision floating-point](https://en.wikipedia.org/wiki/Half-precision_floating-point_format#IEEE_754_half-precision_binary_floating-point_format:_binary16)
third_party/ideep/mkl-dnn/third_party/oneDNN/include/oneapi/dnnl/dnnl.hpp:        /// [16-bit/half-precision floating point](https://en.wikipedia.org/wiki/Half-precision_floating-point_format).
third_party/ideep/mkl-dnn/third_party/oneDNN/tests/gtests/test_softmax.cpp:GPU_INSTANTIATE_TEST_SUITE_P(TestSoftmaxForwardHalf, softmax_forward_test_half,
third_party/ideep/mkl-dnn/third_party/oneDNN/tests/gtests/test_softmax_v2.cpp:GPU_INSTANTIATE_TEST_SUITE_P(Test_Softmax_v2_Forward_Half, softmax_v2_test_t,
third_party/ideep/mkl-dnn/third_party/oneDNN/tests/gtests/test_logsoftmax.cpp:GPU_INSTANTIATE_TEST_SUITE_P(TestLogSoftmaxForwardHalf,
third_party/ideep/mkl-dnn/src/backend/graph_compiler/core/src/compiler/codegen/codegen_llvm.cpp:                ty = builder_.getHalfTy();
third_party/ideep/mkl-dnn/doc/programming_model/data_types.md:f16 | [16-bit/half-precision floating point](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [ RUN      ] UnorderedElementsAreTest.PerformanceHalfStrict
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [       OK ] UnorderedElementsAreTest.PerformanceHalfStrict (4 ms)
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:TEST_F(UnorderedElementsAreTest, PerformanceHalfStrict) {
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:class IsHalfOfMatcher {
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:PolymorphicMatcher<IsHalfOfMatcher> IsHalfOf() {
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  return MakePolymorphicMatcher(IsHalfOfMatcher());
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = Pointwise(IsHalfOf(), rhs);
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = Pointwise(IsHalfOf(), rhs);
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(Pointwise(IsHalfOf(), rhs)));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Pointwise(IsHalfOf(), rhs));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_EQ("", Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(UnorderedPointwise(IsHalfOf(), rhs)));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(UnorderedPointwise(IsHalfOf(), rhs), lhs));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/tensorpipe/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/python-peachpy/peachpy/x86_64/avx.py:    """Convert Single-Precision FP value to Half-Precision FP value"""
third_party/python-peachpy/peachpy/x86_64/avx.py:    """Convert Half-Precision FP Values to Single-Precision FP Values"""
Binary file third_party/python-peachpy/peachpy/x86_64/__pycache__/avx.cpython-38.pyc matches
Binary file third_party/python-peachpy/peachpy/x86_64/__pycache__/avx.cpython-310.pyc matches
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [ RUN      ] UnorderedElementsAreTest.PerformanceHalfStrict
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [       OK ] UnorderedElementsAreTest.PerformanceHalfStrict (4 ms)
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:TEST_F(UnorderedElementsAreTest, PerformanceHalfStrict) {
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:class IsHalfOfMatcher {
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:PolymorphicMatcher<IsHalfOfMatcher> IsHalfOf() {
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  return MakePolymorphicMatcher(IsHalfOfMatcher());
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = Pointwise(IsHalfOf(), rhs);
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = Pointwise(IsHalfOf(), rhs);
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(Pointwise(IsHalfOf(), rhs)));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Pointwise(IsHalfOf(), rhs));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_EQ("", Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(UnorderedPointwise(IsHalfOf(), rhs)));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(UnorderedPointwise(IsHalfOf(), rhs), lhs));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/protobuf/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/protobuf/src/google/protobuf/util/internal/json_stream_parser_test.cc:TEST_F(JsonStreamParserTest, UnicodeEscapeLowHalfSurrogateInvalidCharacters) {
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  const int kHalfBufferSize = 128;
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  const int kBufferSize = kHalfBufferSize * 2;
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  ArrayInputStream array_input(buffer, kBufferSize, kHalfBufferSize);
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  EXPECT_EQ(kHalfBufferSize, array_input.ByteCount());
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  // kHalfBufferSize - 1 to test limiting logic as well.
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  LimitingInputStream input(&array_input, kHalfBufferSize - 1);
third_party/protobuf/src/google/protobuf/io/zero_copy_stream_unittest.cc:  EXPECT_EQ(kHalfBufferSize - 1, input.ByteCount());
third_party/protobuf/objectivec/Tests/GPBMessageTests+Merge.m:  // Half the values that will replace.
third_party/fbgemm/third_party/hipify_torch/hipify/cuda_to_hip_mappings.py:            ("hipResViewFormatHalf1", CONV_TEX, API_DRIVER),
third_party/fbgemm/third_party/hipify_torch/hipify/cuda_to_hip_mappings.py:            ("hipResViewFormatHalf2", CONV_TEX, API_DRIVER),
third_party/fbgemm/third_party/hipify_torch/hipify/cuda_to_hip_mappings.py:            ("hipResViewFormatHalf4", CONV_TEX, API_DRIVER),
third_party/fbgemm/third_party/hipify_torch/hipify/cuda_to_hip_mappings.py:        ("cudaResViewFormatHalf1", ("hipResViewFormatHalf1", CONV_TEX, API_RUNTIME)),
third_party/fbgemm/third_party/hipify_torch/hipify/cuda_to_hip_mappings.py:        ("cudaResViewFormatHalf2", ("hipResViewFormatHalf2", CONV_TEX, API_RUNTIME)),
third_party/fbgemm/third_party/hipify_torch/hipify/cuda_to_hip_mappings.py:        ("cudaResViewFormatHalf4", ("hipResViewFormatHalf4", CONV_TEX, API_RUNTIME)),
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [ RUN      ] UnorderedElementsAreTest.PerformanceHalfStrict
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [       OK ] UnorderedElementsAreTest.PerformanceHalfStrict (4 ms)
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:TEST_F(UnorderedElementsAreTest, PerformanceHalfStrict) {
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:class IsHalfOfMatcher {
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:PolymorphicMatcher<IsHalfOfMatcher> IsHalfOf() {
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  return MakePolymorphicMatcher(IsHalfOfMatcher());
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = Pointwise(IsHalfOf(), rhs);
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = Pointwise(IsHalfOf(), rhs);
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(Pointwise(IsHalfOf(), rhs)));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Pointwise(IsHalfOf(), rhs));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_EQ("", Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(UnorderedPointwise(IsHalfOf(), rhs)));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(UnorderedPointwise(IsHalfOf(), rhs), lhs));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/fbgemm/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/fbgemm/third_party/asmjit/tools/tablegen-x86.js:              (ms === rs / 2) ? "Half"    :
third_party/fbgemm/third_party/asmjit/src/asmjit/core/archtraits.h:  kHalf,
third_party/fbgemm/third_party/asmjit/src/asmjit/core/archtraits.cpp:    ArchTypeNameId::kHalf,
third_party/fbgemm/third_party/asmjit/src/asmjit/x86/x86instapi.cpp:          case InstDB::RWInfoRm::kCategoryHalf:
third_party/fbgemm/third_party/asmjit/src/asmjit/x86/x86instdb_p.h:    kCategoryHalf,
third_party/fbgemm/third_party/asmjit/src/asmjit/x86/x86instdb.cpp:  { InstDB::RWInfoRm::kCategoryHalf      , 0x02, 0 , 0, 0 }, // #46 [ref=19x]
third_party/fbgemm/third_party/asmjit/src/asmjit/x86/x86instdb.cpp:  { InstDB::RWInfoRm::kCategoryHalf      , 0x01, 0 , 0, 0 }, // #48 [ref=10x]
third_party/fbgemm/third_party/asmjit/src/asmjit/arm/armoperand.h:    //! Halfword elements (H4 or H8).
third_party/fbgemm/third_party/asmjit/src/asmjit/arm/armoperand.h:    //! Halfword elements grouped by 2 halfwords (H2).
third_party/fbgemm/third_party/asmjit/src/asmjit/arm/a64assembler.cpp:struct HalfWordImm {
third_party/fbgemm/third_party/asmjit/src/asmjit/arm/a64assembler.cpp:static inline uint32_t countZeroHalfWords64(uint64_t imm) noexcept {
third_party/fbgemm/third_party/asmjit/src/asmjit/arm/a64assembler.cpp:  uint32_t zhw = countZeroHalfWords64( imm);
third_party/fbgemm/third_party/asmjit/src/asmjit/arm/a64assembler.cpp:  uint32_t ohw = countZeroHalfWords64(~imm);
third_party/fbgemm/bench/EmbeddingQuantizeBenchmark.cc:                  ? FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float>(
third_party/fbgemm/bench/EmbeddingQuantizeBenchmark.cc:                  : FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
third_party/fbgemm/src/QuantUtilsAvx2.cc:static inline uint16_t floatToHalf(float val) {
third_party/fbgemm/src/QuantUtilsAvx2.cc:void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfAvx2(
third_party/fbgemm/src/QuantUtilsAvx2.cc:    output_row_scale_bias[1] = floatToHalf(minimum_element);
third_party/fbgemm/src/QuantUtilsAvx2.cc:    std::uint16_t scale_fp16 = floatToHalf(scale);
third_party/fbgemm/src/QuantUtilsAvx2.cc:    output_row_scale_bias[0] = floatToHalf(scale);
third_party/fbgemm/src/QuantUtilsAvx2.cc:void FloatOrHalfToFused8BitRowwiseQuantizedSBFloatAvx2(
third_party/fbgemm/src/QuantUtilsAvx2.cc:void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfAvx2(
third_party/fbgemm/src/QuantUtilsAvx2.cc:void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfAvx2(
third_party/fbgemm/src/QuantUtilsAvx2.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfAvx2<type, bit_rate>( \
third_party/fbgemm/src/QuantUtilsAvx2.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfAvx2<type, bit_rate>( \
third_party/fbgemm/src/QuantUtilsAvx2.cc:  template void FloatOrHalfToFused8BitRowwiseQuantizedSBFloatAvx2<type>( \
third_party/fbgemm/src/QuantUtilsAvx2.cc:  template void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfAvx2<type>( \
third_party/fbgemm/src/GenerateKernelU8S8S32ACC16.cc:  auto extractDestHalf = extractDestFull.half();
third_party/fbgemm/src/GenerateKernelU8S8S32ACC16.cc:        emitExtractHalfVector<instSet, VecT>(
third_party/fbgemm/src/GenerateKernelU8S8S32ACC16.cc:            a, extractDestHalf, VecT(i * colRegs + j), idx);
third_party/fbgemm/src/GenerateKernelU8S8S32ACC16.cc:        a->vpmovsxwd(extractDestFull, extractDestHalf);
third_party/fbgemm/src/CodeGenHelpers.h: * @brief Emit partial extract from Wide regiter to Half Register, eg.
third_party/fbgemm/src/CodeGenHelpers.h:void emitExtractHalfVector(
third_party/fbgemm/src/CodeGenHelpers.h:void emitExtractHalfVector(
third_party/fbgemm/src/CodeGenHelpers.h:void emitExtractHalfVector(
third_party/fbgemm/src/RefImplementations.cc:      // Half float has 10 bits of mantissa, and float has 23, we are shifting
third_party/fbgemm/src/RowWiseSparseAdagradFused.cc:                  // Half float has 10 bits of mantissa, and float has 23, we
third_party/fbgemm/src/QuantUtils.cc:void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef(
third_party/fbgemm/src/QuantUtils.cc:void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/src/QuantUtils.cc:        FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfAvx2<InputType, 2>(
third_party/fbgemm/src/QuantUtils.cc:        FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfAvx2<InputType, 4>(
third_party/fbgemm/src/QuantUtils.cc:        FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfAvx2<InputType, 8>(
third_party/fbgemm/src/QuantUtils.cc:        FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<InputType>(
third_party/fbgemm/src/QuantUtils.cc:    FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<InputType>(
third_party/fbgemm/src/QuantUtils.cc:void FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef(
third_party/fbgemm/src/QuantUtils.cc:void FloatOrHalfToFused8BitRowwiseQuantizedSBFloat(
third_party/fbgemm/src/QuantUtils.cc:    FloatOrHalfToFused8BitRowwiseQuantizedSBFloatAvx2<InputType>(
third_party/fbgemm/src/QuantUtils.cc:    FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<InputType>(
third_party/fbgemm/src/QuantUtils.cc:void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef(
third_party/fbgemm/src/QuantUtils.cc:void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf(
third_party/fbgemm/src/QuantUtils.cc:        FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfAvx2<OutputType, 2>(
third_party/fbgemm/src/QuantUtils.cc:        FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfAvx2<OutputType, 4>(
third_party/fbgemm/src/QuantUtils.cc:        FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfAvx2<OutputType, 8>(
third_party/fbgemm/src/QuantUtils.cc:        FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef<OutputType>(
third_party/fbgemm/src/QuantUtils.cc:    FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef<OutputType>(
third_party/fbgemm/src/QuantUtils.cc:void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef(
third_party/fbgemm/src/QuantUtils.cc:void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf(
third_party/fbgemm/src/QuantUtils.cc:    Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfAvx2<OutputType>(
third_party/fbgemm/src/QuantUtils.cc:    Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef<OutputType>(
third_party/fbgemm/src/QuantUtils.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<type>(                       \
third_party/fbgemm/src/QuantUtils.cc:  template FBGEMM_API void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<type>( \
third_party/fbgemm/src/QuantUtils.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef<type>(                       \
third_party/fbgemm/src/QuantUtils.cc:  template FBGEMM_API void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<type>( \
third_party/fbgemm/src/QuantUtils.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<type>(                      \
third_party/fbgemm/src/QuantUtils.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<type>(                         \
third_party/fbgemm/src/QuantUtils.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef<type>(                      \
third_party/fbgemm/src/QuantUtils.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<type>(                         \
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf(
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void FloatOrHalfToFused8BitRowwiseQuantizedSBFloat(
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf(
third_party/fbgemm/include/fbgemm/QuantUtils.h: * Same as ToFusedNBitRowwiseQuantizedSBHalf but unoptimized.
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef(
third_party/fbgemm/include/fbgemm/QuantUtils.h: * Same as FloatOrHalfToFused8BitRowwiseQuantizedSBFloat but unoptimized.
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef(
third_party/fbgemm/include/fbgemm/QuantUtils.h: * Same as FusedNBitRowwiseQuantizedSBHalfToFloat but unoptimized.
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef(
third_party/fbgemm/include/fbgemm/QuantUtils.h: * Same as Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf but unoptimized.
third_party/fbgemm/include/fbgemm/QuantUtils.h:FBGEMM_API void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef(
third_party/fbgemm/include/fbgemm/QuantUtilsAvx2.h:void FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfAvx2(
third_party/fbgemm/include/fbgemm/QuantUtilsAvx2.h:void FloatOrHalfToFused8BitRowwiseQuantizedSBFloatAvx2(
third_party/fbgemm/include/fbgemm/QuantUtilsAvx2.h:void FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfAvx2(
third_party/fbgemm/include/fbgemm/QuantUtilsAvx2.h:void Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfAvx2(
third_party/fbgemm/fbgemm_gpu/bench/merge_embeddings_benchmark.py:                    else torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/bench/merge_embeddings_benchmark.py:            # Since their performance is similar, keep using Fused8BitRowwiseQuantizedToHalf for now.
third_party/fbgemm/fbgemm_gpu/bench/merge_embeddings_benchmark.py:            return torch.ops.fbgemm.Fused8BitRowwiseQuantizedToHalf(
third_party/fbgemm/fbgemm_gpu/bench/quantize_ops_benchmark.py:    quant_data_4bit = torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/bench/quantize_ops_benchmark.py:    quant_data_2bit = torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/bench/quantize_ops_benchmark.py:        torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf,
third_party/fbgemm/fbgemm_gpu/bench/quantize_ops_benchmark.py:        torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf,
third_party/fbgemm/fbgemm_gpu/bench/quantize_ops_benchmark.py:        torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloat,
third_party/fbgemm/fbgemm_gpu/bench/quantize_ops_benchmark.py:        torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloat,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:    at::PackedTensorAccessor32<c10::Half, 2, at::RestrictPtrTraits> values,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:    const at::PackedTensorAccessor32<c10::Half, 2, at::RestrictPtrTraits>
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:    const at::PackedTensorAccessor32<c10::Half, 3, at::RestrictPtrTraits> y0,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:    const at::PackedTensorAccessor32<c10::Half, 3, at::RestrictPtrTraits> y1,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 2, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 2, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 3, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 3, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 2, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 2, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 3, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:                      .packed_accessor32<c10::Half, 3, at::RestrictPtrTraits>(),
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:          at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:              at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:              at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:              at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:              at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  // TODO: replace Half by BFloat16, after BFloat16 is supported by Nvidia
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  auto output = at::empty({}, input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  at::native::gpu_kernel(iter, [] GPU_LAMBDA(float in) -> at::Half {
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:    return at::Half((temp.I + (1 << 15)) >> 16, at::Half::from_bits());
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  at::native::gpu_kernel(iter, [] GPU_LAMBDA(at::Half in) -> float {
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  return _float_to_fused8bitrowwise_gpu_t<at::Half>(input);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  } else { // T = at::Half
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:        input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  return _fused8bitrowwise_to_float_gpu_t<at::Half>(input);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:      output = _fused8bitrowwise_to_float_gpu_t<at::Half>(input);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  } else { // T = at::Half
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:        input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:          at::zeros({batch_size, output_dim}, input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:      2 * sizeof(at::Half);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  return _float_to_fusednbitrowwise_gpu_t<at::Half>(input, bit_rate);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  const int output_columns = (ncols - 2 * sizeof(at::Half)) * num_elem_per_byte;
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  } else { // T = at::Half
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:        input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:  return _fusednbitrowwise_to_float_gpu_t<at::Half>(input, bit_rate);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops.cu:      output = _fusednbitrowwise_to_float_gpu_t<at::Half>(input, bit_rate);
third_party/fbgemm/fbgemm_gpu/src/metric_ops.cu:  const auto output_options = weights.scalar_type() == at::ScalarType::Half
third_party/fbgemm/fbgemm_gpu/src/metric_ops.cu:        at::ScalarType::Half, labels.scalar_type(), "auc_wrapper_2", [&] {
third_party/fbgemm/fbgemm_gpu/src/ssd_split_embeddings_cache_cuda.cu:      at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:                    at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:                    at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:      B.data_ptr<at::Half>(),
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:      A.data_ptr<at::Half>(),
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:      C.data_ptr<at::Half>(),
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:      B.data_ptr<at::Half>(),
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:      A.data_ptr<at::Half>(),
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:      C.data_ptr<at::Half>(),
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/sparse_ops.cu:        at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/ssd_table_batched_embeddings.h:          {kRowInitBufferSize, max_D}, at::TensorOptions().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/ssd_table_batched_embeddings.h:                    at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/ssd_table_batched_embeddings.h:                    at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/src/split_embeddings_cache_cuda.cu:        if (stochastic_rounding && std::is_same<emb_t, at::Half>::value) {
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "HalfToFused8BitRowwiseQuantized",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "FloatOrHalfToFused8BitRowwiseQuantized",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "Fused8BitRowwiseQuantizedToHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "Fused8BitRowwiseQuantizedToFloatOrHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "FloatToFusedNBitRowwiseQuantizedSBHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "HalfToFusedNBitRowwiseQuantizedSBHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToFloat",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_gpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<input_t>(
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  fbgemm::Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<output_t>(
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      2 * sizeof(at::Half);
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<input_t>(
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      (ncols - 2 * sizeof(at::Half)) * num_elem_per_byte;
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  } else { // T = at::Half
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:        input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  fbgemm::FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<output_t>(
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:        } else { // scalar_t = at::Half
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  auto output = at::empty({0}, input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      output = at::empty({0}, input.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:        } else { // scalar_t = at::Half
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  m.def("HalfToFused8BitRowwiseQuantized(Tensor t) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  m.def("FloatOrHalfToFused8BitRowwiseQuantized(Tensor t) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:  m.def("Fused8BitRowwiseQuantizedToHalf(Tensor input) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "Fused8BitRowwiseQuantizedToFloatOrHalf(Tensor input, int output_dtype=0) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FloatToFusedNBitRowwiseQuantizedSBHalf(Tensor input, int bit_rate) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "HalfToFusedNBitRowwiseQuantizedSBHalf(Tensor input, int bit_rate) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(Tensor input, int bit_rate) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToFloat(Tensor input, int bit_rate) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToHalf(Tensor input, int bit_rate) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf(Tensor input, int bit_rate, int output_dtype=0) -> Tensor");
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "HalfToFused8BitRowwiseQuantized",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FloatOrHalfToFused8BitRowwiseQuantized",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "Fused8BitRowwiseQuantizedToHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "Fused8BitRowwiseQuantizedToFloatOrHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FloatToFusedNBitRowwiseQuantizedSBHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "HalfToFusedNBitRowwiseQuantizedSBHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToFloat",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToHalf",
third_party/fbgemm/fbgemm_gpu/src/quantize_ops_cpu.cpp:      "FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf",
third_party/fbgemm/fbgemm_gpu/src/sparse_ops_cpu.cpp:// TODO: replace Half by BFloat16, after BFloat16 is supported by Nvidia NCCL
third_party/fbgemm/fbgemm_gpu/src/sparse_ops_cpu.cpp:      input.options().dtype(at::kHalf)); // at::kHalf
third_party/fbgemm/fbgemm_gpu/src/sparse_ops_cpu.cpp:      reinterpret_cast<uint16_t*>(output.data_ptr<at::Half>()));
third_party/fbgemm/fbgemm_gpu/src/sparse_ops_cpu.cpp:// TODO: replace Half by BFloat16, after BFloat16 is supported by Nvidia NCCL
third_party/fbgemm/fbgemm_gpu/src/sparse_ops_cpu.cpp:      reinterpret_cast<at::BFloat16*>(input.data_ptr<at::Half>()),
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_quantized_cpu_template.cpp:                    std::is_same<output_t, at::Half>::value,
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_quantized_split_template.cu:    std::is_same<output_t, float>::value || std::is_same<output_t, at::BFloat16>::value || std::is_same<output_t, at::Half>::value || std::is_same<output_t, uint8_t>::value,
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_quantized_split_template.cu:        if (std::is_same<output_t, float>::value || std::is_same<output_t, at::Half>::value || std::is_same<output_t, at::BFloat16>::value) {
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_quantized_split_template.cu:    if (std::is_same<output_t, float>::value || std::is_same<output_t, at::Half>::value || std::is_same<output_t, at::BFloat16>::value) {
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_split_cpu.cpp:                               std::is_same<weights_t, at::Half>::value ||
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_split_cpu.cpp:            std::is_same<weights_t, at::Half>::value,
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_split_cpu.cpp:    output = at::empty({B, total_D}, weights.options().dtype(at::kHalf));
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_split_cpu.cpp:      !indice_weights.defined() || indice_weights.scalar_type() != at::kHalf);
third_party/fbgemm/fbgemm_gpu/codegen/embedding_forward_split_cpu.cpp:            at::ScalarType::Half,
third_party/fbgemm/fbgemm_gpu/codegen/embedding_backward_dense_host_cpu.cpp:    if (host_weights.scalar_type() == at::kHalf ||
third_party/fbgemm/fbgemm_gpu/codegen/embedding_backward_split_cpu_approx_template.cpp:       host_weights.scalar_type() == at::ScalarType::Half*/) &&
third_party/fbgemm/fbgemm_gpu/codegen/embedding_backward_split_cpu_template.cpp:struct half2float16<at::Half> {
third_party/fbgemm/fbgemm_gpu/codegen/embedding_backward_split_cpu_template.cpp:    // || std::is_same<scalar_t, at::Half>::value;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:      PRIVATE_CASE_TYPE_CACHE(at::ScalarType::Half, at::Half, __VA_ARGS__) \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:        at::ScalarType::Half, _cache_t, at::Half, NAME, __VA_ARGS__)          \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:          at::ScalarType::Half,                                    \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:          at::Half,                                                \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:      PRIVATE_CASE_TYPE_OUTPUT2(at::ScalarType::Half, at::Half, __VA_ARGS__) \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:          at::ScalarType::Half, _cache_t, at::Half, NAME, __VA_ARGS__)     \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/dispatch_macros.h:          at::ScalarType::Half, _cache_t, _emb_t, at::Half, NAME, __VA_ARGS__) \
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:// Customized Half4 data types with two half2 (64-bit in total)
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:struct Half4 {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  __device__ inline void store(at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:// Customized 4-element vector data types (with element type Half, float, or
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE Vec4T(const at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:struct Vec4T<at::Half> {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE Vec4T(const at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE static void copy(const at::Half* src, at::Half* dst) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void fma_(Vec4T<at::Half> a, float b) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void add_(Vec4T<at::Half> a) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void element_wise_mul_(Vec4T<at::Half> a) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE Vec4T(const at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void fma_(Vec4T<at::Half> a, float b) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void add_(Vec4T<at::Half> a) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void element_wise_mul_(Vec4T<at::Half> a) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE Vec4T(const at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* p) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Half4 out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    at::Half* output,
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Vec4T<at::Half> value,
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  Half4 v;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    at::Half* output,
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  Half4 v;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Vec4T<at::Half> value,
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:    Vec4T<at::Half> value,
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:DEVICE_INLINE Vec4T<at::Half> dequantize_load(uint8_t* value, float2 qparams) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  Vec4T<at::Half> out;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* output_ptr, int num_valid_outputs = 1) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  store(at::Half* output_ptr, float2 qparams, int num_valid_outputs = 1) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* output_ptr, int num_valid_outputs = 2) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:          output_ptr[i] = *reinterpret_cast<const at::Half*>(&val.x + i);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  store(at::Half* output_ptr, float2 qparams, int num_valid_outputs = 2) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* output_ptr, int num_valid_outputs = 4) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:            *reinterpret_cast<const at::Half*>(&(val.vals[0].x) + 2);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:              *reinterpret_cast<const at::Half*>(&(val.vals[0].x) + i);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  store(at::Half* output_ptr, float2 qparams, int num_valid_outputs = 4) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* output_ptr, int num_valid_outputs = 4) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:            *reinterpret_cast<const at::Half*>(&(val.vals[0].x) + 2);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:              *reinterpret_cast<const at::Half*>(&(val.vals[0].x) + i);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  store(at::Half* output_ptr, float2 qparams, int num_valid_outputs = 4) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* output_ptr, int num_valid_outputs = 8) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:              *reinterpret_cast<const at::Half*>(&(val.vals[0].x) + i);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  store(at::Half* output_ptr, float2 qparams, int num_valid_outputs = 8) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  DEVICE_INLINE void store(at::Half* output_ptr, int num_valid_outputs = 16) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:              *reinterpret_cast<const at::Half*>(&(val.vals[0].x) + i);
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/fbgemm_cuda_utils.cuh:  store(at::Half* output_ptr, float2 qparams, int num_valid_outputs = 16) {
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/embedding_common.h:      return at::kHalf;
third_party/fbgemm/fbgemm_gpu/include/fbgemm_gpu/embedding_common.h:    case at::kHalf:
third_party/fbgemm/fbgemm_gpu/fbgemm_gpu/split_embedding_utils.py:        q_weight = torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/fbgemm_gpu/split_embedding_inference_converter.py:            q_weight = torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/split_embedding_inference_converter_test.py:                    torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloat(
third_party/fbgemm/fbgemm_gpu/test/split_embedding_inference_converter_test.py:                        torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:            quantized_data = torch.ops.fbgemm.FloatOrHalfToFused8BitRowwiseQuantized(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                quantized_data = torch.ops.fbgemm.HalfToFused8BitRowwiseQuantized(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FloatOrHalfToFused8BitRowwiseQuantized(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.HalfToFused8BitRowwiseQuantized(input_data_gpu)
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:            quantized_data = torch.ops.fbgemm.FloatOrHalfToFused8BitRowwiseQuantized(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:            dequantized_data = torch.ops.fbgemm.Fused8BitRowwiseQuantizedToFloatOrHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                quantized_data = torch.ops.fbgemm.HalfToFused8BitRowwiseQuantized(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                dequantized_data = torch.ops.fbgemm.Fused8BitRowwiseQuantizedToHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FloatOrHalfToFused8BitRowwiseQuantized(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.Fused8BitRowwiseQuantizedToFloatOrHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.HalfToFused8BitRowwiseQuantized(input_data_gpu)
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.Fused8BitRowwiseQuantizedToHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                quantized_data = torch.ops.fbgemm.HalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.HalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloat(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                quantized_data = torch.ops.fbgemm.HalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                    torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloat(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.HalfToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                        torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                torch.ops.fbgemm.FloatToFusedNBitRowwiseQuantizedSBHalf(
third_party/fbgemm/fbgemm_gpu/test/quantize_ops_test.py:                torch.ops.fbgemm.FusedNBitRowwiseQuantizedSBHalfToFloat(
third_party/fbgemm/fbgemm_gpu/test/jagged_tensor_ops_test.py:        # torch.bmm not implemented for Half on CPU
third_party/fbgemm/fbgemm_gpu/test/jagged_tensor_ops_test.py:        # torch.bmm not implemented for Half on CPU
third_party/fbgemm/test/QuantUtilsTest.cc:TEST_P(EmbeddingQuantizeFixedNumberTest, embeddingFloatToQuantizedSBHalfTest) {
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<uint8_t> outVectHalfTest(row * out_cols_half);
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, float_test_input.data(), row, col, outVectHalfTest.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      expected_output_half[bit_rate], outVectHalfTest, row, col));
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, float_test_input.data(), row, col, outVectHalfTest.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      expected_output_half[bit_rate], outVectHalfTest, row, col));
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, float16_test_input.data(), row, col, outVectHalfTest.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      expected_output_half[bit_rate], outVectHalfTest, row, col));
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, float16_test_input.data(), row, col, outVectHalfTest.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      expected_output_half[bit_rate], outVectHalfTest, row, col));
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:TEST_P(EmbeddingQuantizeTest, embeddingHalfTest) {
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<float16> inpHalfVec(rows * cols);
third_party/fbgemm/test/QuantUtilsTest.cc:      inpVec.begin(), inpVec.end(), inpHalfVec.begin(), [](float input) {
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<uint8_t> outVecRefFromHalf(outVecSize);
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<uint8_t> outVecTestFromHalf(outVecSize);
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalfRef<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, inpHalfVec.data(), rows, cols, outVecRefFromHalf.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      outVecRefFromHalf, outVecRef, rows, out_emb_cols));
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, inpHalfVec.data(), rows, cols, outVecTestFromHalf.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      outVecRefFromHalf, outVecTestFromHalf, rows, out_emb_cols));
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<float16> dequantOutHalfRef(rows * cols);
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<float16> dequantOutHalfTest(rows * cols);
third_party/fbgemm/test/QuantUtilsTest.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalfRef<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, outVecRef.data(), rows, out_cols, dequantOutHalfRef.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      dequantOutRef, dequantOutHalfRef, 1e-3, pow(2, NumberOfFP16Matissa)));
third_party/fbgemm/test/QuantUtilsTest.cc:  FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      bit_rate, outVecRef.data(), rows, out_cols, dequantOutHalfTest.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      dequantOutHalfRef,
third_party/fbgemm/test/QuantUtilsTest.cc:      dequantOutHalfTest,
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<float16> inpHalfVec(rows * cols);
third_party/fbgemm/test/QuantUtilsTest.cc:      inpVec.begin(), inpVec.end(), inpHalfVec.begin(), [](float input) {
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<uint8_t> outVecRefFromHalf(outVecSize);
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<uint8_t> outVecTestFromHalf(outVecSize);
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloatRef<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      inpHalfVec.data(), rows, cols, outVecRefFromHalf.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      isQEmbeddingClose<float16>(outVecRefFromHalf, outVecRef, rows, cols));
third_party/fbgemm/test/QuantUtilsTest.cc:  FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      inpHalfVec.data(), rows, cols, outVecTestFromHalf.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      outVecRefFromHalf, outVecTestFromHalf, rows, cols));
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<float16> dequantOutHalfRef(rows * cols);
third_party/fbgemm/test/QuantUtilsTest.cc:  vector<float16> dequantOutHalfTest(rows * cols);
third_party/fbgemm/test/QuantUtilsTest.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef<float>(
third_party/fbgemm/test/QuantUtilsTest.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalfRef<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      outVecRef.data(), rows, out_cols, dequantOutHalfRef.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      dequantOutRef, dequantOutHalfRef, 1e-3, pow(2, NumberOfFP16Matissa)));
third_party/fbgemm/test/QuantUtilsTest.cc:  Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<float16>(
third_party/fbgemm/test/QuantUtilsTest.cc:      outVecRef.data(), rows, out_cols, dequantOutHalfTest.data());
third_party/fbgemm/test/QuantUtilsTest.cc:      dequantOutHalfRef,
third_party/fbgemm/test/QuantUtilsTest.cc:      dequantOutHalfTest,
third_party/nccl/nccl/src/nccl.h.in:               ncclFloat16    = 6, ncclHalf       = 6,
third_party/nccl/nccl/src/collectives/device/primitives.h:    return ncclShmem.comm.buffSizes[NCCL_PROTO_LL]/NCCL_STEPS/2; // Half is data
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [ RUN      ] UnorderedElementsAreTest.PerformanceHalfStrict
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:// [       OK ] UnorderedElementsAreTest.PerformanceHalfStrict (4 ms)
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:TEST_F(UnorderedElementsAreTest, PerformanceHalfStrict) {
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:class IsHalfOfMatcher {
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:PolymorphicMatcher<IsHalfOfMatcher> IsHalfOf() {
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  return MakePolymorphicMatcher(IsHalfOfMatcher());
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = Pointwise(IsHalfOf(), rhs);
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = Pointwise(IsHalfOf(), rhs);
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(Pointwise(IsHalfOf(), rhs)));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Pointwise(IsHalfOf(), rhs));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_EQ("", Explain(Pointwise(IsHalfOf(), rhs), lhs));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const vector<int>&> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<const int (&)[2]> m = UnorderedPointwise(IsHalfOf(), rhs);
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, Not(UnorderedPointwise(IsHalfOf(), rhs)));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:            Explain(UnorderedPointwise(IsHalfOf(), rhs), lhs));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  EXPECT_THAT(lhs, UnorderedPointwise(IsHalfOf(), rhs));
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<const double&, const int&>> m1 = IsHalfOf();
third_party/kineto/libkineto/third_party/googletest/googlemock/test/gmock-matchers_test.cc:  const Matcher<std::tuple<double, int>> m2 = IsHalfOf();
third_party/kineto/libkineto/third_party/fmt/doc/bootstrap/glyphicons.less:  font-family: 'Glyphicons Halflings';
third_party/kineto/libkineto/third_party/fmt/doc/bootstrap/glyphicons.less:  font-family: 'Glyphicons Halflings';
third_party/kineto/tb_plugin/torch_tb_profiler/profiler/communication.py:                elif comm_node.input_type[i] == 'c10::Half':
third_party/kineto/tb_plugin/torch_tb_profiler/static/trace_viewer_full.html:const viewSkipDistance=this.skipDistance_*pixelRatio;let selectedCircleRadius;let letterDotRadius;let squareSize;let squareHalfSize;let squareOpacity;let unselectedSeriesColor;let currentStateSeriesColor;ctx.save();ctx.font=DOT_LETTER_FONT_WEIGHT+' '+
third_party/kineto/tb_plugin/torch_tb_profiler/static/trace_viewer_full.html:DOT_LETTER_FONT;ctx.textBaseline='middle';ctx.textAlign='center';switch(component){case ChartSeriesComponent.DOTS:{selectedCircleRadius=(this.selectedPointSize_/2)*pixelRatio;letterDotRadius=Math.max(selectedCircleRadius,DOT_LETTER_RADIUS_PX*pixelRatio);squareSize=this.unselectedPointSize_*pixelRatio;squareHalfSize=squareSize/2;unselectedSeriesColor=EventPresenter.getCounterSeriesColor(this.colorId_,SelectionState.NONE);if(!highDetails){squareOpacity=0;break;}
third_party/kineto/tb_plugin/torch_tb_profiler/static/trace_viewer_full.html:ctx.beginPath();ctx.arc(currentViewX,currentViewY,selectedCircleRadius,0,2*Math.PI);ctx.fill();ctx.stroke();}else if(squareOpacity>0){ctx.fillStyle=currentStateSeriesColor;ctx.fillRect(currentViewX-squareHalfSize,currentViewY-squareHalfSize,squareSize,squareSize);}}
third_party/fmt/doc/bootstrap/glyphicons.less:  font-family: 'Glyphicons Halflings';
third_party/fmt/doc/bootstrap/glyphicons.less:  font-family: 'Glyphicons Halflings';
third_party/onnx/onnx/test/cpp/function_context_test.cc:    //   t.add_int32_data(onnxruntime::math::floatToHalf((float)value));
third_party/gloo/tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py:            ("hipResViewFormatHalf1", CONV_TEX, API_DRIVER),
third_party/gloo/tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py:            ("hipResViewFormatHalf2", CONV_TEX, API_DRIVER),
third_party/gloo/tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py:            ("hipResViewFormatHalf4", CONV_TEX, API_DRIVER),
third_party/gloo/tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py:        ("cudaResViewFormatHalf1", ("hipResViewFormatHalf1", CONV_TEX, API_RUNTIME)),
third_party/gloo/tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py:        ("cudaResViewFormatHalf2", ("hipResViewFormatHalf2", CONV_TEX, API_RUNTIME)),
third_party/gloo/tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py:        ("cudaResViewFormatHalf4", ("hipResViewFormatHalf4", CONV_TEX, API_RUNTIME)),
third_party/gloo/gloo/cuda_private.cu:  half* ptrAsHalf = (half*) ptr;
third_party/gloo/gloo/cuda_private.cu:    ptrAsHalf[i] = __float2half(static_cast<float>((i * stride) + val));
third_party/gloo/gloo/nccl/nccl.cu:  static const ncclDataType_t type = ncclHalf;
third_party/gloo/gloo/test/allreduce_test.cc:TEST_F(AllreduceTestHP, HalfPrecisionTest) {
third_party/gloo/gloo/test/reduce_scatter_test.cc:TEST_F(ReduceScatterTestHP, HalfPrecisionTest) {
third_party/gloo/gloo/test/cuda_allreduce_test.cc:TEST_F(CudaAllreduceTestHP, HalfPrecisionTest) {
third_party/eigen/bench/benchFFT.cpp:        fft.SetFlag(fft.HalfSpectrum);
third_party/eigen/Eigen/src/SparseCore/SparseSelfAdjointView.h:    ProcessFirstHalf =
third_party/eigen/Eigen/src/SparseCore/SparseSelfAdjointView.h:    ProcessSecondHalf = !ProcessFirstHalf
third_party/eigen/Eigen/src/SparseCore/SparseSelfAdjointView.h:      if (ProcessSecondHalf)
third_party/eigen/Eigen/src/SparseCore/SparseSelfAdjointView.h:      for(; (ProcessFirstHalf ? i && i.index() < j : i) ; ++i)
third_party/eigen/Eigen/src/SparseCore/SparseSelfAdjointView.h:      if (ProcessFirstHalf && i && (i.index()==j))
third_party/eigen/Eigen/src/Core/GenericPacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/GenericPacketMath.h:    HasHalfPacket = 0
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  GEBPPacketHalf,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:struct packet_conditional<GEBPPacketHalf, T1, T2, T3> { typedef T2 type; };
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  template<typename ResPacketHalf>
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  EIGEN_STRONG_INLINE void acc(const ResPacketHalf& c, const ResPacketHalf& alpha, ResPacketHalf& r) const
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef gebp_traits<LhsScalar,RhsScalar,ConjugateLhs,ConjugateRhs,Architecture::Target,GEBPPacketHalf> HalfTraits;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename HalfTraits::LhsPacket LhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename HalfTraits::RhsPacket RhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename HalfTraits::ResPacket ResPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename HalfTraits::AccPacket AccPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    LhsProgressHalf      = HalfTraits::LhsProgress,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    RhsProgressHalf      = HalfTraits::RhsProgress,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    const Index peeled_mc_half = mr>=LhsProgressHalf ? peeled_mc1+((rows-peeled_mc1)/(LhsProgressHalf))*(LhsProgressHalf) : 0;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    //---------- Process LhsProgressHalf rows at once ----------
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    if((LhsProgressHalf < LhsProgress) && mr>=LhsProgressHalf)
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:      lhs_process_fraction_of_packet<nr, LhsProgressHalf, RhsProgressHalf, LhsScalar, RhsScalar, ResScalar, AccPacketHalf, LhsPacketHalf, RhsPacketHalf, ResPacketHalf, HalfTraits, LinearMapper, DataMapper> p;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    if((LhsProgressQuarter < LhsProgressHalf) && mr>=LhsProgressQuarter)
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:          const int SResPacketHalfSize = unpacket_traits<typename unpacket_traits<SResPacket>::half>::size;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              (SwappedTraits::LhsProgress!=8  || SResPacketHalfSize==nr) &&
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              typedef typename conditional<SwappedTraits::LhsProgress>=8,typename unpacket_traits<SResPacket>::half,SResPacket>::type SResPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              typedef typename conditional<SwappedTraits::LhsProgress>=8,typename unpacket_traits<SLhsPacket>::half,SLhsPacket>::type SLhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              typedef typename conditional<SwappedTraits::LhsProgress>=8,typename unpacket_traits<SRhsPacket>::half,SRhsPacket>::type SRhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              typedef typename conditional<SwappedTraits::LhsProgress>=8,typename unpacket_traits<SAccPacket>::half,SAccPacket>::type SAccPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              SResPacketHalf R = res.template gatherPacket<SResPacketHalf>(i, j2);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              SResPacketHalf alphav = pset1<SResPacketHalf>(alpha);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:                SLhsPacketHalf a0;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:                SRhsPacketHalf b0;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:                SAccPacketHalf c0 = predux_half_dowto4(C0);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename unpacket_traits<Packet>::half HalfPacket;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HalfPacketSize = unpacket_traits<HalfPacket>::size,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HasHalf = (int)HalfPacketSize < (int)PacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize};
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  const Index peeled_mc_half = Pack1>=HalfPacketSize ? peeled_mc1+((rows-peeled_mc1)/(HalfPacketSize))*(HalfPacketSize) : 0;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  if(HasHalf && Pack1>=HalfPacketSize)
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    for(; i<peeled_mc_half; i+=HalfPacketSize)
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:      if(PanelMode) count += (HalfPacketSize) * offset;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:        HalfPacket A;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:        A = lhs.template loadPacket<HalfPacket>(i+0*(HalfPacketSize), k);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:        count+=HalfPacketSize;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:      if(PanelMode) count += (HalfPacketSize) * (stride-offset-depth);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename unpacket_traits<Packet>::half HalfPacket;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HalfPacketSize = unpacket_traits<HalfPacket>::size,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HasHalf = (int)HalfPacketSize < (int)PacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize};
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:            } else if (HasHalf && psize == HalfPacketSize) {
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              PacketBlock<HalfPacket> kernel_half;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:              for (int p = 0; p < psize; ++p) kernel_half.packet[p] = lhs.template loadPacket<HalfPacket>(i+p+m, k);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:          ((psize/2 == HalfPacketSize && HasHalf && !gone_half) ||
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:  typedef typename unpacket_traits<Packet>::half HalfPacket;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:         HalfPacketSize = unpacket_traits<HalfPacket>::size,
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    const bool HasHalf = (int)HalfPacketSize < (int)PacketSize;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:    const bool HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize;
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:          } else if (HasHalf && HalfPacketSize==4) {
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:            HalfPacket A = rhs.template loadPacket<HalfPacket>(k, j2);
third_party/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:            count += HalfPacketSize;
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:    typedef typename unpacket_traits<typename packet_traits<Scalar>::type>::half HalfPacket;
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:           HalfPacketSize = unpacket_traits<HalfPacket>::size,
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:           HasHalf = (int)HalfPacketSize < (int)PacketSize,
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:           HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize};
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:    const Index peeled_mc_half = Pack1>=HalfPacketSize ? peeled_mc1+((rows-peeled_mc1)/(HalfPacketSize))*(HalfPacketSize) : 0;
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:    if(HasHalf && Pack1>=HalfPacketSize)
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:      for(Index i=peeled_mc1; i<peeled_mc_half; i+=HalfPacketSize)
third_party/eigen/Eigen/src/Core/products/SelfadjointMatrixMatrix.h:        pack<HalfPacketSize>(blockA, lhs, cols, i, count);
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  GEMVPacketHalf,
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:struct gemv_packet_cond<GEMVPacketHalf, T1, T2, T3> { typedef T2 type; };
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef gemv_traits<LhsScalar,RhsScalar,GEMVPacketHalf> HalfTraits;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef typename HalfTraits::LhsPacket LhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef typename HalfTraits::RhsPacket RhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef typename HalfTraits::ResPacket ResPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  conj_helper<LhsPacketHalf,RhsPacketHalf,ConjugateLhs,ConjugateRhs> pcj_half;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         ResPacketSizeHalf = HalfTraits::ResPacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         HasHalf = (int)ResPacketSizeHalf < (int)ResPacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         HasQuarter = (int)ResPacketSizeQuarter < (int)ResPacketSizeHalf
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  const Index n_half = rows-1*ResPacketSizeHalf+1;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  ResPacketHalf palpha_half = pset1<ResPacketHalf>(alpha);
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:    if(HasHalf && i<n_half)
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:      ResPacketHalf c0 = pset1<ResPacketHalf>(ResScalar(0));
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:        RhsPacketHalf b0 = pset1<RhsPacketHalf>(rhs(j,0));
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:        c0 = pcj_half.pmadd(lhs.template load<LhsPacketHalf,LhsAlignment>(i+0,j),b0,c0);
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:      pstoreu(res+i+ResPacketSizeHalf*0, pmadd(c0,palpha_half,ploadu<ResPacketHalf>(res+i+ResPacketSizeHalf*0)));
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:      i+=ResPacketSizeHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef gemv_traits<LhsScalar,RhsScalar,GEMVPacketHalf> HalfTraits;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef typename HalfTraits::LhsPacket LhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef typename HalfTraits::RhsPacket RhsPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  typedef typename HalfTraits::ResPacket ResPacketHalf;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:  conj_helper<LhsPacketHalf,RhsPacketHalf,ConjugateLhs,ConjugateRhs> pcj_half;
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         ResPacketSizeHalf = HalfTraits::ResPacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         LhsPacketSizeHalf = HalfTraits::LhsPacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         HasHalf = (int)ResPacketSizeHalf < (int)ResPacketSize,
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:         HasQuarter = (int)ResPacketSizeQuarter < (int)ResPacketSizeHalf
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:    ResPacketHalf c0_h = pset1<ResPacketHalf>(ResScalar(0));
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:    if (HasHalf) {
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:      for(; j+LhsPacketSizeHalf<=cols; j+=LhsPacketSizeHalf)
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:          RhsPacketHalf b0 = rhs.template load<RhsPacketHalf,Unaligned>(j,0);
third_party/eigen/Eigen/src/Core/products/GeneralMatrixVector.h:          c0_h = pcj_half.pmadd(lhs.template load<LhsPacketHalf,LhsAlignment>(i,j),b0,c0_h);
third_party/eigen/Eigen/src/Core/util/XprHelper.h:         bool TryHalf   =  bool(EIGEN_MIN_ALIGN_BYTES<AlignmentBytes) >
third_party/eigen/Eigen/src/Core/util/XprHelper.h:template<int ArrayBytes, int AlignmentBytes, bool TryHalf>
third_party/eigen/Eigen/src/Core/util/XprHelper.h:struct compute_default_alignment_helper<ArrayBytes, AlignmentBytes, true, TryHalf> // Match
third_party/eigen/Eigen/src/Core/Reverse.h:    const int HalfAtCompileTime = ExpressionType::RowsAtCompileTime==Dynamic?Dynamic:ExpressionType::RowsAtCompileTime/2;
third_party/eigen/Eigen/src/Core/Reverse.h:    xpr.topRows(fix<HalfAtCompileTime>(half))
third_party/eigen/Eigen/src/Core/Reverse.h:       .swap(xpr.bottomRows(fix<HalfAtCompileTime>(half)).colwise().reverse());
third_party/eigen/Eigen/src/Core/Reverse.h:    const int HalfAtCompileTime = ExpressionType::ColsAtCompileTime==Dynamic?Dynamic:ExpressionType::ColsAtCompileTime/2;
third_party/eigen/Eigen/src/Core/Reverse.h:    xpr.leftCols(fix<HalfAtCompileTime>(half))
third_party/eigen/Eigen/src/Core/Reverse.h:       .swap(xpr.rightCols(fix<HalfAtCompileTime>(half)).rowwise().reverse());
third_party/eigen/Eigen/src/Core/Redux.h:    HalfLength = Length/2
third_party/eigen/Eigen/src/Core/Redux.h:    return func(redux_novec_unroller<Func, Evaluator, Start, HalfLength>::run(eval,func),
third_party/eigen/Eigen/src/Core/Redux.h:                redux_novec_unroller<Func, Evaluator, Start+HalfLength, Length-HalfLength>::run(eval,func));
third_party/eigen/Eigen/src/Core/Redux.h:      HalfLength = Length/2
third_party/eigen/Eigen/src/Core/Redux.h:            redux_vec_unroller<Func, Evaluator, Start, HalfLength>::template run<PacketType>(eval,func),
third_party/eigen/Eigen/src/Core/Redux.h:            redux_vec_unroller<Func, Evaluator, Start+HalfLength, Length-HalfLength>::template run<PacketType>(eval,func) );
third_party/eigen/Eigen/src/Core/arch/NEON/Complex.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/NEON/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AltiVec/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AltiVec/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX512/Complex.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX512/Complex.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX512/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX512/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX512/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX512/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/SSE/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SSE/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SSE/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SSE/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SSE/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SSE/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/MSA/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/MSA/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/MSA/PacketMath.h:    HasHalfPacket = 0,  // Packet2f intrinsics not implemented yet
third_party/eigen/Eigen/src/Core/arch/MSA/PacketMath.h:    HasHalfPacket = 0,  // Packet2i intrinsics not implemented yet
third_party/eigen/Eigen/src/Core/arch/MSA/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SYCL/InteropHeaders.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AVX/Complex.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX/Complex.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/AVX/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/AVX/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/GPU/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/GPU/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/GPU/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/ZVector/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/ZVector/Complex.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/ZVector/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/ZVector/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/ZVector/PacketMath.h:    HasHalfPacket = 1,
third_party/eigen/Eigen/src/Core/arch/SVE/PacketMath.h:  typedef PacketXi half;  // Half not implemented yet
third_party/eigen/Eigen/src/Core/arch/SVE/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SVE/PacketMath.h:  typedef PacketXi half;  // Half not yet implemented
third_party/eigen/Eigen/src/Core/arch/SVE/PacketMath.h:    HasHalfPacket = 0,
third_party/eigen/Eigen/src/Core/arch/SVE/PacketMath.h:  typedef PacketXf half;  // Half not yet implemented
third_party/eigen/Eigen/Core:#include "src/Core/arch/Default/Half.h"
third_party/eigen/test/vectorization_logic.cpp:  typedef typename internal::unpacket_traits<PacketType>::half HalfPacketType;
third_party/eigen/test/vectorization_logic.cpp:    HalfPacketSize = internal::unpacket_traits<HalfPacketType>::size
third_party/eigen/test/packetmath_test_shared.h:  bool HasHalf = !internal::is_same<typename internal::unpacket_traits<PacketType>::half,PacketType>::value >
third_party/eigen/test/packetmath.cpp:          bool HasHalf = !internal::is_same<typename internal::unpacket_traits<TgtPacket>::half, TgtPacket>::value>
third_party/eigen/test/packetmath.cpp:    int HalfPacketSize = PacketSize > 4 ? PacketSize / 2 : PacketSize;
third_party/eigen/test/packetmath.cpp:    for (int i = 0; i < HalfPacketSize; ++i) ref[i] = Scalar(0);
third_party/eigen/test/packetmath.cpp:    for (int i = 0; i < PacketSize; ++i) ref[i % HalfPacketSize] += data1[i];
third_party/eigen/test/packetmath.cpp:    VERIFY(test::areApprox(ref, data2, HalfPacketSize) && "internal::predux_half_dowto4");
third_party/eigen/test/half_float.cpp:#include <Eigen/src/Core/arch/Default/Half.h>
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMeta.h:    HasHalfPacket = 0
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void ReductionInitFullReduxKernelHalfFloat(Reducer reducer, const Self input, Index num_coeffs,
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void ReductionInitKernelHalfFloat(Reducer reducer, const Self input, Index num_coeffs, half* output) {
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void FullReductionKernelHalfFloat(Reducer reducer, const Self input, Index num_coeffs,
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void ReductionCleanupKernelHalfFloat(Op reducer, half* output, packet_traits<Eigen::half>::type* scratch) {
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:      LAUNCH_GPU_KERNEL((ReductionInitFullReduxKernelHalfFloat<Self, Op, Index>),
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:    LAUNCH_GPU_KERNEL((FullReductionKernelHalfFloat<block_size, num_per_thread, Self, Op, Index>),
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:      LAUNCH_GPU_KERNEL((ReductionCleanupKernelHalfFloat<Op>),
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void InnerReductionKernelHalfFloat(Reducer reducer, const Self input, Index num_coeffs_to_reduce, Index num_preserved_coeffs,
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:      LAUNCH_GPU_KERNEL((ReductionInitKernelHalfFloat<Self, Op, Index>),
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h:    LAUNCH_GPU_KERNEL((InnerReductionKernelHalfFloat<num_per_thread, Self, Op, Index>),
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void ReductionInitFullReduxKernelHalfFloat(R, const S, I_, internal::packet_traits<half>::type*);
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void FullReductionKernelHalfFloat(R, const S, I_, half*, internal::packet_traits<half>::type*);
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:__global__ EIGEN_HIP_LAUNCH_BOUNDS_1024 void InnerReductionKernelHalfFloat(R, const S, I_, I_, half*);
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:  template <typename S, typename R, typename I_> KERNEL_FRIEND void internal::ReductionInitFullReduxKernelHalfFloat(R, const S, I_, internal::packet_traits<Eigen::half>::type*);
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:  template <int B, int N, typename S, typename R, typename I_> KERNEL_FRIEND void internal::FullReductionKernelHalfFloat(R, const S, I_, half*, internal::packet_traits<Eigen::half>::type*);
third_party/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:  template <int NPT, typename S, typename R, typename I_> KERNEL_FRIEND void internal::InnerReductionKernelHalfFloat(R, const S, I_, I_, half*);
third_party/eigen/unsupported/Eigen/CXX11/src/ThreadPool/RunQueue.h:  // PopBackHalf removes and returns half last elements in the queue.
third_party/eigen/unsupported/Eigen/CXX11/src/ThreadPool/RunQueue.h:  unsigned PopBackHalf(std::vector<Work>* result) {
third_party/eigen/unsupported/Eigen/src/Skyline/SkylineProduct.h:        ProcessFirstHalf = LhsIsSelfAdjoint
third_party/eigen/unsupported/Eigen/src/Skyline/SkylineProduct.h:        ProcessSecondHalf = LhsIsSelfAdjoint && (!ProcessFirstHalf)
third_party/eigen/unsupported/Eigen/src/Skyline/SkylineProduct.h:        ProcessFirstHalf = LhsIsSelfAdjoint
third_party/eigen/unsupported/Eigen/src/Skyline/SkylineProduct.h:        ProcessSecondHalf = LhsIsSelfAdjoint && (!ProcessFirstHalf)
third_party/eigen/unsupported/Eigen/FFT:      HalfSpectrum=2,
third_party/eigen/unsupported/Eigen/FFT:        if ( HasFlag(HalfSpectrum) == false)
third_party/eigen/unsupported/Eigen/FFT:      if ( NumTraits<_Input>::IsComplex == 0 && HasFlag(HalfSpectrum) )
third_party/eigen/unsupported/Eigen/FFT:      if ( NumTraits< src_type >::IsComplex == 0 && HasFlag(HalfSpectrum) )
third_party/eigen/unsupported/Eigen/FFT:        if ( realfft && HasFlag(HalfSpectrum) ) 
third_party/eigen/unsupported/Eigen/FFT:      Index resize_input= ( realfft && HasFlag(HalfSpectrum) )
third_party/eigen/unsupported/Eigen/FFT:          if ( realfft && HasFlag(HalfSpectrum) ) {
third_party/eigen/unsupported/Eigen/FFT:        nfft = ( NumTraits<_Output>::IsComplex == 0 && HasFlag(HalfSpectrum) ) ? 2*(src.size()-1) : src.size();
third_party/eigen/unsupported/Eigen/SpecialFunctions:#include "src/SpecialFunctions/BesselFunctionsHalf.h"
third_party/eigen/unsupported/Eigen/SpecialFunctions:#include "src/SpecialFunctions/SpecialFunctionsHalf.h"
third_party/eigen/unsupported/test/cxx11_tensor_of_float16_gpu.cu:  std::cout << "Half floats are not supported by this version of gpu: skipping the test" << std::endl;
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:  VERIFY_IS_EQUAL(0u, q.PopBackHalf(&stolen));
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:  VERIFY_IS_EQUAL(1u, q.PopBackHalf(&stolen));
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:  VERIFY_IS_EQUAL(2u, q.PopBackHalf(&stolen));
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:  VERIFY_IS_EQUAL(1u, q.PopBackHalf(&stolen));
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:  VERIFY_IS_EQUAL(1u, q.PopBackHalf(&stolen));
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:  VERIFY_IS_EQUAL(0u, q.PopBackHalf(&stolen));
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:          if (q.PopBackHalf(&stolen) == 1) {
third_party/eigen/unsupported/test/cxx11_runqueue.cpp:        if (q.PopBackHalf(&stolen) == 0) {
third_party/eigen/unsupported/test/FFTW.cpp:    fft.SetFlag(fft.HalfSpectrum );
third_party/eigen/unsupported/test/FFTW.cpp:    fft.ClearFlag(fft.HalfSpectrum );
third_party/eigen/unsupported/test/FFTW.cpp:    fft.SetFlag(fft.HalfSpectrum );
third_party/nlohmann/single_include/nlohmann/json.hpp:            case 0xF9: // Half-Precision Float (two-byte IEEE 754)
third_party/nlohmann/docs/mkdocs/docs/features/binary_formats/cbor.md:| Half-Precision Float   | number_float    | 0xF9       |
third_party/nlohmann/include/nlohmann/detail/input/binary_reader.hpp:            case 0xF9: // Half-Precision Float (two-byte IEEE 754)
third_party/nlohmann/tests/src/unit-cbor.cpp:                SECTION("other values from https://en.wikipedia.org/wiki/Half-precision_floating-point_format")
third_party/nlohmann/tests/src/unit-bjdata.cpp:                SECTION("other values from https://en.wikipedia.org/wiki/Half-precision_floating-point_format")
third_party/nlohmann/tests/src/unit-regression1.cpp:        // related test case: incomplete Half-Precision Float (CBOR)
third_party/tbb/src/rml/test/test_server.h:        // Half of the stack is reserved for RSE, so test only remaining half.


####################################################################################################

tools/pyi/gen_pyi.py:        "HalfTensor",

tools/autograd/templates/python_variable_methods.cpp:  return THPVariable_to_type(self, ScalarType::Half, opt_memory_format);
tools/iwyu/c10.imp:    { include: [ "<c10/util/Half-inl.h>", private, "<c10/util/Half.h>", public ] },
tools/test/test_executorch_gen.py:    Generic: add (AllAndComplex, BFloat16, Half, ComplexHalf)
####################################################################################################

torch/_prims_common/__init__.py:        torch.half: DataType.Half,
torch/_prims_common/__init__.py:        lambda: f"{fn_name}: Half precision dtypes not supported. Got {dtype}",
torch/_decomp/decompositions.py:    firstHalf = self.narrow(wrap_dim, 0, inputSize)
torch/_decomp/decompositions.py:    secondHalf = self.narrow(wrap_dim, inputSize, inputSize)
torch/_decomp/decompositions.py:    gradInputFirstHalf = torch.sigmoid(secondHalf)
torch/_decomp/decompositions.py:    gradInputSecondHalf = (
torch/_decomp/decompositions.py:        (1.0 - gradInputFirstHalf) * gradInputFirstHalf * firstHalf * grad_output
torch/_decomp/decompositions.py:    gradInputFirstHalf = gradInputFirstHalf * grad_output
torch/_decomp/decompositions.py:    return torch.cat([gradInputFirstHalf, gradInputSecondHalf], dim=wrap_dim)
torch/distributions/kl.py:from .half_normal import HalfNormal
torch/distributions/kl.py:@register_kl(HalfNormal, HalfNormal)
torch/distributions/half_normal.py:__all__ = ['HalfNormal']
torch/distributions/half_normal.py:class HalfNormal(TransformedDistribution):
torch/distributions/half_normal.py:        Y = |X| ~ HalfNormal(scale)
torch/distributions/half_normal.py:        >>> m = HalfNormal(torch.tensor([1.0]))
torch/distributions/half_normal.py:        super(HalfNormal, self).__init__(base_dist, AbsTransform(),
torch/distributions/half_normal.py:        new = self._get_checked_instance(HalfNormal, _instance)
torch/distributions/half_normal.py:        return super(HalfNormal, self).expand(batch_shape, _instance=new)
torch/distributions/constraints.py:class _HalfOpenInterval(Constraint):
torch/distributions/constraints.py:half_open_interval = _HalfOpenInterval
torch/distributions/half_cauchy.py:__all__ = ['HalfCauchy']
torch/distributions/half_cauchy.py:class HalfCauchy(TransformedDistribution):
torch/distributions/half_cauchy.py:        Y = |X| ~ HalfCauchy(scale)
torch/distributions/half_cauchy.py:        >>> m = HalfCauchy(torch.tensor([1.0]))
torch/distributions/half_cauchy.py:        super(HalfCauchy, self).__init__(base_dist, AbsTransform(),
torch/distributions/half_cauchy.py:        new = self._get_checked_instance(HalfCauchy, _instance)
torch/distributions/half_cauchy.py:        return super(HalfCauchy, self).expand(batch_shape, _instance=new)
torch/distributions/__init__.py:from .half_cauchy import HalfCauchy
torch/distributions/__init__.py:from .half_normal import HalfNormal
torch/distributions/__init__.py:    'HalfCauchy',
torch/distributions/__init__.py:    'HalfNormal',
torch/distributions/transformed_distribution.py:    :class:`~torch.distributions.half_cauchy.HalfCauchy`,
torch/distributions/transformed_distribution.py:    :class:`~torch.distributions.half_normal.HalfNormal`,



torch/csrc/TypeInfo.cpp:      at::kHalf, at::ScalarType::BFloat16, self->type, "epsilon", [] {
torch/csrc/TypeInfo.cpp:      at::kHalf, at::ScalarType::BFloat16, self->type, "max", [] {
torch/csrc/TypeInfo.cpp:      at::kHalf, at::ScalarType::BFloat16, self->type, "lowest", [] {
torch/csrc/TypeInfo.cpp:      at::kHalf, at::ScalarType::BFloat16, self->type, "min", [] {
torch/csrc/TypeInfo.cpp:      at::kHalf, at::ScalarType::BFloat16, self->type, "digits10", [] {
torch/csrc/TypeInfo.cpp:      at::kHalf, at::ScalarType::BFloat16, self->type, "dtype", [primary_name] {
torch/csrc/utils.h:#define THPHalfUtils_checkReal(object) THPUtils_checkReal_FLOAT(object)
torch/csrc/utils.h:#define THPHalfUtils_unpackReal(object) \
torch/csrc/utils.h:  (at::Half) THPUtils_unpackReal_FLOAT(object)
torch/csrc/utils.h:#define THPHalfUtils_newReal(value) PyFloat_FromDouble(value)
torch/csrc/utils.h:#define THPHalfUtils_newAccreal(value) THPUtils_newReal_FLOAT(value)
torch/csrc/cuda/nccl.cpp:    case at::kHalf:
torch/csrc/cuda/nccl.cpp:      return ncclDataType_t::ncclHalf;
torch/csrc/cuda/nccl.h:  Half = 6,
torch/csrc/StorageMethods.cpp:  } else if (scalar_type == at::kHalf) {
torch/csrc/StorageMethods.cpp:    torch::utils::THP_decodeHalfBuffer(
torch/csrc/StorageMethods.cpp:        storage->data<c10::Half>(), src + offset, byte_order, count);








































torch/csrc/utils/byte_order.h:#include <c10/util/Half.h>
torch/csrc/utils/byte_order.h:TORCH_API void THP_decodeHalfBuffer(
torch/csrc/utils/byte_order.h:    c10::Half* dst,
torch/csrc/utils/byte_order.cpp:void THP_decodeHalfBuffer(
torch/csrc/utils/byte_order.cpp:    c10::Half* dst,
torch/csrc/utils/byte_order.cpp:      c10::Half f;



