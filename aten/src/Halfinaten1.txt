ATen/core/type_factory.cpp:  _(HalfTensor, TensorType)         \
ATen/core/PhiloxRNGEngine.h:#include <c10/util/Half.h>
ATen/core/ATen_pch.h:#include <c10/util/Half.h>
ATen/core/DistributionsHelper.h:#include <c10/util/Half.h>
ATen/core/TransformationHelper.h:#include <c10/util/Half.h>
ATen/core/TransformationHelper.h:template <> struct DistAccumType<Half> { using type = float; };
ATen/core/TransformationHelper.h:  } else if (std::is_same<T, at::Half>::value || std::is_same<T, at::BFloat16>::value) {
ATen/DLConvertor.cpp:    case ScalarType::Half:
ATen/DLConvertor.cpp:    case ScalarType::ComplexHalf:
ATen/DLConvertor.cpp:          stype = ScalarType::Half;
ATen/DLConvertor.cpp:          stype = ScalarType::ComplexHalf;
ATen/templates/RegisterDispatchKey.cpp:#include <c10/util/Half.h>
ATen/OpMathType.h:#include <c10/util/Half.h>
ATen/OpMathType.h:struct OpMathType<at::Half> {
ATen/OpMathType.h:struct OpMathType<c10::complex<Half>> {
ATen/cuda/NumericLimits.cuh:struct numeric_limits<at::Half> {
ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half lowest() { return at::Half(0xFBFF, at::Half::from_bits()); }
ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half max() { return at::Half(0x7BFF, at::Half::from_bits()); }
ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half lower_bound() { return at::Half(0xFC00, at::Half::from_bits()); }
ATen/cuda/NumericLimits.cuh:  static inline __host__ __device__ at::Half upper_bound() { return at::Half(0x7C00, at::Half::from_bits()); }
ATen/cuda/Atomic.cuh:#include <c10/util/Half.h>
ATen/cuda/Atomic.cuh:struct AtomicFPOp<at::Half> {
ATen/cuda/Atomic.cuh:  inline __device__ at::Half operator() (at::Half *address, at::Half val, const func_t& func) {
ATen/cuda/Atomic.cuh:    at::Half hsum;
ATen/cuda/Atomic.cuh:static inline  __device__ at::Half gpuAtomicAdd(at::Half *address, at::Half val) {
ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
ATen/cuda/Atomic.cuh:                                [](at::Half hsum, at::Half val) {
ATen/cuda/Atomic.cuh:static inline __device__ at::Half atomicAdd(at::Half *address, at::Half val) {
ATen/cuda/Atomic.cuh:static inline __device__ void gpuAtomicAddNoReturn(at::Half *address, at::Half val) { gpuAtomicAdd(address, val); }
ATen/cuda/Atomic.cuh:inline __device__ at::Half gpuAtomicMul(at::Half * address, at::Half val) {
ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
ATen/cuda/Atomic.cuh:                                [](at::Half bsum, at::Half val) {
ATen/cuda/Atomic.cuh:inline __device__ at::Half gpuAtomicMax(at::Half * address, at::Half val) {
ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
ATen/cuda/Atomic.cuh:                                [](at::Half bsum, at::Half val) {
ATen/cuda/Atomic.cuh:inline __device__ at::Half gpuAtomicMin(at::Half * address, at::Half val) {
ATen/cuda/Atomic.cuh:  return AtomicFPOp<at::Half>()(address, val,
ATen/cuda/Atomic.cuh:                                [](at::Half bsum, at::Half val) {
ATen/cuda/CUDADataType.h:template<> inline cudaDataType getCudaDataType<at::Half>() {
ATen/cuda/CUDADataType.h:template<> inline cudaDataType getCudaDataType<c10::complex<c10::Half>>() {
ATen/cuda/CUDADataType.h:    case c10::ScalarType::Half:
ATen/cuda/CUDADataType.h:    case c10::ScalarType::ComplexHalf:
ATen/cuda/CUDABlas.h:  where Dtype is double, float, at::Half or at::BFloat16 (ROCm, NOT for dot).
ATen/cuda/CUDABlas.h:void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half));
ATen/cuda/CUDABlas.h:void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half));
ATen/cuda/CUDABlas.h:void gemv<at::Half>(CUDABLAS_GEMV_ARGTYPES(at::Half));
ATen/cuda/CUDABlas.h:void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half));
ATen/cuda/CUDABlas.cpp:void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half)) {
ATen/cuda/CUDABlas.cpp:  BGEMM_CHECK_ARGVALUES(at::Half);
ATen/cuda/CUDABlas.cpp:      at::cuda::blas::gemm<at::Half>(
ATen/cuda/CUDABlas.cpp:void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half)) {
ATen/cuda/CUDABlas.cpp:  GEMM_CHECK_ARGVALUES(at::Half);
ATen/cuda/CUDABlas.cpp:  } else if (std::is_same<Dtype, at::Half>::value) {
ATen/cuda/CUDABlas.cpp:    at::opmath_type<at::Half> alpha_val,
ATen/cuda/CUDABlas.cpp:    const at::Half* mat1_ptr,
ATen/cuda/CUDABlas.cpp:    const at::Half* mat2_ptr,
ATen/cuda/CUDABlas.cpp:    const at::Half* bias,
ATen/cuda/CUDABlas.cpp:    at::Half* result_ptr,
ATen/cuda/CUDABlas.cpp:void gemv<at::Half>(CUDABLAS_GEMV_ARGTYPES(at::Half)) {
ATen/cuda/CUDABlas.cpp:  // There's no such thing as "cublasHalfgemv", so here we hack gemv with a gemm.
ATen/cuda/CUDABlas.cpp:  // have to swap args based on whether it's calling blas::gemv<at::Half> or <float>.
ATen/cuda/CUDABlas.cpp:  gemm<at::Half>(
ATen/cuda/CUDABlas.cpp:void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half)) {
ATen/cuda/DeviceUtils.cuh:#include <c10/util/Half.h>
ATen/cuda/DeviceUtils.cuh:__device__ __forceinline__ c10::Half WARP_SHFL_DOWN<c10::Half>(c10::Half value, unsigned int delta, int width, unsigned int mask)
ATen/cuda/DeviceUtils.cuh:  return c10::Half(WARP_SHFL_DOWN<unsigned short>(value.x, delta, width, mask), c10::Half::from_bits_t{});
ATen/cuda/cub-RadixSortKeys.cu:AT_FORALL_SCALAR_TYPES_AND2(Bool, Half, AT_INSTATIATE_CUB_TEMPLATES)
ATen/cuda/CUDATensorMethods.cuh:#include <c10/util/Half.h>
ATen/cuda/CUDATensorMethods.cuh:  return reinterpret_cast<__half*>(data<Half>());
ATen/cuda/llvm_complex.cpp:struct alignas(2) complex<at::Half> {
ATen/cuda/llvm_complex.cpp:  at::Half real_;
ATen/cuda/llvm_complex.cpp:  at::Half imag_;
ATen/cuda/llvm_complex.cpp:  // NOTE: computation of `complex<Half>` will occur in `complex<float>`
ATen/cuda/llvm_complex.cpp:  at::Half real() const {return real_;}
ATen/cuda/llvm_complex.cpp:  at::Half imag() const {return imag_;}
ATen/cuda/cub.cuh:struct cuda_type<c10::Half> {
ATen/cuda/cub-RadixSortPairs.cu:AT_FORALL_SCALAR_TYPES_AND2(Bool, Half, AT_INSTANTIATE_SORT_PAIRS_8)
ATen/cpu/vec/vec_base.h:// at::Half and at::BFloat16 should be treated as floating point
ATen/cpu/vec/vec_base.h:      std::is_same<T, at::Half>::value ||
ATen/Dispatch.h:#include <c10/util/Half.h>
ATen/Dispatch.h:    "use AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, ...) instead")
ATen/Dispatch.h:    "use AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(at::ScalarType::Half, ...) "
ATen/Dispatch.h:// but NOT booleans (bool), half-precision floats (Half) or
ATen/Dispatch.h:  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)   
ATen/Dispatch.h:      AT_DISPATCH_CASE_ALL_TYPES_AND(at::ScalarType::Half, __VA_ARGS__))
ATen/ScalarOps.cpp:      kComplexHalf, kHalf, kBool, kBFloat16, self.scalar_type(), "fill_out", [&]() {
ATen/miopen/Descriptors.h:    case miopenHalf: return 2;
ATen/miopen/Descriptors.h:    if (dataType == miopenHalf || dataType == miopenFloat || dataType == miopenBFloat16) {
ATen/miopen/Types.cpp:  } else if (tensor.scalar_type() == at::kHalf) {
ATen/miopen/Types.cpp:    return miopenHalf;
ATen/miopen/Descriptors.cpp:  } else if (scalar_type == at::kHalf) {
ATen/miopen/Descriptors.cpp:    return miopenHalf;
ATen/miopen/Descriptors.cpp:    case miopenHalf:
ATen/miopen/Descriptors.cpp:      return "miopenHalf";
ATen/native/GatedLinearUnit.cpp:  Tensor firstHalf = self.narrow(wrap_dim, 0, selfSize);
ATen/native/GatedLinearUnit.cpp:  Tensor secondHalf = self.narrow(wrap_dim, selfSize, selfSize);
ATen/native/GatedLinearUnit.cpp:  build_borrowing_binary_op(maybe_get_output(), firstHalf, secondHalf);
ATen/native/GatedLinearUnit.cpp:  Tensor firstHalf = input.narrow(wrap_dim, 0, inputSize);
ATen/native/GatedLinearUnit.cpp:  Tensor secondHalf = input.narrow(wrap_dim, inputSize, inputSize);
ATen/native/GatedLinearUnit.cpp:  Tensor gradInputfirstHalf = grad_input.narrow(wrap_dim, 0, inputSize);
ATen/native/GatedLinearUnit.cpp:  Tensor gradInputsecondHalf = grad_input.narrow(wrap_dim, inputSize, inputSize);
ATen/native/GatedLinearUnit.cpp:  at::sigmoid_out(gradInputfirstHalf, secondHalf);
ATen/native/GatedLinearUnit.cpp:    .add_output(gradInputsecondHalf)
ATen/native/GatedLinearUnit.cpp:    .add_input(gradInputfirstHalf)
ATen/native/GatedLinearUnit.cpp:    .add_input(firstHalf)
ATen/native/GatedLinearUnit.cpp:  gradInputfirstHalf.mul_(grad_output);
ATen/native/TriangularOps.cpp:      ScalarType::Half,
ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_cpu", [&] {
ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_cpu", [&] {
ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_backward_cpu", [&] {
ATen/native/ReflectionPad.cpp:        kHalf, kBFloat16, input.scalar_type(), "reflection_pad3d_backward_cpu", [&] {
ATen/native/RowwisePrune.cpp:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half,
ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "angle_cuda", [&]() {
ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "angle_cuda", [&]() {
ATen/native/cuda/UnaryComplexKernels.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_CASE_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, [&] {
ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_CASE(kComplexHalf, conj_chalf)
ATen/native/cuda/EmbeddingBag.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, weight.scalar_type(), "embedding_bag_cuda", [&] {
ATen/native/cuda/TensorTransformations.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/TensorTransformations.cu:      at::ScalarType::ComplexHalf,
ATen/native/cuda/EmbeddingBackwardKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/EmbeddingBackwardKernel.cu:        if(grad.dtype() == at::kHalf || grad.dtype() == at::kBFloat16) {
ATen/native/cuda/UnaryGeometricSinhKernel.cu:        kComplexHalf, common_dtype, "sinh_name", [&]() {
ATen/native/cuda/UnaryGeometricSinhKernel.cu:        kComplexHalf, common_dtype, "sinh_name", [&]() {
ATen/native/cuda/UnaryGeometricSinhKernel.cu:        ScalarType::Half,
ATen/native/cuda/ReduceLogicKernel.cu:      kHalf, kBFloat16, kBool, iter.common_dtype(), "and_cuda", [&]() {
ATen/native/cuda/ReduceLogicKernel.cu:      kHalf, kBFloat16, kBool, iter.common_dtype(), "or_cuda", [&]() {
ATen/native/cuda/Shape.cu:          kComplexHalf, kHalf, kBool, kBFloat16,
ATen/native/cuda/Shape.cu:          kComplexHalf, kHalf, kBool, kBFloat16,
ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op_cuda", [&]() {
ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op__cuda", [&]() {
ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op__cuda", [&]() {
ATen/native/cuda/ForeachPointwiseOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, input[0].scalar_type(), "foreach_pointwise_op_cuda", [&]() {
ATen/native/cuda/UnaryGeometricCoshKernel.cu:        kComplexHalf, common_dtype, "cosh_name", [&]() {
ATen/native/cuda/UnaryGeometricCoshKernel.cu:        kComplexHalf, common_dtype, "cosh_name", [&]() {
ATen/native/cuda/UnaryGeometricCoshKernel.cu:        ScalarType::Half,
ATen/native/cuda/ActivationLeakyReluKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationLeakyReluKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/PowKernel.cu:/* complex<Half> support impl */
ATen/native/cuda/PowKernel.cu:void pow_scalar_tensor_impl(TensorIteratorBase& iter, c10::complex<at::Half> base) {
ATen/native/cuda/PowKernel.cu:  using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/PowKernel.cu:/* complex<Half> support impl */
ATen/native/cuda/PowKernel.cu:/* complex<Half> support impl */
ATen/native/cuda/PowKernel.cu:  using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/PowKernel.cu:  if (common_dtype == kComplexHalf) {
ATen/native/cuda/PowKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/PowKernel.cu:        kHalf, kBFloat16, iter.common_dtype(), "pow_cuda", [&] {
ATen/native/cuda/PowKernel.cu:    if (iter.common_dtype() == kComplexHalf) {
ATen/native/cuda/PowKernel.cu:      using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/PowKernel.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "pow_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(0), "logical_not_cuda", [&]() {});
ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(1), "logical_not_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "neg_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "neg_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:  AT_DISPATCH_ALL_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, dtype, "neg_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_ALL_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.dtype(), "sign_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, ScalarType::Half, iter.input_dtype(), "signbit_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sgn_cuda", [&]() {
ATen/native/cuda/UnarySignKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sgn_cuda", [&]() {
ATen/native/cuda/ReduceMaxValuesKernel.cu:      kBFloat16, kHalf, kBool, iter.dtype(), "max_values_cuda", [&]() {
ATen/native/cuda/ReduceMaxValuesKernel.cu:      kBFloat16, kHalf, kBool, iter.input_dtype(), "max_cuda", [&]() {
ATen/native/cuda/ReduceMaxValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.input_dtype(), "max_all_cuda", [&] {
ATen/native/cuda/UnfoldBackwardKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/DilatedMaxPool2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/DilatedMaxPool2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/BinaryRemainderKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "remainder_cuda", [&]() {
ATen/native/cuda/BinaryRemainderKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "fmod_cuda", [&]() {
ATen/native/cuda/ReduceArgMaxKernel.cu:  if (iter.dtype(1) == kHalf) {
ATen/native/cuda/ReduceArgMaxKernel.cu:    argmax_kernel_cuda_impl<at::Half, float>(iter);
ATen/native/cuda/NLLLoss2d.cu:        at::ScalarType::Half,
ATen/native/cuda/NLLLoss2d.cu:      at::ScalarType::Half,
ATen/native/cuda/NLLLoss2d.cu:        at::ScalarType::Half,
ATen/native/cuda/NLLLoss2d.cu:        at::ScalarType::Half,
ATen/native/cuda/Col2Im.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/SoftMax.cu:    TORCH_CHECK(input_.scalar_type() == ScalarType::Half, "conversion is supported for Half type only");
ATen/native/cuda/SoftMax.cu:  static_assert(std::is_same<acc_type<at::Half, true>, float>::value, "accscalar_t for half should be float");
ATen/native/cuda/SoftMax.cu:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "host_softmax", [&] {
ATen/native/cuda/SoftMax.cu:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "host_softmax", [&] {
ATen/native/cuda/SoftMax.cu:  static_assert(std::is_same<acc_type<at::Half, true>, float>::value, "accscalar_t for half should be float");
ATen/native/cuda/SoftMax.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, gI.scalar_type(), "host_softmax_backward", [&] {
ATen/native/cuda/SoftMax.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, gI.scalar_type(), "host_softmax_backward", [&] {
ATen/native/cuda/SoftMax.cu:         input_dtype == ScalarType::Half),
ATen/native/cuda/SoftMax.cu:        "expected input and grad types to match, or input to be at::Half and grad to be at::Float");
ATen/native/cuda/SoftMax.cu:         input_dtype == ScalarType::Half),
ATen/native/cuda/SoftMax.cu:        "expected input and grad types to match, or input to be at::Half and grad to be at::Float");
ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
ATen/native/cuda/SoftMax.cu:      ScalarType::Half,
ATen/native/cuda/Nonzero.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
ATen/native/cuda/ActivationHardshrinkKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ReduceMinValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.dtype(), "min_values_cuda", [&]() {
ATen/native/cuda/ReduceMinValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.input_dtype(), "min_cuda", [&]() {
ATen/native/cuda/ReduceMinValuesKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.input_dtype(), "min_all_cuda", [&] {
ATen/native/cuda/UnaryGeometricTanhKernel.cu:        kComplexHalf, common_dtype, "tanh_name", [&]() {
ATen/native/cuda/UnaryGeometricTanhKernel.cu:        kComplexHalf, common_dtype, "tanh_name", [&]() {
ATen/native/cuda/UnaryGeometricTanhKernel.cu:        ScalarType::Half,
ATen/native/cuda/ForeachBinaryOpList.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpList.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpList.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpList.cu:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, tensors1[0].scalar_type(), "foreach_binary_op_list_cuda_", [&]() {
ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/ReflectionPad.cu:      kHalf, kBFloat16, input.scalar_type(), "reflection_pad1d_out_template", [&] {
ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/ReflectionPad.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnaryFractionKernels.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/DilatedMaxPool3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/DilatedMaxPool3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/SummaryOps.cu:  if (self.scalar_type() == ScalarType::Half) {
ATen/native/cuda/SummaryOps.cu:    AT_ERROR("HalfTensor is not supported");
ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/cuda/ReplicationPadding.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "max_elementwise_cuda", [&]() {
ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "min_elementwise_cuda", [&]() {
ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmax_cuda", [&]() {
ATen/native/cuda/MaxMinElementwiseKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmin_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpScalar.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpScalar.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpScalar.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpScalar.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpScalar.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpScalar.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalar_cuda", [&]() {
ATen/native/cuda/SortStable.cu:      kBool, kHalf, kBFloat16, self.scalar_type(), "sort", [&] {
ATen/native/cuda/CopysignKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "copysign_cuda", [&]() {
ATen/native/cuda/CuFFTPlanCache.h:    if (dtype == ScalarType::Half) {
ATen/native/cuda/CuFFTPlanCache.h:    } else if (dtype == ScalarType::Half) {
ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16, iter.dtype(), "index_cuda", [&] {
ATen/native/cuda/IndexKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, kComplexHalf,
ATen/native/cuda/IndexKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, kComplexHalf,
ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16, iter.dtype(), "index_put", [&] {
ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, iter.dtype(), "put_cuda", [&] {
ATen/native/cuda/IndexKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, iter.dtype(), "take_cuda", [&] {
ATen/native/cuda/IndexKernel.cu:      ScalarType::Half,
ATen/native/cuda/IndexKernel.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/Im2Col.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/LogcumsumexpKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/Randperm.cu:    AT_DISPATCH_ALL_TYPES_AND(kHalf, result.scalar_type(), "randperm_out_cuda", [&] {
ATen/native/cuda/Randperm.cu:    AT_DISPATCH_ALL_TYPES_AND(kHalf, result.scalar_type(), "randperm_out_cuda", [&] {
ATen/native/cuda/ActivationMishKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationMishKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/WeightNorm.cu:  // g.scalar_type() may be at::ScalarType::Double, Float, or Half.
ATen/native/cuda/WeightNorm.cu:  // If Half, stash norms as float.
ATen/native/cuda/WeightNorm.cu:  at::ScalarType AccType = g.scalar_type() == at::ScalarType::Half ?
ATen/native/cuda/ComplexKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.input_dtype(0), "complex_cuda", [&]() {
ATen/native/cuda/UnaryGeometricSinKernel.cu:        kComplexHalf, common_dtype, "sin_name", [&]() {
ATen/native/cuda/UnaryGeometricSinKernel.cu:        kComplexHalf, common_dtype, "sin_name", [&]() {
ATen/native/cuda/UnaryGeometricSinKernel.cu:        ScalarType::Half,
ATen/native/cuda/Math.cuh:  static_assert(!std::is_same<scalar_t, Half>() && !std::is_same<scalar_t, BFloat16>(), "don't instantiate with low precision type");
ATen/native/cuda/Math.cuh:  static_assert(!std::is_same<scalar_t, Half>() && !std::is_same<scalar_t, BFloat16>(), "don't instantiate with low precision type");
ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, it.dtype(), "GeluCUDAKernelImpl", [&]() {
ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, it.dtype(), "GeluCUDAKernelImpl", [&]() {
ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/ActivationGeluKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/NaiveConvolutionTranspose2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/NaiveConvolutionTranspose2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/NaiveConvolutionTranspose2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/PointwiseOpsKernel.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, "addcmul_cuda", [&]() {
ATen/native/cuda/PointwiseOpsKernel.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, "addcdiv_cuda", [&]() {
ATen/native/cuda/PointwiseOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND(kHalf, iter.dtype(), "smooth_l1_backward_cuda", [&iter, &norm, beta] {
ATen/native/cuda/PointwiseOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "huber_backward_cuda", [&iter, &norm, delta] {
ATen/native/cuda/PointwiseOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "mse_backward_cuda", [&]() {
ATen/native/cuda/SpectralOps.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "_fft_fill_with_conjugate_symmetry", [&] {
ATen/native/cuda/ForeachReduceOp.cu:      kHalf, kBFloat16, tensor_lists[0][0].scalar_type(), "foreach_tensor_norm_cuda", [&]() {
ATen/native/cuda/ForeachReduceOp.cu:      kHalf, kBFloat16, tensor_lists[0][0].scalar_type(), "foreach_tensor_norm_cuda", [&]() {
ATen/native/cuda/ReduceSumProdKernel.cu:// jiterated specialization for `complex<Half>`
ATen/native/cuda/ReduceSumProdKernel.cu:struct sum_functor<c10::complex<at::Half>> {
ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/ReduceSumProdKernel.cu:// jiterated specialization for `complex<Half>`
ATen/native/cuda/ReduceSumProdKernel.cu:struct prod_functor<c10::complex<at::Half>> {
ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/ReduceSumProdKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/ReduceSumProdKernel.cu:// for handling Half-Precision floating types.
ATen/native/cuda/ReduceSumProdKernel.cu://       except for `at::Half` and `at::BFloat16`.
ATen/native/cuda/ReduceSumProdKernel.cu:  if (iter.dtype() == kHalf) {
ATen/native/cuda/ReduceSumProdKernel.cu:    return OpFunctor<at::Half, float>{}(iter);
ATen/native/cuda/ReduceSumProdKernel.cu:  } else if (iter.dtype(1) == kHalf && iter.dtype() == kFloat) {
ATen/native/cuda/ReduceSumProdKernel.cu:    return OpFunctor<at::Half, float, float>{}(iter);
ATen/native/cuda/ReduceSumProdKernel.cu:        kBool, kComplexHalf, iter.dtype(), "sum_cuda", [&]() {
ATen/native/cuda/ReduceSumProdKernel.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kComplexHalf, kBool, iter.dtype(), "prod_cuda", [&]() {
ATen/native/cuda/ActivationSoftplusKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationSoftplusKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/MultinomialKernel.cu:inline __device__ bool _isinf(c10::Half x) {
ATen/native/cuda/Blas.cpp:           scalar_type == at::ScalarType::Half ||
ATen/native/cuda/Blas.cpp:           (scalar_type != at::ScalarType::Half &&
ATen/native/cuda/Blas.cpp:           (scalar_type != at::ScalarType::Half &&
ATen/native/cuda/Blas.cpp:        at::ScalarType::Half,
ATen/native/cuda/Blas.cpp:        at::ScalarType::Half,
ATen/native/cuda/Blas.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "baddbmm_cuda", [&] {
ATen/native/cuda/Blas.cpp:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/Blas.cpp:      AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, mat.scalar_type(), "addmv_impl_cuda", [&] {
ATen/native/cuda/ActivationThresholdKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/LogAddExpKernel.cu:      ScalarType::BFloat16, ScalarType::Half,
ATen/native/cuda/UnaryGeometricAcosKernel.cu:        kComplexHalf, common_dtype, "acos_name", [&]() {
ATen/native/cuda/UnaryGeometricAcosKernel.cu:        kComplexHalf, common_dtype, "acos_name", [&]() {
ATen/native/cuda/UnaryGeometricAcosKernel.cu:        ScalarType::Half,
ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, ret.scalar_type(), "poisson_cuda", [&] {
ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, ret.scalar_type(), "gamma_cuda", [&] {
ATen/native/cuda/Distributions.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "_standard_gamma_grad_cuda", [&] {
ATen/native/cuda/AveragePool3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/AveragePool3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/AveragePool3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "huber_cuda", [&iter, delta] {
ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "mse_cuda", [&]() {
ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "xlogy_cuda", [&]() {
ATen/native/cuda/BinaryMiscOpsKernels.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "xlog1py_cuda", [&]() {
ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, self.scalar_type(),
ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, self.scalar_type(),
ATen/native/cuda/Normalization.cu:    return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_out.scalar_type(),
ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_out.scalar_type(),
ATen/native/cuda/Normalization.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_out.scalar_type(),
ATen/native/cuda/Normalization.cu:        kHalf, kBFloat16, self.scalar_type(), "batch_norm_stats_cuda", [&] {
ATen/native/cuda/Normalization.cu:          kHalf, kBFloat16, self.scalar_type(), "batch_norm_stats_cuda", [&] {
ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, running_mean.scalar_type(),
ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, running_mean.scalar_type(),
ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, running_var.scalar_type(),
ATen/native/cuda/Normalization.cu:    return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/Normalization.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/Normalization.cu:  return AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, scalar_type, "batch_norm_update_stats_cuda", [&] {
ATen/native/cuda/Normalization.cu:  return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_output.scalar_type(), "batch_norm_backward_reduce", [&] {
ATen/native/cuda/Normalization.cu:  return AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "batch_norm_backward_elemt", [&] {
ATen/native/cuda/Normalization.cu:    bool is_half_float = std::is_same<scalar_t, at::Half>::value && mean_st == at::kFloat;
ATen/native/cuda/UnaryGeometricCosKernel.cu:        kComplexHalf, common_dtype, "cos_name", [&]() {
ATen/native/cuda/UnaryGeometricCosKernel.cu:        kComplexHalf, common_dtype, "cos_name", [&]() {
ATen/native/cuda/UnaryGeometricCosKernel.cu:        ScalarType::Half,
ATen/native/cuda/ForeachTernaryOp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_ternary_cuda",
ATen/native/cuda/ForeachTernaryOp.cu:        at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_ternary_cuda_",
ATen/native/cuda/ForeachTernaryOp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_scalar_cuda",
ATen/native/cuda/ForeachTernaryOp.cu:        at::ScalarType::Half, at::ScalarType::BFloat16, tensors1[0].scalar_type(), "foreach_tensor_lerp_scalar_cuda_",
ATen/native/cuda/Reduce.cuh:  // at::Half/at::ComplexHalf overflows easily as it's range is very small.
ATen/native/cuda/Reduce.cuh:  // So when scalar_t and out_scalar_t are at::Half/at::ComplexHalf, we
ATen/native/cuda/Reduce.cuh:      (std::is_same<at::Half, scalar_t>::value &&
ATen/native/cuda/Reduce.cuh:       std::is_same<at::Half, out_scalar_t>::value) ||
ATen/native/cuda/Reduce.cuh:      (std::is_same<c10::complex<Half>, scalar_t>::value &&
ATen/native/cuda/Reduce.cuh:       std::is_same<c10::complex<Half>, out_scalar_t>::value);
ATen/native/cuda/Reduce.cuh:  // at::Half/at::ComplexHalf overflows easily as it's range is very small.
ATen/native/cuda/Reduce.cuh:  // So when scalar_t and out_scalar_t are at::Half/at::ComplexHalf, we
ATen/native/cuda/Reduce.cuh:      (std::is_same<at::Half, scalar_t>::value &&
ATen/native/cuda/Reduce.cuh:       std::is_same<at::Half, out_scalar_t>::value) ||
ATen/native/cuda/Reduce.cuh:      (std::is_same<c10::complex<Half>, scalar_t>::value &&
ATen/native/cuda/Reduce.cuh:       std::is_same<c10::complex<Half>, out_scalar_t>::value);
ATen/native/cuda/SortingRadixSelect.cuh:struct TopKTypeConfig<at::Half> {
ATen/native/cuda/SortingRadixSelect.cuh:  static inline __device__ RadixType convert(at::Half v) {
ATen/native/cuda/SortingRadixSelect.cuh:  static inline __device__ at::Half deconvert(RadixType v) {
ATen/native/cuda/SortingRadixSelect.cuh:    return static_cast<at::Half>(0);
ATen/native/cuda/UnaryGeometricAtanKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "atan_name", [&]() {
ATen/native/cuda/UnaryGeometricAtanKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "atan_name", [&]() {
ATen/native/cuda/UnaryGeometricAtanKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/KernelUtils.cuh:    typename std::enable_if<std::is_same<c10::Half, scalar_t>::value>::type* =
ATen/native/cuda/KernelUtils.cuh:      reinterpret_cast<at::Half*>(tensor) + index,
ATen/native/cuda/KernelUtils.cuh:      static_cast<at::Half>(value));
ATen/native/cuda/KernelUtils.cuh:    typename std::enable_if<!std::is_same<c10::Half, scalar_t>::value && !std::is_same<c10::BFloat16, scalar_t>::value >::type* =
ATen/native/cuda/ReduceNormKernel.cu:  if (iter.dtype(0) == kHalf) {
ATen/native/cuda/ReduceNormKernel.cu:    return norm_kernel_cuda_impl<at::Half, float>(iter, ord);
ATen/native/cuda/ReduceNormKernel.cu:  } else if (iter.input_dtype() == kHalf && iter.dtype(0) == kFloat) {
ATen/native/cuda/ReduceNormKernel.cu:    return norm_kernel_cuda_impl<at::Half, float, float>(iter, ord);
ATen/native/cuda/UpSampleNearest3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte,input.scalar_type(), "upsample_nearest3d_out_frame", [&] {
ATen/native/cuda/UpSampleNearest3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest3d_backward_out_frame", [&] {
ATen/native/cuda/TriangularOps.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, at::ScalarType::Half, at::ScalarType::Bool,
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "exp2_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:        ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0e_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "i0e_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sigmoid_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sigmoid_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, common_dtype, "sigmoid_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnarySpecialOpsKernel.cu:        ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnarySpecialOpsKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/UnarySpecialOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "erf_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "erfc_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.dtype(), "kaiser_window_cuda", [&](){
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.dtype(), "kaiser_window_cuda", [&](){
ATen/native/cuda/UnarySpecialOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "entr_cuda", [&]() {
ATen/native/cuda/UnarySpecialOpsKernel.cu:        ScalarType::Half,
ATen/native/cuda/MaxUnpooling.cu:    AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
ATen/native/cuda/MaxUnpooling.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
ATen/native/cuda/MaxUnpooling.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
ATen/native/cuda/MaxUnpooling.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half,
ATen/native/cuda/fused_adam_amsgrad_impl.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, params[0].scalar_type(),
ATen/native/cuda/group_norm_kernel.cu:        (X.scalar_type() == kHalf || X.scalar_type() == kBFloat16)
ATen/native/cuda/group_norm_kernel.cu:      at::ScalarType::Half,
ATen/native/cuda/group_norm_kernel.cu:        (X.scalar_type() == kHalf || X.scalar_type() == kBFloat16)
ATen/native/cuda/group_norm_kernel.cu:      (X.scalar_type() == kHalf || X.scalar_type() == kBFloat16)
ATen/native/cuda/group_norm_kernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationPreluKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "prelu_cuda", [&] {
ATen/native/cuda/ActivationPreluKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "prelu_backward_cuda", [&] {
ATen/native/cuda/Copy.cu:        kHalf, kBool, kBFloat16, kComplexHalf, dtype, "copy_", [&] {
ATen/native/cuda/ReduceMomentKernel.cu:  if (input_dtype == kHalf && iter.dtype() == kFloat) {
ATen/native/cuda/ReduceMomentKernel.cu:    std_var_kernel_impl<at::Half, float>(iter, correction, take_sqrt);
ATen/native/cuda/ReduceMomentKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/ReduceMomentKernel.cu:  if (iter.dtype() == kHalf) {
ATen/native/cuda/ReduceMomentKernel.cu:    mean_kernel_impl<at::Half, float>(iter);
ATen/native/cuda/ReduceMomentKernel.cu:  } else if (iter.dtype(1) == kHalf && iter.dtype() == kFloat) {
ATen/native/cuda/ReduceMomentKernel.cu:    mean_kernel_impl<at::Half, float, float>(iter);
ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBFloat16, kBool, iter.dtype(), "where_cuda", [&] {
ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isposinf_cuda", [&]() {
ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isneginf_cuda", [&]() {
ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "clamp_cuda", [&] {
ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.common_dtype(), "clamp_scalar_cuda", [&] {
ATen/native/cuda/TensorCompare.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, self.scalar_type(), "_assert_async_cuda", [&] {
ATen/native/cuda/Pow.cuh:// pow for at::Half
ATen/native/cuda/Pow.cuh:static inline __host__ __device__ at::Half pow_(at::Half base, at::Half exp) {
ATen/native/cuda/Pow.cuh:  return static_cast<at::Half>(std::pow(static_cast<float>(base), static_cast<float>(exp)));
ATen/native/cuda/ActivationEluKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationEluKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/ScatterGatherKernel.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/jit_utils.cpp:  _(at::Half, Half) /* 5 */                                  \
ATen/native/cuda/jit_utils.cpp:  _(std::complex<at::Half>, ComplexHalf) /* 8 */        \
ATen/native/cuda/jit_utils.cpp:  _(at::Half, Half)                                                \
ATen/native/cuda/jit_utils.cpp:  _(std::complex<at::Half>, ComplexHalf)                           \
ATen/native/cuda/jit_utils.cpp:struct alignas(2) Half {
ATen/native/cuda/jit_utils.cpp:  Half() = default;
ATen/native/cuda/jit_utils.cpp:  inline __host__ __device__ Half(float value){
ATen/native/cuda/jit_utils.cpp:  struct static_cast_with_inter_type<std::complex<at::Half>, at::BFloat16> {
ATen/native/cuda/jit_utils.cpp:    static inline std::complex<at::Half> apply(at::BFloat16 src) {
ATen/native/cuda/jit_utils.cpp:      return static_cast<std::complex<at::Half>>(float{src});
ATen/native/cuda/jit_utils.cpp:  struct static_cast_with_inter_type<std::complex<at::Half>, at::Half> {
ATen/native/cuda/jit_utils.cpp:    static inline std::complex<at::Half> apply(at::Half src) {
ATen/native/cuda/jit_utils.cpp:      return static_cast<std::complex<at::Half>>(float{src});
ATen/native/cuda/jit_utils.cpp:      std::complex<at::Half>,
ATen/native/cuda/jit_utils.cpp:    static inline std::complex<at::Half> apply(std::complex<double> src) {
ATen/native/cuda/jit_utils.cpp:      return static_cast<std::complex<at::Half>>(static_cast<std::complex<float>>(src));
ATen/native/cuda/jit_utils.cpp:  if (f_inputs_type == "at::Half" || result_type == "at::Half" ||
ATen/native/cuda/jit_utils.cpp:      f_inputs_type == "std::complex<at::Half>" ||
ATen/native/cuda/jit_utils.cpp:      result_type == "std::complex<at::Half>" || dynamic_casting) {
ATen/native/cuda/jit_utils.cpp:    // complex<Half> depends on complex<T> and Half dtypes.
ATen/native/cuda/jit_utils.cpp:      f_inputs_type == "std::complex<at::Half>" || result_type == "std::complex<at::Half>") {
ATen/native/cuda/jit_utils.cpp:    // complex<Half> depends on complex<T> and Half dtypes.
ATen/native/cuda/jit_utils.cpp:  if (f_inputs_type == "std::complex<at::Half>" ||
ATen/native/cuda/jit_utils.cpp:      result_type == "std::complex<at::Half>" || dynamic_casting) {
ATen/native/cuda/jit_utils.cpp:    // include complex<at::Half>
ATen/native/cuda/jit_utils.cpp:      if (f_inputs_type == "at::Half" || f_inputs_type == "std::complex<at::Half>") {
ATen/native/cuda/jit_utils.cpp:        // complex<Half> depends on complex<T> and Half dtypes.
ATen/native/cuda/jit_utils.cpp:          f_inputs_type == "std::complex<at::Half>" ) {
ATen/native/cuda/jit_utils.cpp:        // complex<Half> depends on complex<T> and Half dtypes.
ATen/native/cuda/jit_utils.cpp:      if (f_inputs_type == "std::complex<at::Half>") {
ATen/native/cuda/CumprodKernel.cu:      ScalarType::Half, ScalarType::BFloat16, self.scalar_type(), "cumprod_cuda", [&]() {
ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "log_cuda", [&]() {
ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, iter.common_dtype(), "log_cuda", [&]() {
ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log_cuda", [&]() {
ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log10_cuda", [&]() {
ATen/native/cuda/UnaryLogKernels.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log1p_cuda", [&]() {
ATen/native/cuda/UnaryLogKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, iter.common_dtype(), "log2_cuda", [&]() {
ATen/native/cuda/NaiveDilatedConvolution.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/CUDAScalar.cu:    kComplexHalf, kHalf, kBool, kBFloat16, self.scalar_type(), "_local_scalar_dense_cuda", [&] {
ATen/native/cuda/BinaryGeometricKernels.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/BinaryGeometricKernels.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/SegmentReduce.cu:            kHalf,
ATen/native/cuda/SegmentReduce.cu:            at::ScalarType::Half,
ATen/native/cuda/CumsumKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/Loss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "binary_cross_entropy_backward_out_cuda", [&]() {
ATen/native/cuda/Loss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "binary_cross_entropy_out_cuda", [&]() {
ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
ATen/native/cuda/Loss.cu:        at::ScalarType::Half,
ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cuda/CompareEQKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBFloat16, kBool,
ATen/native/cuda/DepthwiseConv2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/DepthwiseConv2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_output.scalar_type(),
ATen/native/cuda/DepthwiseConv2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, grad_output.scalar_type(),
ATen/native/cuda/AbsKernel.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "abs_cuda", [&]() {
ATen/native/cuda/AbsKernel.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "abs_cuda", [&]() {
ATen/native/cuda/AbsKernel.cu:        ScalarType::Half,
ATen/native/cuda/FillKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kHalf, kBFloat16, iter.dtype(), "fill_cuda", [&]() {
ATen/native/cuda/UniqueCub.cu:INSTANTIATE_UNIQUE_CUDA_TEMPLATE(at::Half);
ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique", [&] {
ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique", [&] {
ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique_dim", [&] {
ATen/native/cuda/Unique.cu:  return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique_dim", [&] {
ATen/native/cuda/Unique.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kBool, kHalf, self.scalar_type(), "unique", [&] {
ATen/native/cuda/ActivationSoftshrinkKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationSoftshrinkKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/AdaptiveAveragePooling.cu:        AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/jit_utils.h:template <> inline std::string typeName<c10::complex<at::Half>>(){
ATen/native/cuda/jit_utils.h:    return "std::complex<at::Half>";
ATen/native/cuda/jit_utils.h:template <> inline std::string typeName<at::Half>(){
ATen/native/cuda/jit_utils.h:    return "at::Half";
ATen/native/cuda/UnaryGeometricAsinhKernel.cu:        kComplexHalf, common_dtype, "asinh_name", [&]() {
ATen/native/cuda/UnaryGeometricAsinhKernel.cu:        kComplexHalf, common_dtype, "asinh_name", [&]() {
ATen/native/cuda/UnaryGeometricAsinhKernel.cu:        ScalarType::Half,
ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "exp_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "exp_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, common_dtype, "exp_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:      ScalarType::BFloat16, ScalarType::Half,
ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "rsqrt_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "rsqrt_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:      ScalarType::BFloat16, ScalarType::Half,
ATen/native/cuda/UnaryOpsKernel.cu:          // In CUDA, ::rsqrt is overloaded for float and at::Half here is implicitly cast to float.
ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sqrt_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:      AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "sqrt_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, common_dtype, "sqrt_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "clamp_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "clamp_min_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "clamp_max_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "nan_to_num_cuda", [&]() {
ATen/native/cuda/UnaryOpsKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND(ScalarType::Half,
ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, input.scalar_type(), "upsample_nearest2d_nhwc_out_frame", [&] {
ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, input.scalar_type(), "upsample_nearest2d_out_frame", [&] {
ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest2d_backward_nhwc_out_frame", [&] {
ATen/native/cuda/UpSampleNearest2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest2d_backward_out_frame", [&] {
ATen/native/cuda/ActivationGluKernel.cu:      kHalf, kBFloat16, iter.dtype(), "glu_cuda", [&]() {
ATen/native/cuda/ActivationGluKernel.cu:      kHalf, kBFloat16, iter.dtype(), "glu_cuda", [&]() {
ATen/native/cuda/ActivationGluKernel.cu:      kHalf, kBFloat16, iter.common_dtype(), "glu_backward_cuda", [&] {
ATen/native/cuda/BinaryDivFloorKernel.cu:        kHalf, kBFloat16, dtype, "div_floor_cuda", [&]() {
ATen/native/cuda/BinaryDivFloorKernel.cu:        kHalf, kBFloat16, dtype, "div_floor_cuda", [&]() {
ATen/native/cuda/ActivationHardsigmoidKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationHardsigmoidKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationSiluKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/ActivationSiluKernel.cu:      at::ScalarType::Half,
ATen/native/cuda/Embedding.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/Embedding.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "embedding_renorm_cuda_", [&] {
ATen/native/cuda/RreluWithNoise.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/layer_norm_kernel.cu:  if ((std::is_same<T, float>::value || std::is_same<T, at::Half>::value || std::is_same<T, at::BFloat16>::value) &&
ATen/native/cuda/layer_norm_kernel.cu:      at::ScalarType::Half,
ATen/native/cuda/layer_norm_kernel.cu:      at::ScalarType::Half,
ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu:        ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, res_values.scalar_type(),
ATen/native/cuda/NaiveConvolutionTranspose3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/NaiveConvolutionTranspose3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/NaiveConvolutionTranspose3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/TensorTopK.cu:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "topk_out_cuda", [&] { \
ATen/native/cuda/Dropout.cu:      at::ScalarType::Half,
ATen/native/cuda/Dropout.cu:   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, ret.scalar_type(), "masked_scale", [&] {
ATen/native/cuda/ReduceArgMinKernel.cu:  if (iter.dtype(1) == kHalf) {
ATen/native/cuda/ReduceArgMinKernel.cu:    argmin_kernel_cuda_impl<at::Half, float>(iter);
ATen/native/cuda/ForeachBinaryOpScalarList.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpScalarList.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpScalarList.cu:    return AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda", [&]() {
ATen/native/cuda/ForeachBinaryOpScalarList.cu:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpScalarList.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
ATen/native/cuda/ForeachBinaryOpScalarList.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, tensors[0].scalar_type(), "foreach_binary_op_scalarlist_cuda_", [&]() {
ATen/native/cuda/MultiLabelMarginCriterion.cu:        at::ScalarType::Half,
ATen/native/cuda/MultiLabelMarginCriterion.cu:          at::ScalarType::Half,
ATen/native/cuda/MultiLabelMarginCriterion.cu:          at::ScalarType::Half,
ATen/native/cuda/MultiLabelMarginCriterion.cu:        at::ScalarType::Half,
ATen/native/cuda/MultiLabelMarginCriterion.cu:        at::ScalarType::Half,
ATen/native/cuda/CrossKernel.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(kHalf, iter.common_dtype(), "cross_cuda", [&] {
ATen/native/cuda/BinaryDivTruncKernel.cu:        kHalf, kBFloat16, dtype, "div_trunc_cuda", [&]() {
ATen/native/cuda/BinaryDivTruncKernel.cu:        kHalf, kBFloat16, dtype, "div_trunc_cuda", [&]() {
ATen/native/cuda/UnaryGeometricAtanhKernel.cu:        kComplexHalf, common_dtype, "atanh_name", [&]() {
ATen/native/cuda/UnaryGeometricAtanhKernel.cu:        kComplexHalf, common_dtype, "atanh_name", [&]() {
ATen/native/cuda/UnaryGeometricAtanhKernel.cu:        ScalarType::Half,
ATen/native/cuda/StepKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, kBFloat16, iter.dtype(), "heaviside_cuda", [&]() {
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sigmoid_backward_cuda", [&]() {
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "sigmoid_backward_cuda", [&]() {
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, dtype, "sigmoid_backward_cuda", [&]() {
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:      at::ScalarType::Half,
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "tanh_backward_complex_cuda", [&]() {
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "tanh_backward_complex_cuda", [&]() {
ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, dtype, "tanh_backward_cuda", [&]() {
ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf, kBFloat16, input.scalar_type(), "adaptive_max_pool2d_cuda", [&] {
ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf,
ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf,
ATen/native/cuda/AdaptiveMaxPooling2d.cu:        kHalf,
ATen/native/cuda/AdaptiveAveragePooling3d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/AdaptiveAveragePooling3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/AdaptiveAveragePooling3d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16,
ATen/native/cuda/TensorModeKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, self.scalar_type(), "cuda_mode", [&] {
ATen/native/cuda/TensorModeKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, self.scalar_type(), "cuda_mode", [&] {
ATen/native/cuda/fused_adam_impl.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, params[0].scalar_type(),
ATen/native/cuda/LinearAlgebra.cu:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf,
ATen/native/cuda/RangeFactories.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, r.scalar_type(), "linspace_cuda", [&]() {
ATen/native/cuda/RangeFactories.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, r.scalar_type(), "logspace_cuda", [&]() {
ATen/native/cuda/RangeFactories.cu:  AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, result.scalar_type(), "range_cuda", [&]() {
ATen/native/cuda/RangeFactories.cu:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, result.scalar_type(), "arange_cuda", [&]() {
ATen/native/cuda/UnaryGeometricTanKernel.cu:        kComplexHalf, common_dtype, "tan_name", [&]() {
ATen/native/cuda/UnaryGeometricTanKernel.cu:        kComplexHalf, common_dtype, "tan_name", [&]() {
ATen/native/cuda/UnaryGeometricTanKernel.cu:        ScalarType::Half,
ATen/native/cuda/ActivationLogSigmoidKernel.cu:      kHalf, kBFloat16, iter.common_dtype(), "log_sigmoid_forward_cuda", [&] {
ATen/native/cuda/ActivationLogSigmoidKernel.cu:      kHalf, kBFloat16, iter.common_dtype(), "log_sigmoid_backward_cuda", [&] {
ATen/native/cuda/CumminmaxKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/CumminmaxKernel.cu:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/ActivationHardtanhKernel.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/UnaryGeometricAsinKernel.cu:        kComplexHalf, common_dtype, "asin_name", [&]() {
ATen/native/cuda/UnaryGeometricAsinKernel.cu:        kComplexHalf, common_dtype, "asin_name", [&]() {
ATen/native/cuda/UnaryGeometricAsinKernel.cu:        kHalf, kBFloat16, common_dtype, "asin_cuda", [&]() {
ATen/native/cuda/MultiMarginLoss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "multi_margin_loss_cuda", [&] {
ATen/native/cuda/MultiMarginLoss.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/AveragePool2d.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/AveragePool2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/BinaryMulKernel.cu:  if (common_dtype == kComplexHalf) {
ATen/native/cuda/BinaryMulKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/BinaryMulKernel.cu:        kHalf, kBFloat16, kBool, iter.common_dtype(), "mul_cuda", [&]() {
ATen/native/cuda/Sorting.cu:      at::ScalarType::Half, self.scalar_type(), "kthvalue_cuda", [&] {
ATen/native/cuda/Sorting.cu:      at::ScalarType::Half, self.scalar_type(), "median_out_impl", [&] {
ATen/native/cuda/BinaryDivTrueKernel.cu:  if (iter.common_dtype() == kComplexHalf) {
ATen/native/cuda/BinaryDivTrueKernel.cu:    using scalar_t = c10::complex<at::Half>;
ATen/native/cuda/BinaryDivTrueKernel.cu:        kHalf, kBFloat16, common_dtype, "div_true_cuda", [&]() {
ATen/native/cuda/BinaryDivTrueKernel.cu:        kHalf, kBFloat16, common_dtype, "div_true_cuda", [&]() {
ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
ATen/native/cuda/Lerp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
ATen/native/cuda/Lerp.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, dtype, "lerp_cuda", [&] {
ATen/native/cuda/Lerp.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/AdaptiveMaxPooling3d.cu:      kHalf, kBFloat16, input_.scalar_type(), "adaptive_max_pool3d_cuda", [&] {
ATen/native/cuda/AdaptiveMaxPooling3d.cu:        kHalf,
ATen/native/cuda/AdaptiveMaxPooling3d.cu:        kHalf,
ATen/native/cuda/Normalization.cuh:  if (mean_.scalar_type() == at::ScalarType::Half || mean_.scalar_type() == at::ScalarType::BFloat16) {
ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_forward", [&] {
ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_forward", [&] {
ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_reduce", [&] {
ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_reduce", [&] {
ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_element", [&] {
ATen/native/cuda/Normalization.cuh:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), "batchnorm_backward_element", [&] {
ATen/native/cuda/Normalization.cuh:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "batchnorm_backward_element", [&] {
ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(ScalarType::Half,  tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(ScalarType::Half, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(ScalarType::Half, at::ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(ScalarType::Half, at::ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_TYPES_AND(ScalarType::Half,  tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    return AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,  tensors[0].scalar_type(), "foreach_unary_op_cuda", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_unary_op_cuda_", [&]() {
ATen/native/cuda/ForeachUnaryOp.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16, tensors[0].scalar_type(), "foreach_zero_cuda_", [&]() {
ATen/native/cuda/UpSampleNearest1d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, input.scalar_type(), "upsample_nearest1d_out_frame", [&] {
ATen/native/cuda/UpSampleNearest1d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::Byte, grad_output.scalar_type(), "upsample_nearest1d_backward_out_frame", [&] {
ATen/native/cuda/UnaryGeometricAcoshKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "acosh_name", [&]() {
ATen/native/cuda/UnaryGeometricAcoshKernel.cu:  AT_DISPATCH_COMPLEX_TYPES_AND(kComplexHalf, common_dtype, "acosh_name", [&]() {
ATen/native/cuda/UnaryGeometricAcoshKernel.cu:      ScalarType::Half, ScalarType::BFloat16,
ATen/native/cuda/DistributionTemplates.h:#include <c10/util/Half.h>
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "random_from_to_kernel_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, iter.dtype(), "random_kernel_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "normal_kernel_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "uniform_kernel_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "log_normal_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "geometric_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "exponential_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "cauchy_cuda", [&] {
ATen/native/cuda/DistributionTemplates.h:    at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, self.scalar_type(), "bernoulli_tensor_cuda_self_", [&] {
ATen/native/cuda/DistributionTemplates.h:    at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, iter.dtype(), "bernoulli_scalar_cuda_", [&] {
ATen/native/cuda/Indexing.cu:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16,
ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::ComplexHalf, result.scalar_type(), "index_add", [&] {
ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "index_add", [&] {
ATen/native/cuda/Indexing.cu:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, result.scalar_type(), "index_reduce", [&] {
ATen/native/cuda/Indexing.cu:    AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "index_reduce", [&] {
ATen/native/cuda/Indexing.cu:        at::ScalarType::ComplexHalf,
ATen/native/cuda/Indexing.cu:        at::ScalarType::Half,
ATen/native/cuda/Indexing.cu:      kBool, kHalf, kBFloat16, kComplexHalf, iter.common_dtype(), "masked_fill_", [&]() {
ATen/native/cuda/ReduceAMinMaxKernel.cu:      kBFloat16, kHalf, kBool, iter.input_dtype(), "aminmax_all_cuda", [&] {
ATen/native/cuda/ReduceAMinMaxKernel.cu:      kBFloat16, kHalf, kBool, iter.input_dtype(), "aminmax_cuda", [&]() {
ATen/native/cuda/Bucketization.cu:    AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, input.scalar_type(), "searchsorted_out_cuda", [&] {
ATen/native/cuda/Bucketization.cu:    AT_DISPATCH_ALL_TYPES_AND(at::ScalarType::Half, input.scalar_type(), "searchsorted_out_cuda", [&] {
ATen/native/cuda/BinaryLogicalOpsKernels.cu:    AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, ScalarType::BFloat16,
ATen/native/cuda/BinaryLogicalOpsKernels.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, ScalarType::BFloat16,
ATen/native/cuda/BinaryLogicalOpsKernels.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, ScalarType::BFloat16,
ATen/native/cuda/Sort.cu:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, key.scalar_type(), "sortKeyValueInplace", [&]  {
ATen/native/cuda/CompareKernels.cu:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBFloat16, kBool, iter.common_dtype(), "compare_cuda", [&]() {
ATen/native/cuda/ActivationHardswishKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "hardswish_cuda", [&]() {
ATen/native/cuda/ActivationHardswishKernel.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "hardswish_backward_cuda", [&]() {
ATen/native/cuda/ConvolutionMM2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/ConvolutionMM2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/cuda/ConvolutionMM2d.cu:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(),
ATen/native/TensorConversions.cpp:  if ((self.dtype() == at::ScalarType::Half || self.dtype() == at::ScalarType::BFloat16) &&
ATen/native/cpu/UnaryOpsKernel.cpp:  if (dtype == kComplexHalf) {
ATen/native/cpu/UnaryOpsKernel.cpp:    using scalar_t = c10::complex<Half>;
ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), "abs_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "angle_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_CASE_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, [&] {
ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_CASE_COMPLEX_TYPES_AND(kComplexHalf, [&] {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "frac_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(1), "logical_not_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(0), "logical_not_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "reciprocal_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, kBFloat16, kHalf, iter.dtype(), "neg_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, ScalarType::Half, iter.dtype(), "sign_cpu", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:      AT_DISPATCH_CASE_FLOATING_TYPES_AND2(kBFloat16, ScalarType::Half, [&] {
ATen/native/cpu/UnaryOpsKernel.cpp:  if (dtype == kComplexHalf) {
ATen/native/cpu/UnaryOpsKernel.cpp:    using scalar_t = c10::complex<Half>;
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "exp2", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "nan_to_num", [&]() {
ATen/native/cpu/UnaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf,
ATen/native/cpu/Activation.cpp:    const Vec kOneHalfVec(one_half);
ATen/native/cpu/Activation.cpp:            grad_val0 * ((self_val0 / kThreeVec) + kOneHalfVec),
ATen/native/cpu/Activation.cpp:            grad_val1 * ((self_val1 / kThreeVec) + kOneHalfVec),
ATen/native/cpu/Activation.cpp:    const Vec kOneHalfVec(one_half);
ATen/native/cpu/Activation.cpp:            grad_val * ((self_val / kThreeVec) + kOneHalfVec),
ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, self.scalar_type(), "min_cpu", [&] {
ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(ScalarType::Half, ScalarType::BFloat16, ScalarType::Bool, self.scalar_type(), "max_cpu", [&] {
ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBFloat16, kBool,
ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isposinf_cpu", [&]() {
ATen/native/cpu/TensorCompareKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.input_dtype(), "isneginf_cpu", [&]() {
ATen/native/cpu/TensorCompareKernel.cpp:      kHalf, kBFloat16, kBool, self.scalar_type(), "mode_cpu", [&] {
ATen/native/cpu/SumKernel.cpp:      ScalarType::BFloat16, ScalarType::Half, iter.dtype(), "sum_cpu", [&] {
ATen/native/cpu/SumKernel.cpp:      ScalarType::BFloat16, ScalarType::Half, iter.dtype(), "nansum_cpu", [&] {
ATen/native/cpu/LinearAlgebraKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf,
ATen/native/cpu/SortingKernel.cpp:    ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
ATen/native/cpu/MultinomialKernel.cpp:      kHalf, kBFloat16, self.scalar_type(), "multinomial", [&] {
ATen/native/cpu/UnfoldBackwardKernel.cpp:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cpu/FillKernel.cpp:void fill_non_native_type<c10::complex<at::Half>>(TensorIterator& iter, const Scalar& value_scalar) {
ATen/native/cpu/FillKernel.cpp:  static_assert(sizeof(c10::complex<at::Half>) == sizeof(int32_t), "Size of ComplexHalf should be 32-bits");
ATen/native/cpu/FillKernel.cpp:  auto value = c10::complex<at::Half>(value_scalar.to<c10::complex<float>>());
ATen/native/cpu/FillKernel.cpp:  if (iter.dtype() == ScalarType::Half) {
ATen/native/cpu/FillKernel.cpp:    fill_non_native_type<at::Half>(iter, value_scalar);
ATen/native/cpu/FillKernel.cpp:  } else if (iter.dtype() == ScalarType::ComplexHalf) {
ATen/native/cpu/FillKernel.cpp:    fill_non_native_type<c10::complex<at::Half>>(iter, value_scalar);
ATen/native/cpu/RangeFactoriesKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "arange_cpu", [&]() {
ATen/native/cpu/RangeFactoriesKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, iter.dtype(), "linspace_cpu", [&]() {
ATen/native/cpu/PointwiseOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, dtype, "huber_backward_cpu_out", [&] {
ATen/native/cpu/SparseFactories.cpp:      at::ScalarType::Half,
ATen/native/cpu/SparseFactories.cpp:      at::ScalarType::ComplexHalf,
ATen/native/cpu/zmath.h:inline c10::complex<at::Half> conj_impl <c10::complex<at::Half>> (c10::complex<at::Half> z) {
ATen/native/cpu/zmath.h:  return c10::complex<at::Half>{z.real(), -z.imag()};
ATen/native/cpu/CopyKernel.cpp:  } else if (dtype == ScalarType::ComplexHalf) {
ATen/native/cpu/CopyKernel.cpp:    cpu_kernel(iter, [=](c10::complex<at::Half> a) -> c10::complex<at::Half> { return a; });
ATen/native/cpu/CopyKernel.cpp:        kBool, kHalf, kBFloat16, dtype, "copy_kernel", [&] {
ATen/native/cpu/CopyKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::ComplexHalf, ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, dtype, "copy_", [&] {
ATen/native/cpu/CopyKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::ComplexHalf, ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, iter.dtype(1), "copy_", [&] {
ATen/native/cpu/BlasKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::kHalf, at::kBFloat16,
ATen/native/cpu/BlasKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::kHalf, at::kBFloat16, type, "cpublas_axpy_impl",
ATen/native/cpu/BlasKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(at::kComplexHalf, at::kHalf, at::kBFloat16, at::kBool, type, "cpublas_copy_impl",
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kHalf, kBool, kBFloat16,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, kComplexHalf,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, kComplexHalf,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kBFloat16, kHalf,
ATen/native/cpu/IndexKernel.cpp:      ScalarType::Half,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/IndexKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/IndexKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kHalf, kBFloat16, iter.dtype(), "flip_cpu",
ATen/native/cpu/PowKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, dtype, "pow", [&]() {
ATen/native/cpu/PowKernel.cpp:// to use this common-path. Half cannot currently use it, as AVX2 support for
ATen/native/cpu/PowKernel.cpp:  } else if (dtype == ScalarType::Half) {
ATen/native/cpu/PowKernel.cpp:          decltype(c10::impl::ScalarTypeToCPPType<ScalarType::Half>::t);
ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/PixelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
ATen/native/cpu/ScatterGatherKernel.cpp:      ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, iter.dtype(),
ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
ATen/native/cpu/ReduceAllOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "min_all", [&] {
ATen/native/cpu/ReduceAllOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, input.scalar_type(), "max_all", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "std_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, iter.input_dtype(), "norm_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.dtype(), "min_values_cpu", [&iter] {
ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, iter.dtype(), "max_values_cpu", [&iter] {
ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(1), "argmax_cpu", [&] {
ATen/native/cpu/ReduceOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, iter.dtype(1), "argmin_cpu", [&] {
ATen/native/cpu/ComplexKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.input_dtype(), "complex_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  } else if (iter.dtype() == kComplexHalf) {
ATen/native/cpu/BinaryOpsKernel.cpp:        [=](c10::complex<at::Half> a,
ATen/native/cpu/BinaryOpsKernel.cpp:            c10::complex<at::Half> b) -> c10::complex<at::Half> {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), "mul_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "div_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, dtype, "div_trunc_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, dtype, "div_floor_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_and_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), "logical_and_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_or_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_or_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "logical_xor_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.common_dtype(), "logical_xor_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "lt_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "lt_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "le_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "le_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "gt_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "gt_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND3(kBool, kBFloat16, kHalf, iter.common_dtype(), "ge_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "ge_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kBFloat16, kHalf, iter.common_dtype(), "eq_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, kBFloat16, kHalf, iter.common_dtype(), "eq_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kComplexHalf, kBool, kBFloat16, kHalf, iter.common_dtype(), "ne_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kComplexHalf, kBFloat16, kHalf, iter.common_dtype(), "ne_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "maximum_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "minimum_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmax_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.common_dtype(), "fmin_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:        kHalf, iter.dtype(), "smooth_l1_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.dtype(), "huber_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND(kHalf, iter.dtype(), "sigmoid_backward_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  if (iter.dtype() == ScalarType::Half) {
ATen/native/cpu/BinaryOpsKernel.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "fmod_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "igamma_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "igammac_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_ALL_TYPES_AND3(kHalf, kBool, kBFloat16, iter.dtype(), "heaviside_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "copysign_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "xlogy_cpu", [&]() {
ATen/native/cpu/BinaryOpsKernel.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "xlog1py_cpu", [&]() {
ATen/native/cpu/ChannelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/ChannelShuffleKernel.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(ScalarType::Bool, ScalarType::BFloat16, ScalarType::Half,
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "random_from_to_kernel_cpu", [&] {
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool, iter.dtype(), "random_kernel_cpu", [&] {
ATen/native/cpu/DistributionTemplates.h:    AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, self.scalar_type(), "normal_kernel_cpu", [&] {
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "uniform_kernel_cpu", [&]() {
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, iter.dtype(), "cauchy_cpu", [&]() {
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "log_normal_cpu", [&]() {
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "geometric_cpu", [&]() {
ATen/native/cpu/DistributionTemplates.h:  AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, iter.dtype(), "exponential_cpu", [&]() {
ATen/native/cpu/DistributionKernels.cpp:    AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "exponential_cpu", [&] {
ATen/native/SpectralOps.cpp:    TORCH_CHECK(type == kHalf || type == kFloat || type == kDouble, "Unsupported dtype ", type);
ATen/native/SpectralOps.cpp:  case kHalf: return kComplexHalf;
ATen/native/SoftMax.cpp:        input_dtype == ScalarType::Half) {
ATen/native/SoftMax.cpp:      grad_input_options = grad_input_options.dtype(ScalarType::Half);
ATen/native/SoftMax.cpp:        input_dtype == ScalarType::Half) {
ATen/native/SoftMax.cpp:      grad_input_options = grad_input_options.dtype(ScalarType::Half);
ATen/native/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
ATen/native/SoftMax.cpp:  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&
ATen/native/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
ATen/native/SoftMax.cpp:  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&
ATen/native/TensorAdvancedIndexing.h:  if (!(c10::isFloatingType(st)) || st == ScalarType::Half) {
ATen/native/Bucketization.cpp:        ScalarType::Half,
ATen/native/Bucketization.cpp:        ScalarType::Half,
ATen/native/miopen/BatchNorm_miopen.cpp:  if (input->scalar_type() != ScalarType::Half) {
ATen/native/miopen/BatchNorm_miopen.cpp:  if (input->scalar_type() == ScalarType::Half) {
ATen/native/RNN.cpp:    bool is_miopen_acceptable = ((input.scalar_type() == at::kFloat)|| (input.scalar_type() == at::kHalf)) &&
ATen/native/RNN.cpp:          result_dtype == at::kHalf,
ATen/native/transformers/cuda/attention.cu:      ScalarType::Half,
ATen/native/transformers/cuda/flash_attn/fmha_api.cpp:    TORCH_CHECK(q_dtype == at::kHalf || (is_sm8x && q_dtype == at::kBFloat16));
ATen/native/transformers/cuda/sdp_utils.h:  bool is_half = (params.query.dtype() == at::kHalf) ||
ATen/native/transformers/cuda/sdp_utils.h:    static const std::array<at::ScalarType, 2> sm80_flash_dtypes{at::kHalf, at::kBFloat16};
ATen/native/transformers/cuda/sdp_utils.h:    static const std::array<at::ScalarType, 1> default_flash_dtypes{at::kHalf};
ATen/native/transformers/cuda/sdp_utils.h:      at::kHalf, at::kFloat, at::kBFloat16};
ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:  static constexpr bool kIsHalf = cutlass::sizeof_bits<scalar_t>::value <= 16;
ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:  static constexpr bool kOutputInRF = kIsHalf && kMaxK <= kBlockSizeI;
ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:      kIsHalf && ArchTag::kMinComputeCapability >= 80 && kOutputInRF;
ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h:      kIsHalf && (kOutputInRF || ArchTag::kMinComputeCapability != 70);
ATen/native/transformers/cuda/mem_eff_attention/gemm_kernel_utils.h:    } else if (query.scalar_type() == at::ScalarType::Half) {               \
ATen/native/transformers/cuda/mem_eff_attention/gemm_kernel_utils.h:    return at::ScalarType::Half;
ATen/native/transformers/attention.cpp:      ScalarType::Half,
ATen/native/ComplexHelper.h:    self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,
ATen/native/Col2Im.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf,
ATen/native/Copy.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(kHalf, kBool, kBFloat16, kComplexHalf, self.scalar_type(), "copy_", [&] {
ATen/native/Copy.cpp:    if (((self.dtype() == at::kFloat && src.dtype() == at::kHalf) ||
ATen/native/Copy.cpp:         (self.dtype() == at::kHalf && src.dtype() == at::kFloat)) &&
ATen/native/Copy.cpp:      if (src.dtype() == at::kFloat && self.dtype() == at::kHalf) {
ATen/native/Copy.cpp:            reinterpret_cast<fbgemm::float16*>(self.data_ptr<at::Half>());
ATen/native/Copy.cpp:            src.data_ptr<at::Half>());
ATen/native/Scalar.cpp:    kComplexHalf, kHalf, kBool, kBFloat16, self.scalar_type(), "_local_scalar_dense_cpu", [&] {
ATen/native/TensorTransformations.cpp:  return self.to(kComplexHalf, false, false, memory_format);
ATen/native/RangeFactories.cpp:  AT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, result.scalar_type(), "arange_cpu", [&]() {
ATen/native/TensorAdvancedIndexing.cpp:        // scatter_add does not support ComplexHalf
ATen/native/TensorAdvancedIndexing.cpp:        source.scalar_type() != ScalarType::ComplexHalf &&
ATen/native/TensorAdvancedIndexing.cpp:        result.scalar_type() != ScalarType::ComplexHalf) {
ATen/native/TensorAdvancedIndexing.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16, ScalarType::ComplexHalf,
ATen/native/TensorAdvancedIndexing.cpp:      at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/TensorAdvancedIndexing.cpp:    AT_DISPATCH_ALL_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,
ATen/native/TensorAdvancedIndexing.cpp:      AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND4(ScalarType::ComplexHalf, ScalarType::Half, ScalarType::Bool, ScalarType::BFloat16,
ATen/native/TensorAdvancedIndexing.cpp:    at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool,
ATen/native/TensorAdvancedIndexing.cpp:      kComplexHalf, kHalf, kBFloat16, kBool, self.scalar_type(), "nonzero_count_cpu", [&] {
ATen/native/TensorAdvancedIndexing.cpp:      kComplexHalf, kHalf, kBFloat16, kBool, self.scalar_type(), "nonzero_count_cpu", [&] {
ATen/native/TensorAdvancedIndexing.cpp:      kComplexHalf, kHalf, kBFloat16, kBool, self.scalar_type(), "nonzero_cpu", [&] {
ATen/native/BlasKernel.cpp:INSTANTIATE_DOT_IMPL(c10::Half);
ATen/native/Math.h:#include <c10/util/Half.h>
ATen/native/Math.h:C10_UNUSED c10::Half calc_igamma<c10::Half>(c10::Half a, c10::Half x) {
ATen/native/Math.h:C10_UNUSED c10::Half calc_igammac<c10::Half>(c10::Half a, c10::Half x) {
ATen/native/TensorFactories.cpp:#include <ATen/ops/_cast_Half_native.h>
ATen/native/TensorFactories.cpp:  TORCH_CHECK((a.scalar_type() == kFloat || a.scalar_type() == kDouble || a.scalar_type() == kHalf) &&
ATen/native/TensorFactories.cpp:              (b.scalar_type() == kFloat || b.scalar_type() == kDouble || b.scalar_type() == kHalf),
ATen/native/TensorFactories.cpp:              "Expected both inputs to be Half, Float or Double tensors but got ",
ATen/native/TensorFactories.cpp:// AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, DEFINE_CAST_OP)
ATen/native/TensorFactories.cpp:AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16, Float8, DEFINE_CAST_OP)
ATen/native/TensorFactories.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::ScalarType::Half, at::ScalarType::Bool, result.scalar_type(), "eye", [&]() -> void {
ATen/native/TensorFactories.cpp:  AT_DISPATCH_ALL_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, result.scalar_type(), "randperm", [&]() -> void {
ATen/native/SegmentReduce.cpp:      kBFloat16, kHalf, data.scalar_type(), "_segment_reduce_cpu", [&]() {
ATen/native/SegmentReduce.cpp:      kHalf,
ATen/native/sparse/SparseTensor.cpp:      at::ScalarType::ComplexHalf, at::ScalarType::BFloat16, at::ScalarType::Half, at::ScalarType::Bool,
ATen/native/sparse/cuda/SparseMatMul.cu:      std::is_same<c10::Half, scalar_t>::value ||
ATen/native/sparse/cuda/SparseMatMul.cu:    std::is_same<c10::Half, scalar_t>::value ||
ATen/native/sparse/cuda/SparseMatMul.cu:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kHalf, kBFloat16, mat1_.scalar_type(), "sparse_matmul", [&] {
ATen/native/sparse/cuda/SparseBlasImpl.cpp:  if (mat1.scalar_type() == ScalarType::Half || mat1.scalar_type() == ScalarType::BFloat16) {
ATen/native/sparse/cuda/SparseBlasImpl.cpp:      kHalf,
ATen/native/sparse/cuda/SparseBlasImpl.cpp:      kHalf,
ATen/native/sparse/cuda/SparseBlasImpl.cpp:  if (dispatch_scalar_type == at::ScalarType::Half ||
ATen/native/sparse/cuda/SparseCUDATensorMath.cu:        at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
ATen/native/sparse/cuda/SparseCUDATensorMath.cu:        at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16, commonDtype, "add_out_dense_sparse_cuda", [&] {
ATen/native/sparse/cuda/SparseCUDATensorMath.cu:    at::ScalarType::Half, at::ScalarType::BFloat16, commonDtype, "add_out_sparse_cuda", [&] {
ATen/native/sparse/cuda/SparseCUDATensorMath.cu:      AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf, grad_values.scalar_type(), "_sparse_sum_backward_cuda", [&] {
ATen/native/sparse/cuda/SparseCsrTensorMath.cu:      kComplexHalf, kHalf, kBool, kBFloat16,
ATen/native/sparse/cuda/SparseCsrTensorMath.cu:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_sum_cuda",
ATen/native/sparse/cuda/SparseCsrTensorMath.cu:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_prod_cuda",
ATen/native/sparse/cuda/SparseCUDATensor.cu:      at::ScalarType::ComplexHalf, at::ScalarType::Half, at::ScalarType::BFloat16, at::ScalarType::Bool,
ATen/native/sparse/cuda/SparseCUDATensor.cu:    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1(kHalf,
ATen/native/sparse/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
ATen/native/sparse/SoftMax.cpp:    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
ATen/native/sparse/SparseBlasImpl.cpp:  const auto mm_dtype = (result.scalar_type() == kHalf || result.scalar_type() == kBFloat16)
ATen/native/sparse/SparseCsrTensorMath.cpp:      kComplexHalf,
ATen/native/sparse/SparseCsrTensorMath.cpp:      kHalf,
ATen/native/sparse/SparseCsrTensorMath.cpp:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_sum_cpu",
ATen/native/sparse/SparseCsrTensorMath.cpp:    kHalf, kBFloat16, input_.scalar_type(), "_sparse_csr_prod_cpu",
ATen/native/sparse/SparseTensorMath.cpp:          at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
ATen/native/sparse/SparseTensorMath.cpp:          at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
ATen/native/sparse/SparseTensorMath.cpp:        at::ScalarType::ComplexHalf, at::ScalarType::Bool, at::ScalarType::BFloat16, at::ScalarType::Half,
ATen/native/sparse/SparseTensorMath.cpp:    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(at::ScalarType::BFloat16, at::ScalarType::Half,
ATen/native/sparse/SparseBinaryOpIntersectionKernel.cpp:        ScalarType::Bool, ScalarType::Half, ScalarType::BFloat16, res_values.scalar_type(),
ATen/native/UpSampleBicubic2d.cpp:  AT_DISPATCH_FLOATING_TYPES_AND2(ScalarType::Half, ScalarType::BFloat16,
ATen/native/Convolution.cpp:                             input.scalar_type() == kHalf && // only for FP16
ATen/native/Convolution.cpp:                             weight.scalar_type() == kHalf &&
ATen/native/Convolution.cpp:                           input.scalar_type() == kHalf && // only for FP16
ATen/native/Convolution.cpp:                           weight.scalar_type() == kHalf &&
ATen/native/Convolution.cpp:    return ((input.scalar_type() == at::kFloat) || (input.scalar_type() == at::kHalf) || (input.scalar_type() == at::kBFloat16))
ATen/native/Im2Col.cpp:  AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(kBFloat16, kHalf,
ATen/native/Normalization.cpp:      && (input.scalar_type() != at::kHalf
ATen/native/Normalization.cpp:               && (weight.scalar_type() != at::kHalf)
ATen/native/nested/NestedTensorBackward.cpp:    ScalarType::Half, ScalarType::BFloat16, self_grad_buffer.scalar_type(), "nested_sum_dim_cpu", [&]() {
ATen/native/nested/cuda/NestedTensorBinaryOps.cu:    ScalarType::Half, ScalarType::BFloat16, self.scalar_type(), "_nested_op_dense_esuhm", [&]() {
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:    if (padded.dtype() != kFloat && padded.dtype() != kHalf) {
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:    } else if (padded.dtype() == kHalf) {
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            padded_contiguous.data_ptr<c10::Half>(),
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            output.data_ptr<c10::Half>(),
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            padded_contiguous.data_ptr<c10::Half>(),
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:            output.data_ptr<c10::Half>(),
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp:       t.dtype() == at::kHalf)) {
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:template void remove_padding_kernelLauncher<c10::Half>(
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    const c10::Half* input,
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* output,
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:template void remove_padding_transform0213_kernelLauncher<c10::Half>(
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    const c10::Half* input,
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* output,
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:template void add_padding_kernelLauncher<c10::Half>(
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* input,
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half* output,
ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu:    c10::Half padding_value,
ATen/native/nested/cuda/NestedTensorMatmul.cu:    const std::vector<c10::Half*>& aptr_,
ATen/native/nested/cuda/NestedTensorMatmul.cu:    const std::vector<c10::Half*>& bptr_,
ATen/native/nested/cuda/NestedTensorMatmul.cu:    const std::vector<c10::Half*>& dptr_,
ATen/native/nested/NestedTensorUtils.h:        at::ScalarType::Half,
ATen/native/nested/NestedTensorMath.cpp:    ScalarType::Half, ScalarType::BFloat16, buffer.scalar_type(), "nested_sum_dim_cpu", [&]() {
ATen/native/TensorProperties.cpp:  if (!(st == kDouble || st == kFloat || st == kHalf)) return false;
ATen/native/Distributions.cpp:  // Half is not supported on CPU.
ATen/native/Distributions.cpp:      !(self.device().is_cpu() && self.scalar_type() == ScalarType::Half),
ATen/native/cudnn/Conv_v8.cpp:  if (scalar_type == kBFloat16 || scalar_type == kHalf) {
ATen/native/cudnn/Conv_v7.cpp:    const auto kAccType = (grad_weight.scalar_type() == kHalf || grad_weight.scalar_type() == kBFloat16)
ATen/native/cudnn/BatchNorm.cpp:  if (input->scalar_type() == ScalarType::Half) {
ATen/native/cudnn/BatchNorm.cpp:  if (input->scalar_type() == ScalarType::Half) {
ATen/native/native_functions.yaml:- func: _cast_Half(Tensor self, bool non_blocking=False) -> Tensor
ATen/native/native_functions.yaml:    Generic: add (AllAndComplex, BFloat16, Half, ComplexHalf)
ATen/native/TensorCompare.cpp:  return AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, self.scalar_type(), "isinf", [&]() {
ATen/native/TensorCompare.cpp:  return AT_DISPATCH_FLOATING_TYPES_AND2(kHalf, kBFloat16, self.scalar_type(), "isfinite", [&]() {
ATen/native/EmbeddingBag.cpp:#include <c10/util/Half.h>
ATen/native/EmbeddingBag.cpp:  return (src.scalar_type() == kFloat || src.scalar_type() == kHalf) && src.strides()[1] == 1 && output.strides()[1] == 1 && padding_idx < static_cast<index_t>(0);
ATen/native/EmbeddingBag.cpp:  return (src.scalar_type() == kFloat || src.scalar_type() == kHalf) && src.strides()[1] == 1 && output.strides()[1] == 1 && scale.strides()[0] == 1 && padding_idx < static_cast<index_t>(0);
ATen/native/EmbeddingBag.cpp:typename std::enable_if<!std::is_same<data_t, float>::value && !std::is_same<data_t, at::Half>::value, void>::type
ATen/native/EmbeddingBag.cpp:typename std::enable_if<std::is_same<data_t, at::Half>::value, void>::type
ATen/native/EmbeddingBag.cpp:  auto* output_data = output.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:    auto* src_data = src_contig.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:              (output_data + i * ddim)[d] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
ATen/native/EmbeddingBag.cpp:    auto* src_data = src.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:        (output_data + output_stride0 * i)[d * output_stride1] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
ATen/native/EmbeddingBag.cpp:static typename std::enable_if<!std::is_same<data_t, float>::value && !std::is_same<data_t, at::Half>::value, void>::type
ATen/native/EmbeddingBag.cpp:typename std::enable_if<std::is_same<data_t, at::Half>::value, void>::type
ATen/native/EmbeddingBag.cpp:  auto* scale_data = scale.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:  auto* output_data = output.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:    auto* src_data = src_contig.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:              (output_data + i * ddim)[d] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
ATen/native/EmbeddingBag.cpp:    auto* src_data = src.data_ptr<at::Half>();
ATen/native/EmbeddingBag.cpp:        (output_data + output_stride0 * i)[d * output_stride1] = static_cast<at::Half>((output_data_fp32 + ddim * i)[d]);
ATen/native/EmbeddingBag.cpp:  checkScalarTypes("embedding_bag", weight_arg, {kHalf, kFloat, kDouble});
ATen/native/EmbeddingBag.cpp:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, weight.scalar_type(), "embedding_bag_no_grad_cpu_out",
ATen/native/EmbeddingBag.cpp:  checkScalarTypes("embedding_bag", grad_arg, {kHalf, kFloat, kDouble});
ATen/native/EmbeddingBag.cpp:      at::ScalarType::Half,
ATen/native/EmbeddingBag.cpp:      at::ScalarType::Half,
ATen/native/ReduceOpsUtils.h:    self.is_cuda() && (self.scalar_type() == kHalf || self.scalar_type() == kBFloat16) && out_dtype == kFloat);
ATen/native/ReduceOpsUtils.h:      (self.is_cuda() && self.scalar_type() == kHalf && dtype1 == kFloat)) {
ATen/native/ReduceOpsUtils.h:      (self.is_cuda() && self.scalar_type() == kHalf && dtype1 == kFloat)) {
ATen/native/ReduceOpsUtils.h:       (self.scalar_type() == kHalf || self.scalar_type() == kBFloat16) &&
ATen/native/WeightNorm.cpp:  auto has_half_dtype = v.scalar_type() == at::ScalarType::Half
ATen/native/WeightNorm.cpp:    || g.scalar_type() == at::ScalarType::Half;
ATen/native/TensorFactories.h:    case at::ScalarType::Half:
ATen/native/TensorFactories.h:      TORCH_CHECK(n <= (int64_t(1) << 11) + 1, "n cannot be greater than 2049 for Half type.");
ATen/native/ReduceOps.cpp:      iter.common_dtype() != kBFloat16 && iter.common_dtype() != kHalf) {
ATen/native/ReduceOps.cpp:  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(kBool, kBFloat16, kHalf, iter.input_dtype(), "equal_cpu", [&] {
ATen/native/ForeachUtils.h:      kComplexHalf,
ATen/native/ForeachUtils.h:      kHalf,
ATen/native/mps/TensorFactory.h:      AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__))
ATen/native/mps/OperationUtils.h:    at::Half h;
ATen/native/mps/operations/ReduceOps.mm:             (dtype.value() == kFloat || dtype.value() == kHalf || dtype.value() == kInt)) {
ATen/native/mps/operations/ReduceOps.mm:              input_t.scalar_type() != ScalarType::Half) {
ATen/native/mps/operations/ReduceOps.mm:              input_t.scalar_type() != ScalarType::Half) {
ATen/native/mps/operations/ReduceOps.mm:              input_t.scalar_type() != ScalarType::Half) {
ATen/native/mps/operations/Indexing.mm:                inputTensor.scalar_type() == ScalarType::Half,
ATen/native/mps/operations/Distributions.mm:              return (self.scalar_type() == ScalarType::Half) ? MPSDataTypeFloat16 : MPSDataTypeFloat32;
ATen/native/mps/operations/Distributions.mm:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input_dtype, "random_update_from_to", [&] {
ATen/native/mps/operations/Distributions.mm:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input_dtype, "random_from_to_range_calc", [&] {
ATen/native/mps/operations/LossOps.mm:          MPSGraphTensor *mpsGraphHalfTensor = [mpsGraph constantWithScalar: 0.5
ATen/native/mps/operations/LossOps.mm:          MPSGraphTensor *diffSquareMulHalfTensor = [mpsGraph multiplicationWithPrimaryTensor: diffSquare
ATen/native/mps/operations/LossOps.mm:                                                                              secondaryTensor: mpsGraphHalfTensor
ATen/native/mps/operations/LossOps.mm:          MPSGraphTensor *loss1Temp = [mpsGraph divisionWithPrimaryTensor: diffSquareMulHalfTensor
ATen/native/mps/operations/TensorCompare.mm:    AT_DISPATCH_FLOATING_TYPES_AND(kHalf, self.scalar_type(), "nan_to_num_mps", [&]() {
ATen/native/mps/operations/SummaryOps.mm:       weights.scalar_type() != ScalarType::Half) {
ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* firstHalf = outputTensorsArray[0];
ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* secondHalf = [mpsGraph sigmoidWithTensor:outputTensorsArray[1]
ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* outputTensor = [mpsGraph multiplicationWithPrimaryTensor:firstHalf
ATen/native/mps/operations/Activation.mm:                                                   secondaryTensor:secondHalf
ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* firstHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : sigmoidOutputTensor
ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* secondHalfOutputTensor = [mpsGraph subtractionWithPrimaryTensor : one_val
ATen/native/mps/operations/Activation.mm:          secondHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : secondHalfOutputTensor
ATen/native/mps/operations/Activation.mm:          secondHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : secondHalfOutputTensor
ATen/native/mps/operations/Activation.mm:          secondHalfOutputTensor = [mpsGraph multiplicationWithPrimaryTensor : secondHalfOutputTensor
ATen/native/mps/operations/Activation.mm:          MPSGraphTensor* outputTensor = [mpsGraph concatTensor : firstHalfOutputTensor
ATen/native/mps/operations/Activation.mm:                                                     withTensor : secondHalfOutputTensor
ATen/native/mps/operations/Linear.mm:              input.scalar_type() == ScalarType::Half, "MPS device does not support linear for non-float inputs");
ATen/native/mps/operations/Linear.mm:             (weight.scalar_type() == kFloat || (weight.scalar_type() == kHalf)),
ATen/native/mps/operations/Linear.mm:              || grad_output.scalar_type() == ScalarType::Half, "MPS device does not support linear backward for non-float inputs");
ATen/native/mps/operations/Linear.mm:              grad_output.scalar_type() == ScalarType::Half, "MPS device does not support linear backward for non-float inputs");
ATen/native/mps/operations/LinearAlgebra.mm:              || self.scalar_type() == ScalarType::Half, "MPS device does not support mm for non-float inputs");
ATen/native/mps/operations/LinearAlgebra.mm:              || self.scalar_type() == ScalarType::Half, "MPS device does not support addmm for non-float input");
ATen/native/mps/operations/LinearAlgebra.mm:              || batch1.scalar_type() == ScalarType::Half, "MPS device does not support bmm for non-float inputs");
ATen/native/mps/operations/LinearAlgebra.mm:              || batch1.scalar_type() == ScalarType::Half, "MPS device does not support addbmm or baddbmm for non-float inputs");
ATen/native/mps/operations/Scalar.mm:    at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16, self.scalar_type(), "_local_scalar_dense_mps", [&] {
ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:
ATen/native/mps/OperationUtils.mm:    case ScalarType::Half:  return {.value.h = scalar.to<at::Half>(), .size = sizeof(short)  , .type = type};
ATen/native/DistributionTemplates.h:    std::is_same<scalar_t, at::Half>::value ||
ATen/native/DistributionTemplates.h:    std::is_same<scalar_t, at::Half>::value ||
ATen/native/DistributionTemplates.h:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, scalar_type, "check_random_fp_bounds", [&] {
ATen/native/DistributionTemplates.h:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "random_update_from_to", [&] {
ATen/native/DistributionTemplates.h:      AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "random_from_to_range_calc", [&] {
ATen/native/DistributionTemplates.h:    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, self.scalar_type(), "check_uniform_bounds", [&] {
ATen/native/BatchLinearAlgebra.cpp:  // Half optimisation half precondition for some parts of the LAPACK / cuSOLVER
ATen/native/quantized/QTensor.cpp:  TORCH_CHECK( (dtype == ScalarType::QInt8 || dtype == ScalarType::QUInt8 || dtype == ScalarType::Half), "dtype ", dtype, "not supported");
ATen/native/quantized/QTensor.cpp:  if (dtype == ScalarType::Half) {
ATen/native/quantized/QTensor.cpp:    return input_contig.to(ScalarType::Half);
ATen/native/quantized/QTensor.cpp:  xmin = static_cast<at::Half>(xmin);
ATen/native/quantized/QTensor.cpp:      : static_cast<float>(static_cast<at::Half>(data_range / qmax));
ATen/native/quantized/cuda/EmbeddingBag.cu:      static_cast<std::int64_t>(input_columns - 2 * sizeof(at::Half)) *
ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:    fbgemm::Fused8BitRowwiseQuantizedSBFloatToFloatOrHalf<float>(
ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:      static_cast<std::int64_t>(input_columns - 2 * sizeof(at::Half)) *
ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:    fbgemm::FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf<float>(
ATen/native/quantized/cpu/qembeddingbag_unpack.cpp:    const at::Half* input_row_scale_zp = reinterpret_cast<const at::Half*>(
ATen/native/quantized/cpu/qembeddingbag.cpp:            weight_data + (idx + 1) * weight_size - 2 * sizeof(at::Half);
ATen/native/quantized/cpu/qembeddingbag.cpp:        at::Half scale_val = (reinterpret_cast<at::Half*>(&scale_val_int16))[0];
ATen/native/quantized/cpu/qembeddingbag.cpp:        at::Half bias_val = (reinterpret_cast<at::Half*>(&bias_val_int16))[0];
ATen/native/quantized/cpu/qembeddingbag.cpp:      (weight_size - 2 * sizeof(at::Half)) * NUM_ELEM_PER_BYTE; // NB: 2-byte fp16 scale and 2-byte zero_offset
ATen/native/quantized/cpu/qembeddingbag.cpp:         per_sample_weights_.value().scalar_type() == at::kHalf),
ATen/native/quantized/cpu/qembeddingbag.cpp:         per_sample_weights_.value().scalar_type() == at::kHalf),
ATen/native/quantized/cpu/qembeddingbag.cpp:         per_sample_weights_.value().scalar_type() == at::kHalf),
ATen/native/quantized/cpu/LinearUnpackImpl.cpp:      at::empty({ncols, nrows}, at::kHalf, c10::MemoryFormat::Contiguous);
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:        4; // extra 4 bytes to store at::Half scale and bias per row.
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:            at::Half* output_row_scale_bias =
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:                reinterpret_cast<at::Half*>(output_row + embedding_cols);
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          weight.scalar_type() == at::ScalarType::Half,
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:  if (weight_contig->scalar_type() == at::ScalarType::Half) {
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          fbgemm::FloatOrHalfToFused8BitRowwiseQuantizedSBFloat<float>(
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:  const Tensor& float_weight = weight_contig->scalar_type() == at::ScalarType::Half
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          weight.scalar_type() == at::ScalarType::Half,
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:          2 * sizeof(at::Half))};
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:    if (weight_contig.scalar_type() == at::ScalarType::Half) {
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:            fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:            fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float>(
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:        weight_contig.scalar_type() == at::ScalarType::Half
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:      Xmin = static_cast<at::Half>(Xmin);
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:      at::Half scale = range == 0 ? 1.0f : range / ((1 << bit_width) - 1);
ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:      at::Half* output_row_scale_zp = reinterpret_cast<at::Half*>(
ATen/native/quantized/FakeQuantPerChannelAffine.cpp:  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Int || zero_point.scalar_type() == ScalarType::Float || zero_point.scalar_type() == ScalarType::Half,
ATen/native/quantized/FakeQuantPerChannelAffine.cpp:              "Zero-point must be Int32, Float or Half, found ", zero_point.scalar_type());
ATen/native/mkldnn/RNN.cpp:          tensor.scalar_type() == at::ScalarType::Half,
ATen/native/vulkan/ops/Copy.cpp:  } else if (src.dtype() == at::kHalf) {
ATen/native/vulkan/ops/Copy.cpp:    memcpy_to_mapping_impl<c10::Half>(src, dst_mapping);
ATen/native/vulkan/ops/Copy.cpp:        " at::kHalf or at::Float but got ",
ATen/native/vulkan/ops/Copy.cpp:  } else if (dst.dtype() == at::kHalf) {
ATen/native/vulkan/ops/Copy.cpp:    memcpy_from_mapping_impl<c10::Half>(src_mapping, dst);
ATen/native/vulkan/ops/Copy.cpp:        " at::kHalf or at::Float but got ",
ATen/native/vulkan/ops/Copy.cpp:    // If the dtype() of src is at::kHalf, then first convert it to 32 bit
ATen/native/vulkan/ops/Copy.cpp:    if (src.dtype() == at::kHalf) {
ATen/native/vulkan/ops/Copy.cpp:    // If the dtype() of dst is at::kHalf, then copy the data into a float
ATen/native/vulkan/ops/Copy.cpp:    if (dst.dtype() == at::kHalf) {
ATen/native/vulkan/ops/Copy.cpp:      dst = dst_float.to(at::kHalf);
ATen/native/vulkan/api/Resource.cpp: * TODO: enable proper format selection between kFloat and kHalf.
ATen/native/vulkan/api/Resource.cpp: * always created with the corresponding VkFormat. Consequently, kHalf tensors
ATen/native/vulkan/api/Resource.cpp:      return c10::kHalf;
ATen/native/vulkan/api/Utils.h:#include <c10/util/Half.h> // For c10::overflows
ATen/NumericUtils.h:#include <c10/util/Half.h>
ATen/NumericUtils.h:    typename std::enable_if<std::is_same<T, at::Half>::value, int>::type = 0>
ATen/NumericUtils.h:inline C10_HOST_DEVICE bool _isinf(at::Half val) {
ATen/Utils.cpp:// AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, TENSOR)
ATen/Utils.cpp:AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16, Float8, TENSOR)
ATen/ops/tensor.h:// AT_FORALL_SCALAR_TYPES_AND3(Bool, Half, BFloat16, TENSOR)
ATen/ops/tensor.h:AT_FORALL_SCALAR_TYPES_AND4(Bool, Half, BFloat16,Float8, TENSOR)
ATen/EmptyTensor.cpp:  if (dtype == kComplexHalf) {
ATen/EmptyTensor.cpp:        "ComplexHalf support is experimental and many operators don't support it yet.");
ATen/cudnn/Types.cpp:  } else if (dtype == at::kHalf) {
ATen/cudnn/AutocastRNN.cpp:                                 is_eligible(weight_buf) && (weight_buf.scalar_type() != at::kHalf) :
ATen/cudnn/AutocastRNN.cpp:                                 is_eligible(weight[0]) && (weight[0].scalar_type() != at::kHalf));
ATen/cudnn/AutocastRNN.cpp:            /*flat_buf_datatype=*/at::native::getCudnnDataTypeFromScalarType(at::kHalf), // could just hardcode CUDNN_DATA_HALF
ATen/cudnn/AutocastRNN.cpp:            /*flat_buf_options=*/weight[0].options().dtype(at::kHalf),
ATen/cudnn/AutocastRNN.cpp:      cached_cast(at::kHalf, input),
ATen/cudnn/AutocastRNN.cpp:      cached_cast(at::kHalf, hx),
ATen/cudnn/AutocastRNN.cpp:      cached_cast(at::kHalf, cx),
ATen/cudnn/Descriptors.cpp:  } else if (scalar_type == at::kHalf) {
ATen/AccumulateType.h:#include <c10/util/Half.h>
ATen/AccumulateType.h:struct AccumulateType<Half, true> {
ATen/AccumulateType.h:struct AccumulateType<Half, false> {
ATen/AccumulateType.h:struct AccumulateType<c10::complex<Half>, false> {
ATen/AccumulateType.h:struct AccumulateType<c10::complex<Half>, true> {
ATen/SparseCsrTensorUtils.h:          kComplexHalf, kHalf, kBool, kBFloat16, __VA_ARGS__))
ATen/autocast_mode.cpp:thread_local at::ScalarType autocast_gpu_dtype = at::kHalf;
ATen/test/basic.cpp:  if (type.backend() != Backend::CPU || type.scalarType() != kHalf) {
ATen/test/basic.cpp:  if (type.backend() == Backend::CPU && type.scalarType() == kHalf) {
ATen/test/basic.cpp:  if (type.backend() != Backend::CPU || type.scalarType() != kHalf) {
ATen/test/basic.cpp:  if (type.backend() != Backend::CPU || type.scalarType() != kHalf) {
ATen/test/basic.cpp:TEST(BasicTest, BasicTestHalfCPU) {
ATen/test/basic.cpp:  test(CPU(kHalf));
ATen/test/basic.cpp:  at::Tensor tensor1 = at::empty({4}, at::TensorOptions().dtype(at::kHalf));
ATen/test/basic.cpp:  ASSERT_EQ(tensor1.dtype(), at::kHalf);
ATen/test/basic.cpp:  tensor1 = at::empty({4}, at::TensorOptions().dtype(at::kHalf).layout(at::kSparse));
ATen/test/basic.cpp:  ASSERT_EQ(tensor1.dtype(), at::kHalf);
ATen/test/basic.cpp:    tensor1 = at::empty({4}, at::TensorOptions().dtype(at::kHalf).device(at::kCUDA).layout(at::kSparse).requires_grad(false));
ATen/test/basic.cpp:    ASSERT_EQ(tensor1.dtype(), at::kHalf);
ATen/test/mps_test_print.cpp:TEST(MPSPrintTest, PrintHalf4DTensor) {
ATen/test/mps_test_print.cpp:  ss << torch::randn({2, 2, 2, 2}, at::device(at::kMPS).dtype(at::kHalf));
ATen/test/mps_test_print.cpp:  ASSERT_TRUE (ends_with(ss.str(), "[ MPSHalfType{2,2,2,2} ]")) << " got " << ss.str();
ATen/test/scalar_test.cpp:struct Foo<Half> {
ATen/test/scalar_test.cpp:  s1.toHalf();
ATen/test/scalar_test.cpp:  ASSERT_THROW(s1.toHalf(), std::runtime_error);
ATen/test/scalar_test.cpp:  Half h = bar.toHalf();
ATen/test/scalar_test.cpp:  if (x.scalar_type() != ScalarType::Half) {
ATen/test/scalar_test.cpp:  ASSERT_EQ(float_one.item<at::Half>(), 1);
ATen/test/cuda_half_test.cu:  assert(Half(3) == Half(3.0f));
ATen/test/cuda_half_test.cu:  assert(static_cast<Half>(3.0f) == Half(3.0f));
ATen/test/cuda_half_test.cu:  assert(static_cast<Half>(3.0f) == 3.0f);
ATen/test/cuda_half_test.cu:  __half c = a - Half(b);
ATen/test/cuda_half_test.cu:  assert(static_cast<Half>(c) == Half(1.0));
ATen/test/cuda_half_test.cu:  assert(::abs(::lgamma(Half(10.0)) - ::lgamma(10.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::exp(Half(1.0)) - ::exp(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::log(Half(1.0)) - ::log(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::log10(Half(1000.0)) - ::log10(1000.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::log1p(Half(0.0)) - ::log1p(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::log2(Half(1000.0)) - ::log2(1000.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::expm1(Half(1.0)) - ::expm1(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::cos(Half(0.0)) - ::cos(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::sin(Half(0.0)) - ::sin(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::sqrt(Half(100.0)) - ::sqrt(100.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::ceil(Half(2.4)) - ::ceil(2.4f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::floor(Half(2.7)) - ::floor(2.7f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::trunc(Half(2.7)) - ::trunc(2.7f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::acos(Half(-1.0)) - ::acos(-1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::cosh(Half(1.0)) - ::cosh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::acosh(Half(1.0)) - ::acosh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::acosh(Half(1.0)) - ::acosh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::asinh(Half(1.0)) - ::asinh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::atanh(Half(1.0)) - ::atanh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::asin(Half(1.0)) - ::asin(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::sinh(Half(1.0)) - ::sinh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::asinh(Half(1.0)) - ::asinh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::tan(Half(0.0)) - ::tan(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::atan(Half(1.0)) - ::atan(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::tanh(Half(1.0)) - ::tanh(1.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::erf(Half(10.0)) - ::erf(10.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::erfc(Half(10.0)) - ::erfc(10.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::abs(Half(-3.0)) - ::abs(-3.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::round(Half(2.3)) - ::round(2.3f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::pow(Half(2.0), Half(10.0)) - ::pow(2.0f, 10.0f)) <= threshold);
ATen/test/cuda_half_test.cu:      ::abs(::atan2(Half(7.0), Half(0.0)) - ::atan2(7.0f, 0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::isnan((float)Half(0.0)) - ::isnan(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::isinf((float)Half(0.0)) - ::isinf(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::isnan(Half(0.0)) - ::isnan(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  assert(::abs(::isinf(Half(0.0)) - ::isinf(0.0f)) <= threshold);
ATen/test/cuda_half_test.cu:  Half real = 3.0f;
ATen/test/cuda_half_test.cu:  Half imag = -10.0f;
ATen/test/cuda_half_test.cu:  auto complex = c10::complex<Half>(real, imag);
ATen/test/cuda_half_test.cu:TEST(HalfCuda, HalfCuda) {
ATen/test/half_test.cpp:TEST(TestHalf, Arithmetic) {
ATen/test/half_test.cpp:  Half zero = 0;
ATen/test/half_test.cpp:  Half one = 1;
ATen/test/half_test.cpp:  ASSERT_EQ(one + one, Half(2));
ATen/test/half_test.cpp:TEST(TestHalf, Comparisions) {
ATen/test/half_test.cpp:  Half zero = 0;
ATen/test/half_test.cpp:  Half one = 1;
ATen/test/half_test.cpp:TEST(TestHalf, Cast) {
ATen/test/half_test.cpp:  Half value = 1.5f;
ATen/test/half_test.cpp:  ASSERT_EQ((bool)Half(0.0f), false);
ATen/test/half_test.cpp:TEST(TestHalf, Construction) {
ATen/test/half_test.cpp:  ASSERT_EQ(Half((short)3), Half(3.0f));
ATen/test/half_test.cpp:  ASSERT_EQ(Half((unsigned short)3), Half(3.0f));
ATen/test/half_test.cpp:  ASSERT_EQ(Half(3), Half(3.0f));
ATen/test/half_test.cpp:  ASSERT_EQ(Half(3U), Half(3.0f));
ATen/test/half_test.cpp:  ASSERT_EQ(Half(3LL), Half(3.0f));
ATen/test/half_test.cpp:  ASSERT_EQ(Half(3ULL), Half(3.0f));
ATen/test/half_test.cpp:  ASSERT_EQ(Half(3.5), Half(3.5f));
ATen/test/half_test.cpp:static std::string to_string(const Half& h) {
ATen/test/half_test.cpp:TEST(TestHalf, Half2String) {
ATen/test/half_test.cpp:  ASSERT_EQ(to_string(Half(3.5f)), "3.5");
ATen/test/half_test.cpp:  ASSERT_EQ(to_string(Half(-100.0f)), "-100");
ATen/test/half_test.cpp:TEST(TestHalf, HalfNumericLimits) {
ATen/test/half_test.cpp:  using limits = std::numeric_limits<Half>;
ATen/test/half_test.cpp:// Check the declared type of members of numeric_limits<Half> matches
ATen/test/half_test.cpp:          decltype(std::numeric_limits<Half>::name),          \
ATen/test/half_test.cpp:TEST(TestHalf, CommonMath) {
ATen/test/half_test.cpp:  assert(std::abs(std::lgamma(Half(10.0)) - std::lgamma(10.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::exp(Half(1.0)) - std::exp(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::log(Half(1.0)) - std::log(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::log10(Half(1000.0)) - std::log10(1000.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::log1p(Half(0.0)) - std::log1p(0.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::log2(Half(1000.0)) - std::log2(1000.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::expm1(Half(1.0)) - std::expm1(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::cos(Half(0.0)) - std::cos(0.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::sin(Half(0.0)) - std::sin(0.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::sqrt(Half(100.0)) - std::sqrt(100.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::ceil(Half(2.4)) - std::ceil(2.4f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::floor(Half(2.7)) - std::floor(2.7f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::trunc(Half(2.7)) - std::trunc(2.7f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::acos(Half(-1.0)) - std::acos(-1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::cosh(Half(1.0)) - std::cosh(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::acosh(Half(1.0)) - std::acosh(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::asin(Half(1.0)) - std::asin(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::sinh(Half(1.0)) - std::sinh(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::asinh(Half(1.0)) - std::asinh(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::tan(Half(0.0)) - std::tan(0.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::atan(Half(1.0)) - std::atan(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::tanh(Half(1.0)) - std::tanh(1.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::erf(Half(10.0)) - std::erf(10.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::erfc(Half(10.0)) - std::erfc(10.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::abs(Half(-3.0)) - std::abs(-3.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::round(Half(2.3)) - std::round(2.3f)) <= threshold);
ATen/test/half_test.cpp:      std::abs(std::pow(Half(2.0), Half(10.0)) - std::pow(2.0f, 10.0f)) <=
ATen/test/half_test.cpp:      std::abs(std::atan2(Half(7.0), Half(0.0)) - std::atan2(7.0f, 0.0f)) <=
ATen/test/half_test.cpp:  // @TODO: can macos do implicit conversion of Half?
ATen/test/half_test.cpp:      std::abs(std::isnan(static_cast<float>(Half(0.0))) - std::isnan(0.0f)) <=
ATen/test/half_test.cpp:      std::abs(std::isinf(static_cast<float>(Half(0.0))) - std::isinf(0.0f)) <=
ATen/test/half_test.cpp:  assert(std::abs(std::isnan(Half(0.0)) - std::isnan(0.0f)) <= threshold);
ATen/test/half_test.cpp:  assert(std::abs(std::isinf(Half(0.0)) - std::isinf(0.0f)) <= threshold);
ATen/test/half_test.cpp:TEST(TestHalf, ComplexHalf) {
ATen/test/half_test.cpp:  Half real = 3.0f;
ATen/test/half_test.cpp:  Half imag = -10.0f;
ATen/test/half_test.cpp:  auto complex = c10::complex<Half>(real, imag);
ATen/test/reduce_ops_test.cpp:    for (const auto dtype : {kHalf, kFloat, kDouble, kShort, kInt, kLong}) {
ATen/test/reduce_ops_test.cpp:      auto a = at::rand({H, W}, TensorOptions(kCUDA).dtype(at::kHalf));
ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(Interleave FirstHalf), std::get<0>(cc), vec::loadu(interleaved)).check(true);
ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(Interleave SecondHalf), std::get<1>(cc), vec::loadu(interleaved + vec::size())).check(true);
ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(DeInterleave FirstHalf), std::get<0>(cc), vec::loadu(vals)).check(true);
ATen/test/vec_test_all_types.cpp:        AssertVectorized<vec>(NAME_INFO(DeInterleave SecondHalf), std::get<1>(cc), vec::loadu(vals + vec::size())).check(true);
ATen/test/cuda_atomic_ops_test.cu:  test_atomic_add<at::Half>();
ATen/test/cuda_atomic_ops_test.cu:  test_atomic_mul<at::Half>();
ATen/test/cuda_atomic_ops_test.cu:  test_atomic_max<at::Half>();
ATen/test/cuda_atomic_ops_test.cu:  test_atomic_min<at::Half>();
halfinaten.txt:ATen/cuda/CUDATensorMethods.cuh:  return reinterpret_cast<__half*>(data<Half>());
halfinaten.txt:ATen/Dispatch.h:// but NOT booleans (bool), half-precision floats (Half) or
halfinaten.txt:ATen/native/cuda/UnaryComplexKernels.cu:    AT_DISPATCH_CASE(kComplexHalf, conj_chalf)
halfinaten.txt:ATen/native/cuda/SoftMax.cu:  static_assert(std::is_same<acc_type<at::Half, true>, float>::value, "accscalar_t for half should be float");
halfinaten.txt:ATen/native/cuda/SoftMax.cu:  static_assert(std::is_same<acc_type<at::Half, true>, float>::value, "accscalar_t for half should be float");
halfinaten.txt:ATen/native/cuda/Normalization.cu:    bool is_half_float = std::is_same<scalar_t, at::Half>::value && mean_st == at::kFloat;
halfinaten.txt:ATen/native/cpu/Activation.cpp:    const Vec kOneHalfVec(one_half);
halfinaten.txt:ATen/native/cpu/Activation.cpp:    const Vec kOneHalfVec(one_half);
halfinaten.txt:ATen/native/transformers/cuda/sdp_utils.h:  bool is_half = (params.query.dtype() == at::kHalf) ||
halfinaten.txt:ATen/native/WeightNorm.cpp:  auto has_half_dtype = v.scalar_type() == at::ScalarType::Half
halfinaten.txt:ATen/native/BatchLinearAlgebra.cpp:  // Half optimisation half precondition for some parts of the LAPACK / cuSOLVER
halfinaten.txt:ATen/test/cuda_half_test.cu:  __half c = a - Half(b);
